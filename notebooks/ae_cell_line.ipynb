{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.ae_selection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m mse_loss\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mae_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils_pp\u001b[39;00m \u001b[39mimport\u001b[39;00m replace_cell_names_with_id\n\u001b[0;32m     10\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mautoreload\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.ae_selection'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.ae_selection import train_test_split\n",
    "from utils_pp import replace_cell_names_with_id\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# columns = [\"cell_line\", \"drugA_name\", \"drugB_name\", \"drugA_conc\", \"drugB_conc\", \"target\"]\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# df_train = pd.read_csv(\"../data_raw/oneil.csv\", usecols=(1,2,3,4,5,12)).iloc[:,[0,1,3,2,4,5]].set_axis(columns, axis=1)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# df_train[\"cell_line\"]\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# df_test = pd.read_csv(\"../data/test_yosua.csv\").set_axis(columns + [\"std\"], axis=1).convert_dtypes()\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m drug_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m\"\u001b[39m\u001b[39m../data/drug_data.pkl.compress\u001b[39m\u001b[39m\"\u001b[39m, compression\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m cell_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m\"\u001b[39m\u001b[39m../data/cell_line_data.pkl.compress\u001b[39m\u001b[39m\"\u001b[39m, compression\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m cell_data_val \u001b[39m=\u001b[39m cell_data\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# columns = [\"cell_line\", \"drugA_name\", \"drugB_name\", \"drugA_conc\", \"drugB_conc\", \"target\"]\n",
    "# df_train = pd.read_csv(\"../data_raw/oneil.csv\", usecols=(1,2,3,4,5,12)).iloc[:,[0,1,3,2,4,5]].set_axis(columns, axis=1)\n",
    "# df_train[\"cell_line\"]\n",
    "# df_test = pd.read_csv(\"../data/test_yosua.csv\").set_axis(columns + [\"std\"], axis=1).convert_dtypes()\n",
    "\n",
    "drug_data = pd.read_pickle(\"../data/drug_data.pkl.compress\", compression=\"gzip\")\n",
    "cell_data = pd.read_pickle(\"../data/cell_line_data.pkl.compress\", compression=\"gzip\")\n",
    "cell_data_val = cell_data.copy()\n",
    "X_train_cell, X_val_cell, y_train_cell, y_val_cell = train_test_split(cell_data, cell_data_val, test_size=0.2)\n",
    "# df_train = replace_cell_names_with_id(dataframe=df_train, mapping_file=\"../data/mappingccl.csv\")\n",
    "# df_test = replace_cell_names_with_id(dataframe=df_test, mapping_file=\"../data/mappingccl.csv\")\n",
    "# df_train = df_train[df_train.cell_line.isin(cell_data.index)]\n",
    "# df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True)\n",
    "# cell_data = cell_data[cell_data.index.isin(pd.concat([df_train.cell_line, df_test.cell_line]))]\n",
    "# print(\"oneil\", df_train.memory_usage().sum()/1e6, df_train.shape,\"\\n\", df_train.dtypes)\n",
    "print(\"drug_feat\", X_train_cell.memory_usage().sum()/1e6, X_train_cell.shape)\n",
    "print(\"drug_feat\", X_val_cell.memory_usage().sum()/1e6, X_val_cell.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2416]) torch.Size([1, 2416])\n"
     ]
    }
   ],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.dataset = data.to_numpy()\n",
    "        self.labels = labels.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx], self.labels[idx]\n",
    "                \n",
    "train_set_drug  = dataset(X_train_cell, y_train_cell)\n",
    "val_set_drug  = dataset(X_val_cell, y_val_cell)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=1, shuffle=True)\n",
    "val_dl_drug = DataLoader(val_set_drug, batch_size=1, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "print(xi.shape, yi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2416]) torch.Size([1, 2416])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a PyTorch class\n",
    "# 28*28 ==> 9 ==> 28*28\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, h_sizes):\n",
    "        super().__init__()\n",
    "         \n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[0], h_sizes[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(h_sizes[1], h_sizes[2])\n",
    "        )\n",
    "         \n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # The Sigmoid activation function\n",
    "        # outputs the value between 0 and 1\n",
    "        # 9 ==> 784\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[2], h_sizes[3]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(h_sizes[3], h_sizes[0])\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "ae = AE([xi.shape[1],512,256,512])\n",
    "yi = ae.forward(xi)\n",
    "print(xi.shape, yi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, training_loader, optimizer, loss_fn, verbose=False):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ae(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() / inputs.shape[0]\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            if verbose:print('  batch {} loss: {}'.format(i + 1, last_loss), outputs[0][0].item(), labels[0][0].item())\n",
    "            # tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 512]       1,237,504\n",
      "              ReLU-2               [-1, 1, 512]               0\n",
      "           Dropout-3               [-1, 1, 512]               0\n",
      "            Linear-4               [-1, 1, 256]         131,328\n",
      "            Linear-5               [-1, 1, 512]         131,584\n",
      "              ReLU-6               [-1, 1, 512]               0\n",
      "           Dropout-7               [-1, 1, 512]               0\n",
      "            Linear-8              [-1, 1, 2416]       1,239,408\n",
      "================================================================\n",
      "Total params: 2,739,824\n",
      "Trainable params: 2,739,824\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 10.45\n",
      "Estimated Total Size (MB): 10.50\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "EPOCH 1:\n",
      "LOSS train 0.7750085800886154 valid 0.6197288632392883\n",
      "EPOCH 2:\n",
      "LOSS train 0.8209628701210022 valid 0.6200272440910339\n",
      "EPOCH 3:\n",
      "LOSS train 0.7147777259349823 valid 0.6202571988105774\n",
      "EPOCH 4:\n",
      "LOSS train 0.575224193930626 valid 0.6205303072929382\n",
      "EPOCH 5:\n",
      "LOSS train 0.5871476143598556 valid 0.6193109750747681\n",
      "EPOCH 6:\n",
      "LOSS train 0.62122762799263 valid 0.6163123846054077\n",
      "EPOCH 7:\n",
      "LOSS train 0.5617733746767044 valid 0.6141545176506042\n",
      "EPOCH 8:\n",
      "LOSS train 0.49448718726634977 valid 0.6117339134216309\n",
      "EPOCH 9:\n",
      "LOSS train 0.42837237417697904 valid 0.6106465458869934\n",
      "EPOCH 10:\n",
      "LOSS train 0.33110565692186356 valid 0.6091455817222595\n",
      "EPOCH 11:\n",
      "LOSS train 0.35119360983371734 valid 0.6079074740409851\n",
      "EPOCH 12:\n",
      "LOSS train 0.32441517412662507 valid 0.6074188351631165\n",
      "EPOCH 13:\n",
      "LOSS train 0.3018967717885971 valid 0.6069472432136536\n",
      "EPOCH 14:\n",
      "LOSS train 0.23747881650924682 valid 0.6058141589164734\n",
      "EPOCH 15:\n",
      "LOSS train 0.2572784319519997 valid 0.6054909229278564\n",
      "EPOCH 16:\n",
      "LOSS train 0.20759315118193628 valid 0.6047930717468262\n",
      "EPOCH 17:\n",
      "LOSS train 0.17436327487230302 valid 0.6046684980392456\n",
      "EPOCH 18:\n",
      "LOSS train 0.1985921159386635 valid 0.6039159297943115\n",
      "EPOCH 19:\n",
      "LOSS train 0.1426784060895443 valid 0.6038150191307068\n",
      "EPOCH 20:\n",
      "LOSS train 0.15015728548169135 valid 0.6035352349281311\n",
      "EPOCH 21:\n",
      "LOSS train 0.14755333065986634 valid 0.6027730107307434\n",
      "EPOCH 22:\n",
      "LOSS train 0.11318524740636349 valid 0.6019955277442932\n",
      "EPOCH 23:\n",
      "LOSS train 0.12104516588151455 valid 0.6016857028007507\n",
      "EPOCH 24:\n",
      "LOSS train 0.10213952027261257 valid 0.6011130213737488\n",
      "EPOCH 25:\n",
      "LOSS train 0.07501498498022556 valid 0.6009450554847717\n",
      "EPOCH 26:\n",
      "LOSS train 0.11507371142506599 valid 0.5999854803085327\n",
      "EPOCH 27:\n",
      "LOSS train 0.11021355912089348 valid 0.6001121401786804\n",
      "EPOCH 28:\n",
      "LOSS train 0.09350745789706708 valid 0.5990774631500244\n",
      "EPOCH 29:\n",
      "LOSS train 0.09953316934406757 valid 0.5984771251678467\n",
      "EPOCH 30:\n",
      "LOSS train 0.07710419520735741 valid 0.5978628396987915\n",
      "EPOCH 31:\n",
      "LOSS train 0.09378499314188957 valid 0.5982380509376526\n",
      "EPOCH 32:\n",
      "LOSS train 0.07734664101153613 valid 0.5969760417938232\n",
      "EPOCH 33:\n",
      "LOSS train 0.07732861265540122 valid 0.5962267518043518\n",
      "EPOCH 34:\n",
      "LOSS train 0.06929846070706844 valid 0.5956182479858398\n",
      "EPOCH 35:\n",
      "LOSS train 0.05269678346812725 valid 0.5958337783813477\n",
      "EPOCH 36:\n",
      "LOSS train 0.07359645906835795 valid 0.5950345396995544\n",
      "EPOCH 37:\n",
      "LOSS train 0.06515266690403224 valid 0.5949846506118774\n",
      "EPOCH 38:\n",
      "LOSS train 0.07629747353494168 valid 0.5945987105369568\n",
      "EPOCH 39:\n",
      "LOSS train 0.038471946865320204 valid 0.5935638546943665\n",
      "EPOCH 40:\n",
      "LOSS train 0.07862350326031446 valid 0.5935527086257935\n",
      "EPOCH 41:\n",
      "LOSS train 0.05920210722833872 valid 0.5933311581611633\n",
      "EPOCH 42:\n",
      "LOSS train 0.050424418039619924 valid 0.5934715270996094\n",
      "EPOCH 43:\n",
      "LOSS train 0.05236795898526907 valid 0.5923750996589661\n",
      "EPOCH 44:\n",
      "LOSS train 0.06592954471707344 valid 0.5930485129356384\n",
      "EPOCH 45:\n",
      "LOSS train 0.040331294387578966 valid 0.5926733016967773\n",
      "EPOCH 46:\n",
      "LOSS train 0.04386376021429896 valid 0.5917847156524658\n",
      "EPOCH 47:\n",
      "LOSS train 0.05090552065521479 valid 0.5924038290977478\n",
      "EPOCH 48:\n",
      "LOSS train 0.043681503646075724 valid 0.5917161107063293\n",
      "EPOCH 49:\n",
      "LOSS train 0.04401840474456549 valid 0.5921223759651184\n",
      "EPOCH 50:\n",
      "LOSS train 0.06408588103950023 valid 0.5916926264762878\n",
      "EPOCH 51:\n",
      "LOSS train 0.03448263183236122 valid 0.5915054082870483\n",
      "EPOCH 52:\n",
      "LOSS train 0.07004757802933455 valid 0.5915601849555969\n",
      "EPOCH 53:\n",
      "LOSS train 0.053355606645345686 valid 0.5920005440711975\n",
      "EPOCH 54:\n",
      "LOSS train 0.052392796240746976 valid 0.5914345383644104\n",
      "EPOCH 55:\n",
      "LOSS train 0.04535672459751368 valid 0.5915305614471436\n",
      "EPOCH 56:\n",
      "LOSS train 0.04543017651885748 valid 0.5911244750022888\n",
      "EPOCH 57:\n",
      "LOSS train 0.03967991657555103 valid 0.59122633934021\n",
      "EPOCH 58:\n",
      "LOSS train 0.038796167261898516 valid 0.5908529162406921\n",
      "EPOCH 59:\n",
      "LOSS train 0.05555120576173067 valid 0.5912504196166992\n",
      "EPOCH 60:\n",
      "LOSS train 0.040758002828806636 valid 0.591505765914917\n",
      "EPOCH 61:\n",
      "LOSS train 0.03730772566050291 valid 0.5907496809959412\n",
      "EPOCH 62:\n",
      "LOSS train 0.04602186549454927 valid 0.591180145740509\n",
      "EPOCH 63:\n",
      "LOSS train 0.04246379323303699 valid 0.5914116501808167\n",
      "EPOCH 64:\n",
      "LOSS train 0.03410208392888307 valid 0.5914075970649719\n",
      "EPOCH 65:\n",
      "LOSS train 0.032596899941563603 valid 0.5912572145462036\n",
      "EPOCH 66:\n",
      "LOSS train 0.03803240153938532 valid 0.5914171934127808\n",
      "EPOCH 67:\n",
      "LOSS train 0.03994848541915417 valid 0.5913766026496887\n",
      "EPOCH 68:\n",
      "LOSS train 0.034731467440724376 valid 0.5913291573524475\n",
      "EPOCH 69:\n",
      "LOSS train 0.03486608974635601 valid 0.5912145376205444\n",
      "EPOCH 70:\n",
      "LOSS train 0.033740935288369654 valid 0.5916317701339722\n",
      "EPOCH 71:\n",
      "LOSS train 0.029465360287576913 valid 0.5914259552955627\n",
      "EPOCH 72:\n",
      "LOSS train 0.034935731533914806 valid 0.5915979146957397\n",
      "EPOCH 73:\n",
      "LOSS train 0.03459835667163134 valid 0.591518759727478\n",
      "EPOCH 74:\n",
      "LOSS train 0.02887151055037975 valid 0.5909526944160461\n",
      "EPOCH 75:\n",
      "LOSS train 0.03748113065958023 valid 0.5913452506065369\n",
      "EPOCH 76:\n",
      "LOSS train 0.03201752509921789 valid 0.5911591649055481\n",
      "EPOCH 77:\n",
      "LOSS train 0.032947337813675405 valid 0.5910835266113281\n",
      "EPOCH 78:\n",
      "LOSS train 0.03110105162486434 valid 0.5915533900260925\n",
      "EPOCH 79:\n",
      "LOSS train 0.020293442998081446 valid 0.592223584651947\n",
      "EPOCH 80:\n",
      "LOSS train 0.032639619708061215 valid 0.5915105938911438\n",
      "EPOCH 81:\n",
      "LOSS train 0.03206387860700488 valid 0.5920669436454773\n",
      "EPOCH 82:\n",
      "LOSS train 0.028965551406145096 valid 0.5918634533882141\n",
      "EPOCH 83:\n",
      "LOSS train 0.025962688867002724 valid 0.5917225480079651\n",
      "EPOCH 84:\n",
      "LOSS train 0.024703848548233508 valid 0.591999888420105\n",
      "EPOCH 85:\n",
      "LOSS train 0.025143936462700366 valid 0.5918882489204407\n",
      "EPOCH 86:\n",
      "LOSS train 0.02286061104387045 valid 0.5927016139030457\n",
      "EPOCH 87:\n",
      "LOSS train 0.032476738560944796 valid 0.5920470952987671\n",
      "EPOCH 88:\n",
      "LOSS train 0.02643413972109556 valid 0.5927150845527649\n",
      "EPOCH 89:\n",
      "LOSS train 0.032148609310388564 valid 0.5922812819480896\n",
      "EPOCH 90:\n",
      "LOSS train 0.032986486237496136 valid 0.5921657681465149\n",
      "EPOCH 91:\n",
      "LOSS train 0.020429424196481704 valid 0.5919536352157593\n",
      "EPOCH 92:\n",
      "LOSS train 0.03401747085154057 valid 0.5917912125587463\n",
      "EPOCH 93:\n",
      "LOSS train 0.027526425942778588 valid 0.5919813513755798\n",
      "EPOCH 94:\n",
      "LOSS train 0.021261000633239747 valid 0.5923464894294739\n",
      "EPOCH 95:\n",
      "LOSS train 0.025663999561220407 valid 0.5921986699104309\n",
      "EPOCH 96:\n",
      "LOSS train 0.02245534546673298 valid 0.592296838760376\n",
      "EPOCH 97:\n",
      "LOSS train 0.028003018442541362 valid 0.5919326543807983\n",
      "EPOCH 98:\n",
      "LOSS train 0.02773456322029233 valid 0.5919410586357117\n",
      "EPOCH 99:\n",
      "LOSS train 0.017419323418289424 valid 0.59201580286026\n",
      "EPOCH 100:\n",
      "LOSS train 0.020988545473665 valid 0.5924468040466309\n"
     ]
    }
   ],
   "source": [
    "train_set_drug  = dataset(X_train_drug, y_train_drug)\n",
    "val_set_drug  = dataset(X_val_drug, y_val_drug)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=1, shuffle=True)\n",
    "val_dl_drug = DataLoader(val_set_drug, batch_size=1, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "\n",
    "ae = AE([xi.shape[1],512,256,512])\n",
    "\n",
    "print(summary(ae.to(\"cuda\"), xi.shape))\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    ae.train(True)\n",
    "    ae = ae.to(device=device)\n",
    "\n",
    "    avg_loss = train_one_epoch(epoch_number, \"writer\", train_dl_drug, optimizer, loss_fn)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the ae to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    ae.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dl_drug):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            voutputs = ae(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels) / vinputs.shape[0]\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1) \n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    epoch_number += 1\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, data in enumerate(val_dl_drug):\n",
    "#         inputs, labels = data\n",
    "#         inputs = inputs.to(device=device)\n",
    "#         labels = labels.to(device=device)\n",
    "#         outputs = ae(inputs)\n",
    "#         print(outputs.detach().to(\"cpu\").numpy(), labels.detach().to(\"cpu\").numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
