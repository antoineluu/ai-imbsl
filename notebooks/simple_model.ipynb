{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_pp import replace_cell_names_with_id\n",
    "from utils_pp import Encoder\n",
    "from utils_pp import EarlyStopper\n",
    "from utils_pp import Dataset_from_pd\n",
    "from utils_pp import train_one_epoch\n",
    "from utils_pp import AE_DNN\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneil 13.13536 (234560, 6) \n",
      " cell_line      object\n",
      "drugA_name     object\n",
      "drugB_name     object\n",
      "drugA_conc    float64\n",
      "drugB_conc    float64\n",
      "target        float64\n",
      "dtype: object\n",
      "drug_feat 0.427236 (42, 2412)\n",
      "cell_feat 0.686136 (32, 5011)\n"
     ]
    }
   ],
   "source": [
    "columns = [\"cell_line\", \"drugA_name\", \"drugB_name\", \"drugA_conc\", \"drugB_conc\", \"target\"]\n",
    "data_train = pd.read_csv(\"../data_raw/oneil.csv\", usecols=(1,2,3,4,5,12)).iloc[:,[0,1,3,2,4,5]].set_axis(columns, axis=1)\n",
    "data_test = pd.read_csv(\"../data/test_yosua.csv\").set_axis(columns + [\"std\"], axis=1).convert_dtypes()\n",
    "data_train = replace_cell_names_with_id(dataframe=data_train, mapping_file=\"../data/mappingccl.csv\")\n",
    "data_test = replace_cell_names_with_id(dataframe=data_test, mapping_file=\"../data/mappingccl.csv\")\n",
    "drug_data = pd.read_pickle(\"../data/drug_data.pkl.compress\", compression=\"gzip\")\n",
    "cell_data = pd.read_pickle(\"../data/cell_line_data.pkl.compress\", compression=\"gzip\")\n",
    "data_train = data_train[data_train.cell_line.isin(cell_data.index)]\n",
    "\n",
    "df_train, df_val = train_test_split(data_train, test_size=0.2, shuffle=True, random_state=42)\n",
    "df_test = data_test\n",
    "\n",
    "cell_data = cell_data[cell_data.index.isin(pd.concat([df_train.cell_line, df_test.cell_line]))]\n",
    "drug_data = drug_data[drug_data.index.isin(pd.concat([df_train.drugA_name, df_train.drugB_name,df_test.drugA_name, df_test.drugB_name]))]\n",
    "print(\"oneil\", df_train.memory_usage().sum()/1e6, df_train.shape,\"\\n\", df_train.dtypes)\n",
    "print(\"drug_feat\", drug_data.memory_usage().sum()/1e6, drug_data.shape)\n",
    "print(\"cell_feat\", cell_data.memory_usage().sum()/1e6, cell_data.shape)\n",
    "DRUG_LENGTH = drug_data.shape[1]\n",
    "CELL_LENGTH = cell_data.shape[1]\n",
    "EMBED_SIZE = 770"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AE_DNN([770,256,256,256, 128,128,128,64,64,64], drug_length=DRUG_LENGTH, cell_length=CELL_LENGTH)\n",
    "model.load_state_dict(torch.load(\"../models/ae_dnn_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data_train, data_val, L, n_epochs=100):\n",
    "    batch_size = 1024\n",
    "    train_set  = Dataset_from_pd(data_train, drug_data, cell_data)\n",
    "    val_set = Dataset_from_pd(data_val, drug_data, cell_data)\n",
    "    test_set  = Dataset_from_pd(df_test, drug_data, cell_data)\n",
    "    train_dl = DataLoader(train_set, batch_size=batch_size)\n",
    "    xi, yi = next(iter(train_dl))\n",
    "    val_dl = DataLoader(val_set, batch_size=batch_size)\n",
    "    test_dl = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "    model = AE_DNN([770,256,256,256, 128,128,128,64,64,64], DRUG_LENGTH, CELL_LENGTH)\n",
    "    model.drug_encoder = Encoder(model.drug_encoder.h_sizes)\n",
    "    model.drug_encoder.encoder.load_state_dict(torch.load(\"../models/drug_encoder.pt\"))\n",
    "    model.cell_encoder = Encoder(model.cell_encoder.h_sizes)\n",
    "    model.cell_encoder.encoder.load_state_dict(torch.load(\"../models/cell_encoder.pt\"))\n",
    "    model.drug_encoder.eval()\n",
    "    model.cell_encoder.eval()\n",
    "    # print(summary(model.to(\"cuda\"), 770))\n",
    "    optimizer = torch.optim.Adam(model.hidden.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    mae_fn = torch.nn.L1Loss()\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    early_stopper = EarlyStopper(patience=10)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, verbose=True, patience=5, min_lr=1e-7)\n",
    "\n",
    "    epoch_number = 0\n",
    "    EPOCHS = n_epochs\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.hidden.train(True)\n",
    "        model = model.to(device=device)\n",
    "        avg_loss = train_one_epoch(model, epoch_number, \"writer\", train_dl, optimizer, loss_fn, device, 0)\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        running_MAE = 0.\n",
    "        running_PCC = 0.\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_dl):\n",
    "\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs = vinputs.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "                vx = voutputs - torch.mean(voutputs)\n",
    "                vy = vlabels - torch.mean(vlabels)\n",
    "                cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "                running_PCC += cost.item()\n",
    "\n",
    "                mae_loss = mae_fn(voutputs, vlabels)\n",
    "                running_MAE += mae_loss.item()\n",
    "\n",
    "        avg_vMAE = running_MAE/(i+1)\n",
    "        avg_vPCC = running_PCC/(i+1)\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('epoch {} mse {:.{round}f} vmse {:.{round}f} vmae {:.{round}f} vpcc {:.{round}f} '.format(epoch_number+1, avg_loss, avg_vloss, avg_vMAE, avg_vPCC, round=4))\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if early_stopper.early_stop(avg_vloss):             \n",
    "            break\n",
    "        epoch_number += 1\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dl):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(inputs)\n",
    "            loss_test = loss_fn(outputs, labels).item()\n",
    "            mae_test = mae_fn(outputs, labels).item()\n",
    "            vx = voutputs - torch.mean(outputs)\n",
    "            vy = vlabels - torch.mean(labels)\n",
    "            pcc_test = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2))).item()\n",
    "    print([avg_vloss, avg_vMAE, avg_vPCC, loss_test, mae_test, pcc_test])\n",
    "    L.append([avg_vloss, avg_vMAE, avg_vPCC, loss_test, mae_test, pcc_test, outputs.to(\"cpu\").numpy().reshape(-1)])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LPO/LTO cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs = data_train.loc[:,[\"drugA_name\",\"drugB_name\"]].drop_duplicates()\n",
    "unique_triplets = data_train.loc[:,[\"cell_line\", \"drugA_name\",\"drugB_name\"]].drop_duplicates()\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "batch_size = 2048\n",
    "\n",
    "L_leave_triplet = []\n",
    "for train_index, val_index in kf.split(unique_triplets):\n",
    "    train_unique_triplets = unique_triplets.iloc[train_index]\n",
    "    val_unique_triplets =unique_triplets.iloc[val_index]\n",
    "    combined_train = train_unique_triplets.loc[:,\"cell_line\"].str.cat([train_unique_triplets.loc[:,\"drugA_name\"], train_unique_triplets.loc[:,\"drugB_name\"]], sep= \" + \")\n",
    "    combined_val = val_unique_triplets.loc[:,\"cell_line\"].str.cat([val_unique_triplets.loc[:,\"drugA_name\"], val_unique_triplets.loc[:,\"drugB_name\"]], sep= \" + \")\n",
    "    \n",
    "    data_train = df_train[data_train.loc[:,\"cell_line\"].str.cat([data_train.loc[:,\"drugA_name\"],data_train.loc[:,\"drugB_name\"]],sep=\" + \").isin(combined_train)]\n",
    "    data_val = df_train[data_train.loc[:,\"cell_line\"].str.cat([data_train.loc[:,\"drugA_name\"],data_train.loc[:,\"drugB_name\"]],sep=\" + \").isin(combined_val)]\n",
    "    print(data_train.shape, data_val.shape)\n",
    "    training(data_train, data_val, L_leave_triplet)\n",
    "L_leave_pair = []\n",
    "for train_index, val_index in kf.split(unique_pairs):\n",
    "    train_unique_pairs = unique_pairs.iloc[train_index]\n",
    "    val_unique_pairs =unique_pairs.iloc[val_index]\n",
    "    combined_train = train_unique_pairs.loc[:,\"drugA_name\"].str.cat(train_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "    combined_val = val_unique_pairs.loc[:,\"drugA_name\"].str.cat(val_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "    \n",
    "    data_train = df_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_train)]\n",
    "    data_val = df_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_val)]\n",
    "    print(data_train.shape, data_val.shape)\n",
    "    training(data_train, data_val, L_leave_pair)\n",
    "\n",
    "L_leave_pair_records = np.empty((5,4))\n",
    "for i in range(5):\n",
    "    for j in range(4): L_leave_pair_records[i,j]=L_leave_pair[i][j]\n",
    "np.save(\"Leave_pair_records\", L_leave_pair_records)\n",
    "new_array = np.load(\"Leave_pair_records.npy\")\n",
    "print(new_array)\n",
    "L_leave_triplet_records = np.empty((5,4))\n",
    "for i in range(5):\n",
    "    for j in range(4): L_leave_triplet_records[i,j]=L_leave_triplet[i][j]\n",
    "np.save(\"Leave_triplet_records\", L_leave_triplet_records)\n",
    "new_array = np.load(\"Leave_triplet_records.npy\")\n",
    "print(new_array)\n",
    "\n",
    "test_pair_outputs = np.empty((5,24))\n",
    "for i in range(5):\n",
    "    for j in range(24): test_pair_outputs[i,j]=L_leave_pair[i][6][j]\n",
    "np.save(\"test_pair_outputs\", test_pair_outputs)\n",
    "new_array = np.load(\"test_pair_outputs.npy\")\n",
    "print(new_array)\n",
    "test_triplet_outputs = np.empty((5,24))\n",
    "for i in range(5):\n",
    "    for j in range(24): test_triplet_outputs[i,j]=L_leave_triplet[i][6][j]\n",
    "np.save(\"test_triplet_outputs\", test_triplet_outputs)\n",
    "new_array = np.load(\"test_triplet_outputs.npy\")\n",
    "print(new_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL TRAINING LPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168344, 6) (42732, 6)\n",
      "\n",
      "\n",
      "\n",
      "epoch 1 mse 0.0892 vmse 0.0981 vmae 0.2629 vpcc 0.4269 \n",
      "epoch 2 mse 0.0730 vmse 0.0783 vmae 0.2332 vpcc 0.5460 \n",
      "epoch 3 mse 0.0636 vmse 0.0740 vmae 0.2235 vpcc 0.6018 \n",
      "epoch 4 mse 0.0596 vmse 0.0686 vmae 0.2129 vpcc 0.6218 \n",
      "epoch 5 mse 0.0574 vmse 0.0618 vmae 0.1998 vpcc 0.6490 \n",
      "epoch 6 mse 0.0540 vmse 0.0596 vmae 0.1968 vpcc 0.6612 \n",
      "epoch 7 mse 0.0527 vmse 0.0583 vmae 0.1930 vpcc 0.6731 \n",
      "epoch 8 mse 0.0522 vmse 0.0575 vmae 0.1893 vpcc 0.6717 \n",
      "epoch 9 mse 0.0503 vmse 0.0558 vmae 0.1860 vpcc 0.6760 \n",
      "epoch 10 mse 0.0488 vmse 0.0547 vmae 0.1833 vpcc 0.6833 \n",
      "epoch 11 mse 0.0488 vmse 0.0535 vmae 0.1808 vpcc 0.6895 \n",
      "epoch 12 mse 0.0470 vmse 0.0530 vmae 0.1802 vpcc 0.6943 \n",
      "epoch 13 mse 0.0460 vmse 0.0517 vmae 0.1767 vpcc 0.7023 \n",
      "epoch 14 mse 0.0465 vmse 0.0522 vmae 0.1757 vpcc 0.7024 \n",
      "epoch 15 mse 0.0461 vmse 0.0510 vmae 0.1734 vpcc 0.7045 \n",
      "epoch 16 mse 0.0451 vmse 0.0498 vmae 0.1698 vpcc 0.7149 \n",
      "epoch 17 mse 0.0442 vmse 0.0494 vmae 0.1694 vpcc 0.7151 \n",
      "epoch 18 mse 0.0442 vmse 0.0489 vmae 0.1680 vpcc 0.7187 \n",
      "epoch 19 mse 0.0435 vmse 0.0485 vmae 0.1677 vpcc 0.7218 \n",
      "epoch 20 mse 0.0430 vmse 0.0480 vmae 0.1662 vpcc 0.7253 \n",
      "epoch 21 mse 0.0418 vmse 0.0482 vmae 0.1662 vpcc 0.7244 \n",
      "epoch 22 mse 0.0412 vmse 0.0471 vmae 0.1646 vpcc 0.7316 \n",
      "epoch 23 mse 0.0414 vmse 0.0474 vmae 0.1646 vpcc 0.7295 \n",
      "epoch 24 mse 0.0406 vmse 0.0467 vmae 0.1628 vpcc 0.7343 \n",
      "epoch 25 mse 0.0413 vmse 0.0471 vmae 0.1635 vpcc 0.7305 \n",
      "epoch 26 mse 0.0407 vmse 0.0463 vmae 0.1617 vpcc 0.7365 \n",
      "epoch 27 mse 0.0405 vmse 0.0463 vmae 0.1614 vpcc 0.7368 \n",
      "epoch 28 mse 0.0390 vmse 0.0453 vmae 0.1596 vpcc 0.7432 \n",
      "epoch 29 mse 0.0387 vmse 0.0453 vmae 0.1583 vpcc 0.7427 \n",
      "epoch 30 mse 0.0385 vmse 0.0448 vmae 0.1587 vpcc 0.7475 \n",
      "epoch 31 mse 0.0387 vmse 0.0443 vmae 0.1569 vpcc 0.7497 \n",
      "epoch 32 mse 0.0386 vmse 0.0450 vmae 0.1580 vpcc 0.7457 \n",
      "epoch 33 mse 0.0382 vmse 0.0447 vmae 0.1570 vpcc 0.7492 \n",
      "epoch 34 mse 0.0382 vmse 0.0443 vmae 0.1572 vpcc 0.7503 \n",
      "epoch 35 mse 0.0375 vmse 0.0441 vmae 0.1567 vpcc 0.7515 \n",
      "epoch 36 mse 0.0375 vmse 0.0433 vmae 0.1546 vpcc 0.7567 \n",
      "epoch 37 mse 0.0362 vmse 0.0433 vmae 0.1549 vpcc 0.7574 \n",
      "epoch 38 mse 0.0370 vmse 0.0436 vmae 0.1555 vpcc 0.7546 \n",
      "epoch 39 mse 0.0366 vmse 0.0419 vmae 0.1518 vpcc 0.7663 \n",
      "epoch 40 mse 0.0357 vmse 0.0425 vmae 0.1533 vpcc 0.7631 \n",
      "epoch 41 mse 0.0358 vmse 0.0420 vmae 0.1521 vpcc 0.7653 \n",
      "epoch 42 mse 0.0353 vmse 0.0425 vmae 0.1535 vpcc 0.7633 \n",
      "epoch 43 mse 0.0353 vmse 0.0422 vmae 0.1523 vpcc 0.7651 \n",
      "epoch 44 mse 0.0351 vmse 0.0419 vmae 0.1518 vpcc 0.7668 \n",
      "epoch 45 mse 0.0349 vmse 0.0409 vmae 0.1501 vpcc 0.7747 \n",
      "epoch 46 mse 0.0356 vmse 0.0412 vmae 0.1500 vpcc 0.7705 \n",
      "epoch 47 mse 0.0346 vmse 0.0410 vmae 0.1486 vpcc 0.7714 \n",
      "epoch 48 mse 0.0336 vmse 0.0413 vmae 0.1516 vpcc 0.7723 \n",
      "epoch 49 mse 0.0332 vmse 0.0401 vmae 0.1473 vpcc 0.7785 \n",
      "epoch 50 mse 0.0334 vmse 0.0402 vmae 0.1487 vpcc 0.7776 \n",
      "epoch 51 mse 0.0336 vmse 0.0404 vmae 0.1488 vpcc 0.7766 \n",
      "epoch 52 mse 0.0333 vmse 0.0392 vmae 0.1450 vpcc 0.7828 \n",
      "epoch 53 mse 0.0330 vmse 0.0397 vmae 0.1476 vpcc 0.7815 \n",
      "epoch 54 mse 0.0326 vmse 0.0394 vmae 0.1468 vpcc 0.7824 \n",
      "epoch 55 mse 0.0328 vmse 0.0398 vmae 0.1479 vpcc 0.7807 \n",
      "epoch 56 mse 0.0333 vmse 0.0400 vmae 0.1483 vpcc 0.7790 \n",
      "epoch 57 mse 0.0323 vmse 0.0391 vmae 0.1470 vpcc 0.7859 \n",
      "epoch 58 mse 0.0332 vmse 0.0400 vmae 0.1473 vpcc 0.7791 \n",
      "epoch 59 mse 0.0313 vmse 0.0387 vmae 0.1458 vpcc 0.7898 \n",
      "epoch 60 mse 0.0310 vmse 0.0380 vmae 0.1449 vpcc 0.7952 \n",
      "epoch 61 mse 0.0314 vmse 0.0377 vmae 0.1431 vpcc 0.7948 \n",
      "epoch 62 mse 0.0316 vmse 0.0396 vmae 0.1473 vpcc 0.7815 \n",
      "epoch 63 mse 0.0313 vmse 0.0376 vmae 0.1430 vpcc 0.7952 \n",
      "epoch 64 mse 0.0316 vmse 0.0378 vmae 0.1438 vpcc 0.7958 \n",
      "epoch 65 mse 0.0313 vmse 0.0369 vmae 0.1411 vpcc 0.7997 \n",
      "epoch 66 mse 0.0310 vmse 0.0375 vmae 0.1435 vpcc 0.7969 \n",
      "epoch 67 mse 0.0313 vmse 0.0375 vmae 0.1421 vpcc 0.7962 \n",
      "epoch 68 mse 0.0300 vmse 0.0365 vmae 0.1396 vpcc 0.8013 \n",
      "epoch 69 mse 0.0309 vmse 0.0371 vmae 0.1418 vpcc 0.7989 \n",
      "epoch 70 mse 0.0303 vmse 0.0373 vmae 0.1427 vpcc 0.7980 \n",
      "epoch 71 mse 0.0303 vmse 0.0369 vmae 0.1419 vpcc 0.8001 \n",
      "epoch 72 mse 0.0296 vmse 0.0366 vmae 0.1404 vpcc 0.8032 \n",
      "epoch 73 mse 0.0297 vmse 0.0362 vmae 0.1402 vpcc 0.8042 \n",
      "epoch 74 mse 0.0301 vmse 0.0373 vmae 0.1422 vpcc 0.7984 \n",
      "epoch 75 mse 0.0294 vmse 0.0360 vmae 0.1395 vpcc 0.8068 \n",
      "epoch 76 mse 0.0289 vmse 0.0359 vmae 0.1402 vpcc 0.8094 \n",
      "epoch 77 mse 0.0294 vmse 0.0360 vmae 0.1393 vpcc 0.8068 \n",
      "epoch 78 mse 0.0297 vmse 0.0374 vmae 0.1423 vpcc 0.7974 \n",
      "epoch 79 mse 0.0294 vmse 0.0360 vmae 0.1393 vpcc 0.8065 \n",
      "epoch 80 mse 0.0302 vmse 0.0366 vmae 0.1417 vpcc 0.8056 \n",
      "epoch 81 mse 0.0293 vmse 0.0375 vmae 0.1406 vpcc 0.7940 \n",
      "epoch 82 mse 0.0284 vmse 0.0358 vmae 0.1382 vpcc 0.8062 \n",
      "epoch 83 mse 0.0286 vmse 0.0358 vmae 0.1384 vpcc 0.8075 \n",
      "epoch 84 mse 0.0292 vmse 0.0361 vmae 0.1402 vpcc 0.8078 \n",
      "epoch 85 mse 0.0280 vmse 0.0345 vmae 0.1355 vpcc 0.8141 \n",
      "epoch 86 mse 0.0287 vmse 0.0362 vmae 0.1409 vpcc 0.8062 \n",
      "epoch 87 mse 0.0280 vmse 0.0362 vmae 0.1392 vpcc 0.8072 \n",
      "epoch 88 mse 0.0285 vmse 0.0357 vmae 0.1382 vpcc 0.8085 \n",
      "epoch 89 mse 0.0295 vmse 0.0369 vmae 0.1424 vpcc 0.8035 \n",
      "epoch 90 mse 0.0278 vmse 0.0350 vmae 0.1379 vpcc 0.8142 \n",
      "epoch 91 mse 0.0280 vmse 0.0348 vmae 0.1371 vpcc 0.8156 \n",
      "epoch 92 mse 0.0277 vmse 0.0343 vmae 0.1363 vpcc 0.8183 \n",
      "epoch 93 mse 0.0269 vmse 0.0346 vmae 0.1358 vpcc 0.8143 \n",
      "epoch 94 mse 0.0283 vmse 0.0350 vmae 0.1376 vpcc 0.8138 \n",
      "epoch 95 mse 0.0282 vmse 0.0341 vmae 0.1356 vpcc 0.8198 \n",
      "epoch 96 mse 0.0266 vmse 0.0331 vmae 0.1334 vpcc 0.8255 \n",
      "epoch 97 mse 0.0272 vmse 0.0349 vmae 0.1361 vpcc 0.8136 \n",
      "epoch 98 mse 0.0276 vmse 0.0350 vmae 0.1376 vpcc 0.8142 \n",
      "epoch 99 mse 0.0293 vmse 0.0348 vmae 0.1381 vpcc 0.8163 \n",
      "epoch 100 mse 0.0272 vmse 0.0350 vmae 0.1376 vpcc 0.8148 \n",
      "epoch 101 mse 0.0265 vmse 0.0340 vmae 0.1350 vpcc 0.8200 \n",
      "epoch 102 mse 0.0284 vmse 0.0340 vmae 0.1354 vpcc 0.8189 \n",
      "epoch 103 mse 0.0294 vmse 0.0350 vmae 0.1377 vpcc 0.8137 \n",
      "epoch 104 mse 0.0270 vmse 0.0343 vmae 0.1360 vpcc 0.8202 \n",
      "epoch 105 mse 0.0270 vmse 0.0337 vmae 0.1350 vpcc 0.8249 \n",
      "epoch 106 mse 0.0268 vmse 0.0337 vmae 0.1332 vpcc 0.8203 \n",
      "[0.033737391366490295, 0.13322725927545911, 0.8203254157588595, 0.8074450492858887, 0.7786823511123657, tensor(-0.3462, device='cuda:0')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AE_DNN(\n",
       "  (drug_encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=2412, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cell_encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=5011, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (hidden): ModuleList(\n",
       "    (0): Linear(in_features=770, out_features=256, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Dropout(p=0.1, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): Dropout(p=0.1, inplace=False)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (13): Dropout(p=0.1, inplace=False)\n",
       "    (14): ReLU()\n",
       "    (15): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (16): Dropout(p=0.1, inplace=False)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (19): Dropout(p=0.1, inplace=False)\n",
       "    (20): ReLU()\n",
       "    (21): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (22): Dropout(p=0.1, inplace=False)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (25): Dropout(p=0.1, inplace=False)\n",
       "    (26): ReLU()\n",
       "    (27): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_leave_pair=[]\n",
    "unique_pairs = data_train.loc[:,[\"drugA_name\",\"drugB_name\"]].drop_duplicates()\n",
    "train_unique_pairs, val_unique_pairs = train_test_split(unique_pairs, test_size=0.2, random_state=42)\n",
    "combined_train = train_unique_pairs.loc[:,\"drugA_name\"].str.cat(train_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "combined_val = val_unique_pairs.loc[:,\"drugA_name\"].str.cat(val_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "\n",
    "df_train = data_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_train)]\n",
    "df_val = data_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_val)]\n",
    "print(df_train.shape, df_val.shape)\n",
    "training(df_train,  df_val, L_leave_pair, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL TRAINING LTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "epoch 1 mse 0.0887 vmse 0.0884 vmae 0.2510 vpcc 0.4438 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m L_full_train_ep \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m df_train, df_val \u001b[39m=\u001b[39m train_test_split(data_train, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m training(df_train, df_val, L_full_train_ep, \u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[62], line 36\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(data_train, data_val, L, n_epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m model\u001b[39m.\u001b[39mhidden\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m---> 36\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(model, epoch_number, \u001b[39m\"\u001b[39;49m\u001b[39mwriter\u001b[39;49m\u001b[39m\"\u001b[39;49m, train_dl, optimizer, loss_fn, device, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m# statistics for batch normalization.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\ai-imbsl\\notebooks\\utils_pp.py:44\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, epoch_index, tb_writer, training_loader, optimizer, loss_fn, device, L1, verbose)\u001b[0m\n\u001b[0;32m     42\u001b[0m last_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m     43\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 44\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(training_loader):\n\u001b[0;32m     45\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     46\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\ai-imbsl\\notebooks\\utils_pp.py:158\u001b[0m, in \u001b[0;36mDataset_from_pd.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    154\u001b[0m drug_B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrug_feat[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrug_mapping[combi[\u001b[39m2\u001b[39m]]]\n\u001b[0;32m    155\u001b[0m cell_line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_feat[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_mapping[combi[\u001b[39m0\u001b[39m]]]\n\u001b[1;32m--> 158\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mconcatenate([drug_A, drug_B, cell_line, combi[\u001b[39m3\u001b[39;49m:\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m\"\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m\"\u001b[39;49m)], dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m\"\u001b[39;49m), combi[\u001b[39m5\u001b[39m:\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "L_full_train_ep = []\n",
    "df_train, df_val = train_test_split(data_train, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "model = training(df_train, df_val, L_full_train_ep, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL training without validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "time = str(datetime.now()).replace(\" \", \"_\").replace(\":\",\"_\")\n",
    "time = \"model_finetune\"\n",
    "path  =\"../models/ae_dnn_{}.pt\".format(time)\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning with testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     cell_line   drugA_name    drugB_name  drugA_conc  drugB_conc    target  \\\n",
      "0   ACH-000768       GW9662      PD168393       2.975      4.0675   1.02498   \n",
      "1   ACH-000768       GW9662      PD168393        5.95       8.135  0.788251   \n",
      "2   ACH-000768       GW9662      PD168393       8.925     12.2025  0.711825   \n",
      "3   ACH-000768       GW9662      PD168393        11.9       16.27  0.542289   \n",
      "4   ACH-000768       GW9662  Rocilinostat       2.975       4.395  0.970456   \n",
      "5   ACH-000768       GW9662  Rocilinostat        5.95        8.79   0.79969   \n",
      "6   ACH-000768       GW9662  Rocilinostat       8.925      13.185  0.523822   \n",
      "7   ACH-000768       GW9662  Rocilinostat        11.9       17.58  0.211632   \n",
      "8   ACH-000768       GW9662   Saracatinib       2.975      0.0125  1.951693   \n",
      "9   ACH-000768       GW9662   Saracatinib        5.95       0.025  1.492152   \n",
      "10  ACH-000768       GW9662   Saracatinib       8.925      0.0375  1.485964   \n",
      "11  ACH-000768       GW9662   Saracatinib        11.9        0.05  1.414594   \n",
      "12  ACH-000768     PD168393  Rocilinostat      4.0675       4.395  0.362846   \n",
      "13  ACH-000768     PD168393  Rocilinostat       8.135        8.79   0.01175   \n",
      "14  ACH-000768     PD168393  Rocilinostat     12.2025      13.185  0.004133   \n",
      "15  ACH-000768     PD168393  Rocilinostat       16.27       17.58  0.010231   \n",
      "16  ACH-000768     PD168393   Saracatinib      4.0675      0.0125  1.504466   \n",
      "17  ACH-000768     PD168393   Saracatinib       8.135       0.025  1.161032   \n",
      "18  ACH-000768     PD168393   Saracatinib     12.2025      0.0375  1.021711   \n",
      "19  ACH-000768     PD168393   Saracatinib       16.27        0.05  1.078313   \n",
      "20  ACH-000768  Saracatinib  Rocilinostat      0.0125       4.395  1.508316   \n",
      "21  ACH-000768  Saracatinib  Rocilinostat       0.025        8.79  1.374209   \n",
      "22  ACH-000768  Saracatinib  Rocilinostat      0.0375      13.185  1.138154   \n",
      "23  ACH-000768  Saracatinib  Rocilinostat        0.05       17.58  0.964295   \n",
      "\n",
      "         std  \n",
      "0   0.213373  \n",
      "1   0.105838  \n",
      "2   0.177698  \n",
      "3   0.188043  \n",
      "4    0.20208  \n",
      "5   0.023706  \n",
      "6   0.183895  \n",
      "7   0.025602  \n",
      "8   6.440154  \n",
      "9   1.795536  \n",
      "10  1.266538  \n",
      "11  0.574059  \n",
      "12  0.082831  \n",
      "13  0.000102  \n",
      "14  0.000008  \n",
      "15  0.000071  \n",
      "16   0.61118  \n",
      "17  0.157102  \n",
      "18  0.210985  \n",
      "19  0.134132  \n",
      "20  0.624652  \n",
      "21   0.68497  \n",
      "22  0.322111  \n",
      "23   0.22007  \n",
      "epoch 1 mse 0.0414 vmse 0.0737 vmae 0.2261 vpcc 0.9333 \n",
      "epoch 2 mse 0.0560 vmse 0.0381 vmae 0.1498 vpcc 0.9530 \n",
      "epoch 3 mse 0.0573 vmse 0.0334 vmae 0.1483 vpcc 0.9693 \n",
      "epoch 4 mse 0.0639 vmse 0.0175 vmae 0.1024 vpcc 0.9699 \n",
      "epoch 5 mse 0.0244 vmse 0.0253 vmae 0.1218 vpcc 0.9633 \n",
      "epoch 6 mse 0.0218 vmse 0.0387 vmae 0.1545 vpcc 0.9655 \n",
      "epoch 7 mse 0.0482 vmse 0.0370 vmae 0.1487 vpcc 0.9683 \n",
      "epoch 8 mse 0.0860 vmse 0.0322 vmae 0.1305 vpcc 0.9671 \n",
      "epoch 9 mse 0.0436 vmse 0.0461 vmae 0.1608 vpcc 0.9534 \n",
      "epoch 10 mse 0.0487 vmse 0.0457 vmae 0.1643 vpcc 0.9553 \n",
      "epoch 11 mse 0.0745 vmse 0.0360 vmae 0.1436 vpcc 0.9576 \n",
      "epoch 12 mse 0.0610 vmse 0.0214 vmae 0.1080 vpcc 0.9708 \n",
      "epoch 13 mse 0.0313 vmse 0.0201 vmae 0.1054 vpcc 0.9746 \n",
      "epoch 14 mse 0.0255 vmse 0.0296 vmae 0.1317 vpcc 0.9724 \n",
      "epoch 15 mse 0.0537 vmse 0.0334 vmae 0.1436 vpcc 0.9702 \n",
      "epoch 16 mse 0.0379 vmse 0.0287 vmae 0.1310 vpcc 0.9696 \n",
      "epoch 17 mse 0.0213 vmse 0.0197 vmae 0.1039 vpcc 0.9706 \n",
      "epoch 18 mse 0.0288 vmse 0.0129 vmae 0.0840 vpcc 0.9787 \n",
      "epoch 19 mse 0.0292 vmse 0.0153 vmae 0.0911 vpcc 0.9787 \n",
      "epoch 20 mse 0.0437 vmse 0.0173 vmae 0.0961 vpcc 0.9785 \n",
      "epoch 21 mse 0.0274 vmse 0.0235 vmae 0.1138 vpcc 0.9780 \n",
      "epoch 22 mse 0.0297 vmse 0.0334 vmae 0.1399 vpcc 0.9746 \n",
      "epoch 23 mse 0.0653 vmse 0.0354 vmae 0.1476 vpcc 0.9727 \n",
      "epoch 24 mse 0.0541 vmse 0.0292 vmae 0.1317 vpcc 0.9739 \n",
      "epoch 25 mse 0.0150 vmse 0.0244 vmae 0.1181 vpcc 0.9735 \n",
      "epoch 26 mse 0.0456 vmse 0.0204 vmae 0.1072 vpcc 0.9741 \n",
      "epoch 27 mse 0.0317 vmse 0.0206 vmae 0.1158 vpcc 0.9713 \n",
      "epoch 28 mse 0.0340 vmse 0.0223 vmae 0.1231 vpcc 0.9691 \n",
      "epoch 29 mse 0.0436 vmse 0.0206 vmae 0.1169 vpcc 0.9688 \n",
      "epoch 30 mse 0.0385 vmse 0.0225 vmae 0.1167 vpcc 0.9666 \n",
      "epoch 31 mse 0.0535 vmse 0.0238 vmae 0.1175 vpcc 0.9691 \n",
      "epoch 32 mse 0.0419 vmse 0.0243 vmae 0.1192 vpcc 0.9733 \n",
      "epoch 33 mse 0.0400 vmse 0.0249 vmae 0.1194 vpcc 0.9763 \n",
      "epoch 34 mse 0.0298 vmse 0.0259 vmae 0.1217 vpcc 0.9770 \n",
      "epoch 35 mse 0.0364 vmse 0.0253 vmae 0.1198 vpcc 0.9782 \n",
      "epoch 36 mse 0.0246 vmse 0.0249 vmae 0.1199 vpcc 0.9785 \n",
      "Epoch 00036: reducing learning rate of group 0 to 5.0000e-04.\n",
      "epoch 37 mse 0.0328 vmse 0.0241 vmae 0.1158 vpcc 0.9801 \n",
      "epoch 38 mse 0.0342 vmse 0.0228 vmae 0.1108 vpcc 0.9815 \n"
     ]
    }
   ],
   "source": [
    "def training_test(model,data_train, n_epochs=100, patience=10):\n",
    "    batch_size = 1024\n",
    "    test_set  = Dataset_from_pd(data_train, drug_data, cell_data)\n",
    "    test_dl = DataLoader(test_set, batch_size=batch_size)    \n",
    "    print(df_test)\n",
    "    optimizer = torch.optim.Adam(model.hidden.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    mae_fn = torch.nn.L1Loss()\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    early_stopper = EarlyStopper(patience=patience)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, verbose=True, patience=patience//2, min_lr=1e-7)\n",
    "\n",
    "    epoch_number = 0\n",
    "    EPOCHS = n_epochs\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.hidden.train(True)\n",
    "        model = model.to(device=device)\n",
    "        avg_loss = train_one_epoch(model, epoch_number, \"writer\", test_dl, optimizer, loss_fn, device, 0, print_every=1)\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        running_MAE = 0.\n",
    "        running_PCC = 0.\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(test_dl):\n",
    "\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs = vinputs.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "                vx = voutputs - torch.mean(voutputs)\n",
    "                vy = vlabels - torch.mean(vlabels)\n",
    "                cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "                running_PCC += cost.item()\n",
    "\n",
    "                mae_loss = mae_fn(voutputs, vlabels)\n",
    "                running_MAE += mae_loss.item()\n",
    "\n",
    "        avg_vMAE = running_MAE/(i+1)\n",
    "        avg_vPCC = running_PCC/(i+1)\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('epoch {} mse {:.{round}f} vmse {:.{round}f} vmae {:.{round}f} vpcc {:.{round}f} '.format(epoch_number+1, avg_loss, avg_vloss, avg_vMAE, avg_vPCC, round=4))\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if early_stopper.early_stop(avg_vloss):             \n",
    "            break\n",
    "        epoch_number += 1\n",
    "\n",
    "training_test(model, df_test,n_epochs=300, patience=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
