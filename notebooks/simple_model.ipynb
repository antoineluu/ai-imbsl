{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_pp import replace_cell_names_with_id\n",
    "from utils_pp import Encoder\n",
    "from utils_pp import EarlyStopper\n",
    "from utils_pp import Dataset_from_pd\n",
    "from utils_pp import train_one_epoch\n",
    "from utils_pp import AE_DNN\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39,)\n",
      "(32,)\n",
      "(31,)\n",
      "oneil 13.13536 (234560, 6) \n",
      " cell_line      object\n",
      "drugA_name     object\n",
      "drugB_name     object\n",
      "drugA_conc    float64\n",
      "drugB_conc    float64\n",
      "target        float64\n",
      "dtype: object\n",
      "drug_feat 0.427236 (42, 2412)\n",
      "cell_feat 0.686136 (32, 5011)\n"
     ]
    }
   ],
   "source": [
    "columns = [\"cell_line\", \"drugA_name\", \"drugB_name\", \"drugA_conc\", \"drugB_conc\", \"target\"]\n",
    "data_train = pd.read_csv(\"../data_raw/oneil.csv\", usecols=(1,2,3,4,5,12)).iloc[:,[0,1,3,2,4,5]].set_axis(columns, axis=1)\n",
    "data_test = pd.read_csv(\"../data/test_yosua.csv\").set_axis(columns + [\"std\"], axis=1).convert_dtypes()\n",
    "print(data_train.cell_line.drop_duplicates().shape)\n",
    "data_train = replace_cell_names_with_id(dataframe=data_train, mapping_file=\"../data/mappingccl.csv\")\n",
    "data_test = replace_cell_names_with_id(dataframe=data_test, mapping_file=\"../data/mappingccl.csv\")\n",
    "drug_data = pd.read_pickle(\"../data/drug_data.pkl.compress\", compression=\"gzip\")\n",
    "cell_data = pd.read_pickle(\"../data/cell_line_data.pkl.compress\", compression=\"gzip\")\n",
    "print(data_train.cell_line.drop_duplicates().shape)\n",
    "data_train = data_train[data_train.cell_line.isin(cell_data.index)]\n",
    "print(data_train.cell_line.drop_duplicates().shape)\n",
    "\n",
    "df_train, df_val = train_test_split(data_train, test_size=0.2, shuffle=True, random_state=42)\n",
    "df_test = data_test\n",
    "\n",
    "cell_data = cell_data[cell_data.index.isin(pd.concat([df_train.cell_line, df_test.cell_line]))]\n",
    "drug_data = drug_data[drug_data.index.isin(pd.concat([df_train.drugA_name, df_train.drugB_name,df_test.drugA_name, df_test.drugB_name]))]\n",
    "print(\"oneil\", df_train.memory_usage().sum()/1e6, df_train.shape,\"\\n\", df_train.dtypes)\n",
    "print(\"drug_feat\", drug_data.memory_usage().sum()/1e6, drug_data.shape)\n",
    "print(\"cell_feat\", cell_data.memory_usage().sum()/1e6, cell_data.shape)\n",
    "DRUG_LENGTH = drug_data.shape[1]\n",
    "CELL_LENGTH = cell_data.shape[1]\n",
    "EMBED_SIZE = 770"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AE_DNN([EMBED_SIZE,256,256,256, 128,128,128,64,64,64], drug_length=DRUG_LENGTH, cell_length=CELL_LENGTH)\n",
    "model.load_state_dict(torch.load(\"../models/ae_dnn_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data_train, data_val, L, n_epochs=100):\n",
    "    batch_size = 1024\n",
    "    train_set  = Dataset_from_pd(data_train, drug_data, cell_data)\n",
    "    val_set = Dataset_from_pd(data_val, drug_data, cell_data)\n",
    "    test_set  = Dataset_from_pd(df_test, drug_data, cell_data)\n",
    "    train_dl = DataLoader(train_set, batch_size=batch_size)\n",
    "    xi, yi = next(iter(train_dl))\n",
    "    val_dl = DataLoader(val_set, batch_size=batch_size)\n",
    "    test_dl = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "    model = AE_DNN([770,256,256,256, 128,128,128,64,64,64], DRUG_LENGTH, CELL_LENGTH)\n",
    "    model.drug_encoder = Encoder(model.drug_encoder.h_sizes)\n",
    "    model.drug_encoder.encoder.load_state_dict(torch.load(\"../models/drug_encoder.pt\"))\n",
    "    model.cell_encoder = Encoder(model.cell_encoder.h_sizes)\n",
    "    model.cell_encoder.encoder.load_state_dict(torch.load(\"../models/cell_encoder.pt\"))\n",
    "    model.drug_encoder.eval()\n",
    "    model.cell_encoder.eval()\n",
    "    # print(summary(model.to(\"cuda\"), 770))\n",
    "    optimizer = torch.optim.Adam(model.hidden.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    mae_fn = torch.nn.L1Loss()\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    early_stopper = EarlyStopper(patience=10)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, verbose=True, patience=5, min_lr=1e-7)\n",
    "\n",
    "    epoch_number = 0\n",
    "    EPOCHS = n_epochs\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.hidden.train(True)\n",
    "        model = model.to(device=device)\n",
    "        avg_loss = train_one_epoch(model, epoch_number, \"writer\", train_dl, optimizer, loss_fn, device, 0)\n",
    "        # avg_loss= 0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        running_MAE = 0.\n",
    "        running_PCC = 0.\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_dl):\n",
    "\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs = vinputs.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "                vx = voutputs - torch.mean(voutputs)\n",
    "                vy = vlabels - torch.mean(vlabels)\n",
    "                cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "                running_PCC += cost.item()\n",
    "\n",
    "                mae_loss = mae_fn(voutputs, vlabels)\n",
    "                running_MAE += mae_loss.item()\n",
    "\n",
    "        avg_vMAE = running_MAE/(i+1)\n",
    "        avg_vPCC = running_PCC/(i+1)\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('epoch {} mse {:.{round}f} vmse {:.{round}f} vmae {:.{round}f} vpcc {:.{round}f} '.format(epoch_number+1, avg_loss, avg_vloss, avg_vMAE, avg_vPCC, round=4))\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if early_stopper.early_stop(avg_vloss):             \n",
    "            break\n",
    "        epoch_number += 1\n",
    "    \n",
    "    model.eval()\n",
    "    pcc_test = 0\n",
    "    running_MAE_test =0\n",
    "    running_MSE_test=0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dl):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(inputs)\n",
    "            loss_test = loss_fn(outputs, labels).item()\n",
    "            mae_test = mae_fn(outputs, labels).item()\n",
    "            vx = voutputs - torch.mean(outputs)\n",
    "            vy = vlabels - torch.mean(labels)\n",
    "            cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "            pcc_test += cost.item()\n",
    "            running_MAE_test += mae_test\n",
    "            running_MSE_test += loss_test\n",
    "    loss_test = running_MSE_test/(i+1)\n",
    "    mae_test =running_MAE_test/(i+1)\n",
    "    cost = pcc_test/(i+1)\n",
    "    print([avg_vloss, avg_vMAE, avg_vPCC, loss_test, mae_test, cost])\n",
    "    L.append([avg_vloss, avg_vMAE, avg_vPCC, loss_test, mae_test, cost, outputs.to(\"cpu\").numpy().reshape(-1)])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LPO/LTO cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs = data_train.loc[:,[\"drugA_name\",\"drugB_name\"]].drop_duplicates()\n",
    "unique_triplets = data_train.loc[:,[\"cell_line\", \"drugA_name\",\"drugB_name\"]].drop_duplicates()\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "batch_size = 2048\n",
    "\n",
    "L_leave_triplet = []\n",
    "for train_index, val_index in kf.split(unique_triplets):\n",
    "    train_unique_triplets = unique_triplets.iloc[train_index]\n",
    "    val_unique_triplets =unique_triplets.iloc[val_index]\n",
    "    combined_train = train_unique_triplets.loc[:,\"cell_line\"].str.cat([train_unique_triplets.loc[:,\"drugA_name\"], train_unique_triplets.loc[:,\"drugB_name\"]], sep= \" + \")\n",
    "    combined_val = val_unique_triplets.loc[:,\"cell_line\"].str.cat([val_unique_triplets.loc[:,\"drugA_name\"], val_unique_triplets.loc[:,\"drugB_name\"]], sep= \" + \")\n",
    "    \n",
    "    data_train = df_train[data_train.loc[:,\"cell_line\"].str.cat([data_train.loc[:,\"drugA_name\"],data_train.loc[:,\"drugB_name\"]],sep=\" + \").isin(combined_train)]\n",
    "    data_val = df_train[data_train.loc[:,\"cell_line\"].str.cat([data_train.loc[:,\"drugA_name\"],data_train.loc[:,\"drugB_name\"]],sep=\" + \").isin(combined_val)]\n",
    "    print(data_train.shape, data_val.shape)\n",
    "    training(data_train, data_val, L_leave_triplet)\n",
    "L_leave_pair = []\n",
    "for train_index, val_index in kf.split(unique_pairs):\n",
    "    train_unique_pairs = unique_pairs.iloc[train_index]\n",
    "    val_unique_pairs =unique_pairs.iloc[val_index]\n",
    "    combined_train = train_unique_pairs.loc[:,\"drugA_name\"].str.cat(train_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "    combined_val = val_unique_pairs.loc[:,\"drugA_name\"].str.cat(val_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "    \n",
    "    data_train = df_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_train)]\n",
    "    data_val = df_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_val)]\n",
    "    print(data_train.shape, data_val.shape)\n",
    "    training(data_train, data_val, L_leave_pair)\n",
    "\n",
    "L_leave_pair_records = np.empty((5,4))\n",
    "for i in range(5):\n",
    "    for j in range(4): L_leave_pair_records[i,j]=L_leave_pair[i][j]\n",
    "np.save(\"Leave_pair_records\", L_leave_pair_records)\n",
    "new_array = np.load(\"Leave_pair_records.npy\")\n",
    "print(new_array)\n",
    "L_leave_triplet_records = np.empty((5,4))\n",
    "for i in range(5):\n",
    "    for j in range(4): L_leave_triplet_records[i,j]=L_leave_triplet[i][j]\n",
    "np.save(\"Leave_triplet_records\", L_leave_triplet_records)\n",
    "new_array = np.load(\"Leave_triplet_records.npy\")\n",
    "print(new_array)\n",
    "\n",
    "test_pair_outputs = np.empty((5,24))\n",
    "for i in range(5):\n",
    "    for j in range(24): test_pair_outputs[i,j]=L_leave_pair[i][6][j]\n",
    "np.save(\"test_pair_outputs\", test_pair_outputs)\n",
    "new_array = np.load(\"test_pair_outputs.npy\")\n",
    "print(new_array)\n",
    "test_triplet_outputs = np.empty((5,24))\n",
    "for i in range(5):\n",
    "    for j in range(24): test_triplet_outputs[i,j]=L_leave_triplet[i][6][j]\n",
    "np.save(\"test_triplet_outputs\", test_triplet_outputs)\n",
    "new_array = np.load(\"test_triplet_outputs.npy\")\n",
    "print(new_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL TRAINING LPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234400, 6) (58800, 6)\n",
      "\n",
      "\n",
      "\n",
      "epoch 1 mse 0.0000 vmse 0.6364 vmae 0.7294 vpcc -0.0193 \n",
      "[0.6364419223933384, 0.7294164741861409, -0.01933403072501372, 1.4286596775054932, 1.0724239349365234, -0.27591803669929504]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AE_DNN(\n",
       "  (drug_encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=2412, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cell_encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=5011, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (hidden): ModuleList(\n",
       "    (0): Linear(in_features=770, out_features=256, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Dropout(p=0.1, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): Dropout(p=0.1, inplace=False)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (13): Dropout(p=0.1, inplace=False)\n",
       "    (14): ReLU()\n",
       "    (15): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (16): Dropout(p=0.1, inplace=False)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (19): Dropout(p=0.1, inplace=False)\n",
       "    (20): ReLU()\n",
       "    (21): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (22): Dropout(p=0.1, inplace=False)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (25): Dropout(p=0.1, inplace=False)\n",
       "    (26): ReLU()\n",
       "    (27): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_leave_pair=[]\n",
    "unique_pairs = data_train.loc[:,[\"drugA_name\",\"drugB_name\"]].drop_duplicates()\n",
    "train_unique_pairs, val_unique_pairs = train_test_split(unique_pairs, test_size=0.2, random_state=42)\n",
    "combined_train = train_unique_pairs.loc[:,\"drugA_name\"].str.cat(train_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "combined_val = val_unique_pairs.loc[:,\"drugA_name\"].str.cat(val_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "\n",
    "df_train = data_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_train)]\n",
    "df_val = data_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_val)]\n",
    "print(df_train.shape, df_val.shape)\n",
    "training(df_train,  df_val, L_leave_pair, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL TRAINING LTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "epoch 1 mse 0.0887 vmse 0.0884 vmae 0.2510 vpcc 0.4438 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m L_full_train_ep \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m df_train, df_val \u001b[39m=\u001b[39m train_test_split(data_train, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m training(df_train, df_val, L_full_train_ep, \u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[62], line 36\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(data_train, data_val, L, n_epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m model\u001b[39m.\u001b[39mhidden\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m---> 36\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(model, epoch_number, \u001b[39m\"\u001b[39;49m\u001b[39mwriter\u001b[39;49m\u001b[39m\"\u001b[39;49m, train_dl, optimizer, loss_fn, device, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m# statistics for batch normalization.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\ai-imbsl\\notebooks\\utils_pp.py:44\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, epoch_index, tb_writer, training_loader, optimizer, loss_fn, device, L1, verbose)\u001b[0m\n\u001b[0;32m     42\u001b[0m last_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m     43\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 44\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(training_loader):\n\u001b[0;32m     45\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     46\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\ai-imbsl\\notebooks\\utils_pp.py:158\u001b[0m, in \u001b[0;36mDataset_from_pd.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    154\u001b[0m drug_B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrug_feat[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrug_mapping[combi[\u001b[39m2\u001b[39m]]]\n\u001b[0;32m    155\u001b[0m cell_line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_feat[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_mapping[combi[\u001b[39m0\u001b[39m]]]\n\u001b[1;32m--> 158\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mconcatenate([drug_A, drug_B, cell_line, combi[\u001b[39m3\u001b[39;49m:\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m\"\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m\"\u001b[39;49m)], dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m\"\u001b[39;49m), combi[\u001b[39m5\u001b[39m:\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "L_full_train_ep = []\n",
    "df_train, df_val = train_test_split(data_train, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "model = training(df_train, df_val, L_full_train_ep, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL training without validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "time = str(datetime.now()).replace(\" \", \"_\").replace(\":\",\"_\")\n",
    "time = \"model_finetune\"\n",
    "path  =\"../models/ae_dnn_{}.pt\".format(time)\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning with testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "     cell_line   drugA_name    drugB_name  drugA_conc  drugB_conc    target  \\\n",
      "0   ACH-000768       GW9662      PD168393       2.975      4.0675   1.02498   \n",
      "1   ACH-000768       GW9662      PD168393        5.95       8.135  0.788251   \n",
      "2   ACH-000768       GW9662      PD168393       8.925     12.2025  0.711825   \n",
      "3   ACH-000768       GW9662      PD168393        11.9       16.27  0.542289   \n",
      "4   ACH-000768       GW9662  Rocilinostat       2.975       4.395  0.970456   \n",
      "5   ACH-000768       GW9662  Rocilinostat        5.95        8.79   0.79969   \n",
      "6   ACH-000768       GW9662  Rocilinostat       8.925      13.185  0.523822   \n",
      "7   ACH-000768       GW9662  Rocilinostat        11.9       17.58  0.211632   \n",
      "8   ACH-000768       GW9662   Saracatinib       2.975      0.0125  1.951693   \n",
      "9   ACH-000768       GW9662   Saracatinib        5.95       0.025  1.492152   \n",
      "10  ACH-000768       GW9662   Saracatinib       8.925      0.0375  1.485964   \n",
      "11  ACH-000768       GW9662   Saracatinib        11.9        0.05  1.414594   \n",
      "12  ACH-000768     PD168393  Rocilinostat      4.0675       4.395  0.362846   \n",
      "13  ACH-000768     PD168393  Rocilinostat       8.135        8.79   0.01175   \n",
      "14  ACH-000768     PD168393  Rocilinostat     12.2025      13.185  0.004133   \n",
      "15  ACH-000768     PD168393  Rocilinostat       16.27       17.58  0.010231   \n",
      "16  ACH-000768     PD168393   Saracatinib      4.0675      0.0125  1.504466   \n",
      "17  ACH-000768     PD168393   Saracatinib       8.135       0.025  1.161032   \n",
      "18  ACH-000768     PD168393   Saracatinib     12.2025      0.0375  1.021711   \n",
      "19  ACH-000768     PD168393   Saracatinib       16.27        0.05  1.078313   \n",
      "20  ACH-000768  Saracatinib  Rocilinostat      0.0125       4.395  1.508316   \n",
      "21  ACH-000768  Saracatinib  Rocilinostat       0.025        8.79  1.374209   \n",
      "22  ACH-000768  Saracatinib  Rocilinostat      0.0375      13.185  1.138154   \n",
      "23  ACH-000768  Saracatinib  Rocilinostat        0.05       17.58  0.964295   \n",
      "\n",
      "         std  \n",
      "0   0.213373  \n",
      "1   0.105838  \n",
      "2   0.177698  \n",
      "3   0.188043  \n",
      "4    0.20208  \n",
      "5   0.023706  \n",
      "6   0.183895  \n",
      "7   0.025602  \n",
      "8   6.440154  \n",
      "9   1.795536  \n",
      "10  1.266538  \n",
      "11  0.574059  \n",
      "12  0.082831  \n",
      "13  0.000102  \n",
      "14  0.000008  \n",
      "15  0.000071  \n",
      "16   0.61118  \n",
      "17  0.157102  \n",
      "18  0.210985  \n",
      "19  0.134132  \n",
      "20  0.624652  \n",
      "21   0.68497  \n",
      "22  0.322111  \n",
      "23   0.22007  \n",
      "epoch 2  vmse 0.0357 vmae 0.1436 vpcc -0.4407  test_mse 0.8295 test_mae 0.7936 test_pcc 0.7017\n"
     ]
    }
   ],
   "source": [
    "def training_test(model,data_train, data_val,n_epochs=100, patience=10):\n",
    "    batch_size = 1024\n",
    "    val_set = Dataset_from_pd(data_val, drug_data, cell_data)\n",
    "    val_dl = DataLoader(val_set, batch_size=batch_size)\n",
    "    test_set  = Dataset_from_pd(data_train, drug_data, cell_data)\n",
    "    test_dl = DataLoader(test_set, batch_size=batch_size)    \n",
    "    print(df_test)\n",
    "    optimizer = torch.optim.Adam(model.hidden.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    mae_fn = torch.nn.L1Loss()\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    early_stopper = EarlyStopper(patience=patience)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, verbose=True, patience=patience//2, min_lr=1e-7)\n",
    "\n",
    "    epoch_number = 0\n",
    "    EPOCHS = n_epochs\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.hidden.train(True)\n",
    "        model = model.to(device=device)\n",
    "        # avg_loss = train_one_epoch(model, epoch_number, \"writer\", test_dl, optimizer, loss_fn, device, 0, print_every=1)\n",
    "        avg_loss =0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        running_MAE = 0.\n",
    "        running_PCC = 0.\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(test_dl):\n",
    "\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs = vinputs.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "                vx = voutputs - torch.mean(voutputs)\n",
    "                vy = vlabels - torch.mean(vlabels)\n",
    "                cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "                running_PCC += cost.item()\n",
    "\n",
    "                mae_loss = mae_fn(voutputs, vlabels)\n",
    "                running_MAE += mae_loss.item()\n",
    "\n",
    "        avg_vMAE = running_MAE/(i+1)\n",
    "        avg_vPCC = running_PCC/(i+1)\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if early_stopper.early_stop(avg_vloss):             \n",
    "            break\n",
    "        epoch_number += 1\n",
    "\n",
    "        model.eval()\n",
    "        pcc_test = 0\n",
    "        running_MAE_test =0\n",
    "        running_MSE_test=0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_dl):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(inputs)\n",
    "                loss_test = loss_fn(outputs, labels).item()\n",
    "                mae_test = mae_fn(outputs, labels).item()\n",
    "                vx = voutputs - torch.mean(outputs)\n",
    "                vy = vlabels - torch.mean(labels)\n",
    "                cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "                pcc_test += cost.item()\n",
    "                running_MAE_test += mae_test\n",
    "                running_MSE_test += loss_test\n",
    "        loss_test = running_MSE_test/(i+1)\n",
    "        mae_test =running_MAE_test/(i+1)\n",
    "        cost = pcc_test/(i+1)\n",
    "        print('epoch {}  vmse {:.{round}f} vmae {:.{round}f} vpcc {:.{round}f}  test_mse {:.{round}f} test_mae {:.{round}f} test_pcc {:.{round}f}'.format(epoch_number+1,loss_test,mae_test,cost,  avg_vloss, avg_vMAE, avg_vPCC, round=4))\n",
    "\n",
    "        # print([loss_test, mae_test, pcc_test,avg_vloss, avg_vMAE, avg_vPCC, ])\n",
    "L_leave_pair=[]\n",
    "unique_pairs = data_train.loc[:,[\"drugA_name\",\"drugB_name\"]].drop_duplicates()\n",
    "train_unique_pairs, val_unique_pairs = train_test_split(unique_pairs, test_size=0.2, random_state=42)\n",
    "combined_train = train_unique_pairs.loc[:,\"drugA_name\"].str.cat(train_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "combined_val = val_unique_pairs.loc[:,\"drugA_name\"].str.cat(val_unique_pairs.loc[:,\"drugB_name\"], sep= \" + \")\n",
    "\n",
    "df_train = data_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_train)]\n",
    "df_val = data_train[data_train.loc[:,\"drugA_name\"].str.cat(data_train.loc[:,\"drugB_name\"],sep=\" + \").isin(combined_val)]\n",
    "training_test(model, df_test,df_val,n_epochs=1, patience=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
