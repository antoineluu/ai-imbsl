{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_pp import replace_cell_names_with_id\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_feat 5.213636 (538, 2412)\n",
      "drug_feat 1.324686 (135, 2412)\n",
      "cell_feat 29.111364 (1450, 5011)\n"
     ]
    }
   ],
   "source": [
    "# columns = [\"cell_line\", \"drugA_name\", \"drugB_name\", \"drugA_conc\", \"drugB_conc\", \"target\"]\n",
    "# df_train = pd.read_csv(\"../data_raw/oneil.csv\", usecols=(1,2,3,4,5,12)).iloc[:,[0,1,3,2,4,5]].set_axis(columns, axis=1)\n",
    "# df_train[\"cell_line\"]\n",
    "# df_test = pd.read_csv(\"../data/test_yosua.csv\").set_axis(columns + [\"std\"], axis=1).convert_dtypes()\n",
    "\n",
    "drug_data = pd.read_pickle(\"../data/drug_data.pkl.compress\", compression=\"gzip\")\n",
    "cell_data = pd.read_pickle(\"../data/cell_line_data.pkl.compress\", compression=\"gzip\")\n",
    "drug_data_val = drug_data.copy()\n",
    "X_train_drug, X_val_drug, y_train_drug, y_val_drug = train_test_split(drug_data, drug_data_val, test_size=0.2, shuffle=True)\n",
    "# df_train = replace_cell_names_with_id(dataframe=df_train, mapping_file=\"../data/mappingccl.csv\")\n",
    "# df_test = replace_cell_names_with_id(dataframe=df_test, mapping_file=\"../data/mappingccl.csv\")\n",
    "# df_train = df_train[df_train.cell_line.isin(cell_data.index)]\n",
    "# df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True)\n",
    "# cell_data = cell_data[cell_data.index.isin(pd.concat([df_train.cell_line, df_test.cell_line]))]\n",
    "# print(\"oneil\", df_train.memory_usage().sum()/1e6, df_train.shape,\"\\n\", df_train.dtypes)\n",
    "print(\"drug_feat\", X_train_drug.memory_usage().sum()/1e6, X_train_drug.shape)\n",
    "print(\"drug_feat\", X_val_drug.memory_usage().sum()/1e6, X_val_drug.shape)\n",
    "print(\"cell_feat\", cell_data.memory_usage().sum()/1e6, cell_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2412]) torch.Size([64, 2412])\n"
     ]
    }
   ],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.dataset = data.to_numpy()\n",
    "        self.labels = labels.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx], self.labels[idx]\n",
    "                \n",
    "train_set_drug  = dataset(X_train_drug, y_train_drug)\n",
    "val_set_drug  = dataset(X_val_drug, y_val_drug)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=64, shuffle=True)\n",
    "val_dl_drug = DataLoader(val_set_drug, batch_size=64, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "print(xi.shape, yi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2412]) torch.Size([128, 2412])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a PyTorch class\n",
    "# 28*28 ==> 9 ==> 28*28\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, h_sizes, dropout=0.2):\n",
    "        super().__init__()\n",
    "         \n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[0], h_sizes[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[1], h_sizes[2]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[2], h_sizes[3])\n",
    "        )\n",
    "         \n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # The Sigmoid activation function\n",
    "        # outputs the value between 0 and 1\n",
    "        # 9 ==> 784\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[3], h_sizes[4]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[4], h_sizes[5]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[5], h_sizes[0])\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "ae = AE([xi.shape[1],512,512,256,512,512])\n",
    "yi = ae.forward(xi)\n",
    "print(xi.shape, yi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, epoch_index, tb_writer, training_loader, optimizer, loss_fn, device, verbose=False):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    model = model.to(device)\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # last_loss =  loss.item() / inputs.shape[0]  # loss per sample\n",
    "        # if verbose:print('  batch {} loss: {}'.format(i + 1, last_loss), outputs[0][0].item(), labels[0][0].item())\n",
    "        # tb_x = epoch_index * len(training_loader) + i + 1\n",
    "        # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "    \n",
    "    return running_loss / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9189367492993673 valid 0.9363616704940796\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 68\u001b[0m\n\u001b[0;32m     64\u001b[0m         epoch_number \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_vloss\n\u001b[1;32m---> 68\u001b[0m objective()\n\u001b[0;32m     70\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     71\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dl_drug):\n",
      "Cell \u001b[1;32mIn[126], line 49\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     47\u001b[0m vinputs \u001b[39m=\u001b[39m vinputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     48\u001b[0m vlabels \u001b[39m=\u001b[39m vlabels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 49\u001b[0m voutputs \u001b[39m=\u001b[39m ae(vinputs)\n\u001b[0;32m     50\u001b[0m vloss \u001b[39m=\u001b[39m loss_fn(voutputs, vlabels) \u001b[39m/\u001b[39m vinputs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     51\u001b[0m running_vloss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m vloss\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[103], line 37\u001b[0m, in \u001b[0;36mAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     36\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[1;32m---> 37\u001b[0m     decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(encoded)\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m decoded\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "batch_size=256\n",
    "train_set_drug  = dataset(X_train_drug, y_train_drug)\n",
    "val_set_drug  = dataset(X_val_drug, y_val_drug)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=batch_size, shuffle=True)\n",
    "val_dl_drug = DataLoader(val_set_drug, batch_size=1, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "\n",
    "def objective(trial=None):\n",
    "    if trial is None:\n",
    "        lr = 1e-3\n",
    "        dropout=0.2\n",
    "        weight_decay=1e-4\n",
    "    else:\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-2, log=True)\n",
    "        lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.3) \n",
    "    ae = AE([xi.shape[1],1024,512,256,512,1024], dropout=dropout)\n",
    "    # print(summary(ae.to(\"cuda\"), xi.shape))\n",
    "    early_stopper = EarlyStopper(patience=30)\n",
    "    optimizer = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, verbose=True, patience=20, min_lr=1e-7)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    epoch_number = 0\n",
    "\n",
    "    EPOCHS = 500\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        ae.train(True)\n",
    "        ae = ae.to(device=device)\n",
    "\n",
    "        avg_loss = train_one_epoch(ae, epoch_number, \"writer\", train_dl_drug, optimizer, loss_fn, device=device)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        # Set the ae to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        ae.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_dl_drug):\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs = vinputs.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "                voutputs = ae(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels) / vinputs.shape[0]\n",
    "                running_vloss += vloss\n",
    "        avg_vloss = running_vloss / (i + 1) \n",
    "\n",
    "        if epoch_number%10==9:print('EPOCH {}:'.format(epoch_number + 1),'LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if early_stopper.early_stop(avg_loss):             \n",
    "            break\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(avg_vloss, epoch_number)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        epoch_number += 1\n",
    "\n",
    "    return avg_vloss\n",
    "\n",
    "objective()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_dl_drug):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = ae(inputs)\n",
    "with open(\"output.txt\", mode=\"w\") as f: \n",
    "    [f.write(\n",
    "        str(outputs[0][i].to(\"cpu\").numpy().round(3))+\"   \"+\n",
    "        str(inputs[0][i].to(\"cpu\").numpy().round(3))+\"\\n\") for i in range(len(outputs[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1af5fcf1970>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "inputs_np = inputs.to(\"cpu\").numpy().round(3)\n",
    "outputs_np = outputs.to(\"cpu\").numpy().round(3)\n",
    "mask = inputs_np<0.5\n",
    "\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.scatter(inputs_np[mask][:100], outputs_np[mask][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 15:59:01,277] A new study created in memory with name: no-name-7928a3f7-6d42-486d-9ef1-79f5cbcbc7bc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8964327971140543 valid 0.9187698364257812\n",
      "EPOCH 20: LOSS train 0.735660175482432 valid 0.8696612119674683\n",
      "EPOCH 30: LOSS train 0.6578816970189413 valid 0.841172456741333\n",
      "EPOCH 40: LOSS train 0.5808011690775553 valid 0.820204496383667\n",
      "EPOCH 50: LOSS train 0.5016927619775137 valid 0.8065064549446106\n",
      "EPOCH 60: LOSS train 0.4470178186893463 valid 0.792937695980072\n",
      "EPOCH 70: LOSS train 0.40131019552548725 valid 0.7838071584701538\n",
      "EPOCH 80: LOSS train 0.33746330936749774 valid 0.7746938467025757\n",
      "EPOCH 90: LOSS train 0.2870992024739583 valid 0.7652873992919922\n",
      "EPOCH 100: LOSS train 0.2520986000696818 valid 0.7589479684829712\n",
      "EPOCH 110: LOSS train 0.21655350923538208 valid 0.7564115524291992\n",
      "EPOCH 120: LOSS train 0.1925071726242701 valid 0.7516952157020569\n",
      "EPOCH 130: LOSS train 0.16322414577007294 valid 0.7490956783294678\n",
      "EPOCH 140: LOSS train 0.15513590474923453 valid 0.7476940155029297\n",
      "EPOCH 150: LOSS train 0.13686367372671762 valid 0.7477654218673706\n",
      "EPOCH 160: LOSS train 0.12129672120014827 valid 0.7454369068145752\n",
      "EPOCH 170: LOSS train 0.10734537492195766 valid 0.7446942925453186\n",
      "EPOCH 180: LOSS train 0.10167135546604793 valid 0.7438770532608032\n",
      "EPOCH 190: LOSS train 0.10192689796288808 valid 0.7446224689483643\n",
      "EPOCH 200: LOSS train 0.08543896426757176 valid 0.7416666746139526\n",
      "EPOCH 210: LOSS train 0.07875833412011464 valid 0.7413647174835205\n",
      "EPOCH 220: LOSS train 0.07314243912696838 valid 0.7403231263160706\n",
      "EPOCH 230: LOSS train 0.07135440409183502 valid 0.7394425868988037\n",
      "EPOCH 240: LOSS train 0.06494936098655064 valid 0.7378414869308472\n",
      "EPOCH 250: LOSS train 0.06255896637837093 valid 0.7389729022979736\n",
      "EPOCH 260: LOSS train 0.05897484595576922 valid 0.7375266551971436\n",
      "EPOCH 270: LOSS train 0.05670225992798805 valid 0.7380004525184631\n",
      "EPOCH 280: LOSS train 0.05100616067647934 valid 0.7365856170654297\n",
      "EPOCH 290: LOSS train 0.05102398246526718 valid 0.7358337640762329\n",
      "EPOCH 300: LOSS train 0.04808092614014944 valid 0.7344580888748169\n",
      "EPOCH 310: LOSS train 0.04456238572796186 valid 0.7348489761352539\n",
      "EPOCH 320: LOSS train 0.04453641548752785 valid 0.7345283627510071\n",
      "EPOCH 330: LOSS train 0.03864537055293719 valid 0.7350440621376038\n",
      "EPOCH 340: LOSS train 0.04187974209586779 valid 0.733487606048584\n",
      "EPOCH 350: LOSS train 0.03828267504771551 valid 0.7332189679145813\n",
      "EPOCH 360: LOSS train 0.03748240942756335 valid 0.7333937883377075\n",
      "EPOCH 370: LOSS train 0.03635464360316595 valid 0.732234537601471\n",
      "EPOCH 380: LOSS train 0.036361670742432274 valid 0.7323731780052185\n",
      "EPOCH 390: LOSS train 0.035253641506036125 valid 0.7325578927993774\n",
      "EPOCH 400: LOSS train 0.03422976595660051 valid 0.7317403554916382\n",
      "EPOCH 410: LOSS train 0.032615957160790764 valid 0.7309823632240295\n",
      "EPOCH 420: LOSS train 0.03614353636900584 valid 0.7302196621894836\n",
      "EPOCH 430: LOSS train 0.03245235234498978 valid 0.7297706007957458\n",
      "EPOCH 440: LOSS train 0.03251208861668905 valid 0.7300291657447815\n",
      "EPOCH 450: LOSS train 0.030467564240098 valid 0.72901451587677\n",
      "EPOCH 460: LOSS train 0.02866931880513827 valid 0.7283378839492798\n",
      "EPOCH 470: LOSS train 0.0357675701379776 valid 0.7295905947685242\n",
      "EPOCH 480: LOSS train 0.030265297119816143 valid 0.7280257344245911\n",
      "Epoch 00482: reducing learning rate of group 0 to 2.0271e-04.\n",
      "EPOCH 490: LOSS train 0.02426232894261678 valid 0.7267191410064697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:00:58,622] Trial 0 finished with value: 0.7264183163642883 and parameters: {'weight_decay': 2.0742200279799252e-07, 'learning_rate': 0.0004054200648538034, 'dropout': 0.006690157715080158}. Best is trial 0 with value: 0.7264183163642883.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 0.023595905552307766 valid 0.7264183163642883\n",
      "EPOCH 10: LOSS train 0.951216459274292 valid 0.9843992590904236\n",
      "EPOCH 20: LOSS train 0.9661097923914591 valid 0.9843155145645142\n",
      "EPOCH 30: LOSS train 0.9769763350486755 valid 0.9841616153717041\n",
      "Epoch 00031: reducing learning rate of group 0 to 5.5889e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:01:08,133] Trial 1 finished with value: 0.9839949607849121 and parameters: {'weight_decay': 4.203455077345441e-08, 'learning_rate': 1.117773330334623e-05, 'dropout': 0.0952681856404866}. Best is trial 0 with value: 0.7264183163642883.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 40: LOSS train 0.992885410785675 valid 0.9839949607849121\n",
      "EPOCH 10: LOSS train 0.9817602038383484 valid 0.9846104383468628\n",
      "EPOCH 20: LOSS train 1.0348960955937703 valid 0.9844990968704224\n",
      "EPOCH 30: LOSS train 0.979311486085256 valid 0.9841519594192505\n",
      "EPOCH 40: LOSS train 0.9859962662061056 valid 0.9827821850776672\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.1116e-05.\n",
      "EPOCH 50: LOSS train 0.9563847581545512 valid 0.9793707728385925\n",
      "EPOCH 60: LOSS train 0.9439239700635275 valid 0.9754704236984253\n",
      "EPOCH 70: LOSS train 0.9693473776181539 valid 0.9726516008377075\n",
      "Epoch 00078: reducing learning rate of group 0 to 5.5580e-06.\n",
      "EPOCH 80: LOSS train 0.9469285011291504 valid 0.9708676934242249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:01:28,502] Trial 2 finished with value: 0.9703244566917419 and parameters: {'weight_decay': 4.15138552676534e-05, 'learning_rate': 2.223214019936377e-05, 'dropout': 0.2413583926687849}. Best is trial 0 with value: 0.7264183163642883.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8919724424680074 valid 0.9378631711006165\n",
      "EPOCH 20: LOSS train 0.7777864138285319 valid 0.8827853202819824\n",
      "EPOCH 30: LOSS train 0.7335690855979919 valid 0.8555580973625183\n",
      "EPOCH 40: LOSS train 0.6549035310745239 valid 0.8372163772583008\n",
      "EPOCH 50: LOSS train 0.6154762705167135 valid 0.8238299489021301\n",
      "EPOCH 60: LOSS train 0.57394939661026 valid 0.8137056827545166\n",
      "EPOCH 70: LOSS train 0.5238625804583231 valid 0.8031138181686401\n",
      "EPOCH 80: LOSS train 0.4946559965610504 valid 0.793481171131134\n",
      "EPOCH 90: LOSS train 0.4698922435442607 valid 0.7873779535293579\n",
      "EPOCH 100: LOSS train 0.4195818801720937 valid 0.7816359400749207\n",
      "EPOCH 110: LOSS train 0.3874383866786957 valid 0.775000274181366\n",
      "EPOCH 120: LOSS train 0.35152796904246014 valid 0.7669376134872437\n",
      "EPOCH 130: LOSS train 0.326005220413208 valid 0.7634682059288025\n",
      "EPOCH 140: LOSS train 0.3027735749880473 valid 0.7595819234848022\n",
      "EPOCH 150: LOSS train 0.2903686265150706 valid 0.7560952305793762\n",
      "EPOCH 160: LOSS train 0.26672661304473877 valid 0.7526129484176636\n",
      "EPOCH 170: LOSS train 0.24182156225045523 valid 0.7502630949020386\n",
      "EPOCH 180: LOSS train 0.2283750126759211 valid 0.7489017248153687\n",
      "EPOCH 190: LOSS train 0.21366961797078451 valid 0.7470662593841553\n",
      "EPOCH 200: LOSS train 0.204206849137942 valid 0.745410680770874\n",
      "EPOCH 210: LOSS train 0.1932932734489441 valid 0.745098888874054\n",
      "EPOCH 220: LOSS train 0.18241887787977853 valid 0.7428849339485168\n",
      "EPOCH 230: LOSS train 0.16514284908771515 valid 0.7404677867889404\n",
      "EPOCH 240: LOSS train 0.1620408147573471 valid 0.739734411239624\n",
      "EPOCH 250: LOSS train 0.15635974208513895 valid 0.7389975786209106\n",
      "EPOCH 260: LOSS train 0.15052869419256845 valid 0.7372521758079529\n",
      "EPOCH 270: LOSS train 0.14050684372584024 valid 0.7370405793190002\n",
      "EPOCH 280: LOSS train 0.13471698264280954 valid 0.7360329031944275\n",
      "EPOCH 290: LOSS train 0.12037451316912968 valid 0.7346099615097046\n",
      "EPOCH 300: LOSS train 0.12812136113643646 valid 0.7345526218414307\n",
      "EPOCH 310: LOSS train 0.12274571508169174 valid 0.7331241965293884\n",
      "EPOCH 320: LOSS train 0.1143628532687823 valid 0.7325507998466492\n",
      "EPOCH 330: LOSS train 0.11030027270317078 valid 0.73206627368927\n",
      "EPOCH 340: LOSS train 0.11364121486743291 valid 0.7317461371421814\n",
      "EPOCH 350: LOSS train 0.10142390429973602 valid 0.7319912910461426\n",
      "EPOCH 360: LOSS train 0.09780903408924739 valid 0.7309766411781311\n",
      "EPOCH 370: LOSS train 0.09461903820435207 valid 0.7309693098068237\n",
      "EPOCH 380: LOSS train 0.09572915732860565 valid 0.7302047610282898\n",
      "EPOCH 390: LOSS train 0.0948386862874031 valid 0.7293186783790588\n",
      "EPOCH 400: LOSS train 0.0882595752676328 valid 0.728451669216156\n",
      "EPOCH 410: LOSS train 0.09092836827039719 valid 0.7278311848640442\n",
      "EPOCH 420: LOSS train 0.08352906008561452 valid 0.7277846932411194\n",
      "Epoch 00429: reducing learning rate of group 0 to 1.3139e-04.\n",
      "EPOCH 430: LOSS train 0.0829838290810585 valid 0.7267609238624573\n",
      "EPOCH 440: LOSS train 0.07637704163789749 valid 0.7258407473564148\n",
      "EPOCH 450: LOSS train 0.07316878934701283 valid 0.7254552841186523\n",
      "EPOCH 460: LOSS train 0.07381473481655121 valid 0.7256873846054077\n",
      "Epoch 00463: reducing learning rate of group 0 to 6.5695e-05.\n",
      "EPOCH 470: LOSS train 0.07368133962154388 valid 0.7248638272285461\n",
      "EPOCH 480: LOSS train 0.07277396569649379 valid 0.7246636152267456\n",
      "EPOCH 490: LOSS train 0.06934069097042084 valid 0.7245638370513916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:03:33,424] Trial 3 finished with value: 0.7246367931365967 and parameters: {'weight_decay': 5.898295395253151e-06, 'learning_rate': 0.0002627818048917611, 'dropout': 0.025916582435828884}. Best is trial 3 with value: 0.7246367931365967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 0.06845741967360179 valid 0.7246367931365967\n",
      "EPOCH 10: LOSS train 0.9985620776812235 valid 0.9844756722450256\n",
      "EPOCH 20: LOSS train 1.016975224018097 valid 0.9842193722724915\n",
      "EPOCH 30: LOSS train 0.961137056350708 valid 0.9833748936653137\n",
      "Epoch 00036: reducing learning rate of group 0 to 7.3645e-06.\n",
      "EPOCH 40: LOSS train 0.9730969866116842 valid 0.98159259557724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:03:43,668] Trial 4 finished with value: 0.9805439114570618 and parameters: {'weight_decay': 3.7266483575972175e-06, 'learning_rate': 1.4729021005959363e-05, 'dropout': 0.008743042594170646}. Best is trial 3 with value: 0.7246367931365967.\n",
      "[I 2023-08-24 16:03:43,919] Trial 5 pruned. \n",
      "[I 2023-08-24 16:03:44,200] Trial 6 pruned. \n",
      "[I 2023-08-24 16:03:44,557] Trial 7 pruned. \n",
      "[I 2023-08-24 16:03:44,844] Trial 8 pruned. \n",
      "[I 2023-08-24 16:03:45,126] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9588246941566467 valid 0.9845335483551025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:03:47,843] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8747256000836691 valid 0.9270272254943848\n",
      "EPOCH 20: LOSS train 0.7646091977755228 valid 0.8801366686820984\n",
      "EPOCH 30: LOSS train 0.7038793365160624 valid 0.8500660061836243\n",
      "EPOCH 40: LOSS train 0.6081476012865702 valid 0.8313676714897156\n",
      "EPOCH 50: LOSS train 0.5615212321281433 valid 0.8169065117835999\n",
      "EPOCH 60: LOSS train 0.46607200304667157 valid 0.8064845204353333\n",
      "EPOCH 70: LOSS train 0.43679653604825336 valid 0.7933388948440552\n",
      "EPOCH 80: LOSS train 0.38767583171526593 valid 0.7843006253242493\n",
      "EPOCH 90: LOSS train 0.33839816848436993 valid 0.7754285335540771\n",
      "EPOCH 100: LOSS train 0.3081312378247579 valid 0.7693676352500916\n",
      "EPOCH 110: LOSS train 0.28139880299568176 valid 0.7649153470993042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:04:14,112] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9078738490740458 valid 0.9338170289993286\n",
      "EPOCH 20: LOSS train 0.8005250891049703 valid 0.8890096545219421\n",
      "EPOCH 30: LOSS train 0.7068562110265096 valid 0.8619912266731262\n",
      "EPOCH 40: LOSS train 0.6463988224665324 valid 0.8406268954277039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:04:24,466] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9222878217697144 valid 0.9636880159378052\n",
      "EPOCH 20: LOSS train 0.8707720637321472 valid 0.9342678785324097\n",
      "EPOCH 30: LOSS train 0.8585448265075684 valid 0.9104251265525818\n",
      "EPOCH 40: LOSS train 0.8641272187232971 valid 0.8953218460083008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:04:35,031] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8664694825808207 valid 0.9002593755722046\n",
      "EPOCH 20: LOSS train 0.719032883644104 valid 0.860633134841919\n",
      "EPOCH 30: LOSS train 0.6373923420906067 valid 0.8339985609054565\n",
      "EPOCH 40: LOSS train 0.5355844000975291 valid 0.8143650889396667\n",
      "EPOCH 50: LOSS train 0.4605963130791982 valid 0.8016321659088135\n",
      "EPOCH 60: LOSS train 0.4268210728963216 valid 0.7890103459358215\n",
      "EPOCH 70: LOSS train 0.4153373638788859 valid 0.7804701328277588\n",
      "EPOCH 80: LOSS train 0.34659204880396527 valid 0.7723497152328491\n",
      "EPOCH 90: LOSS train 0.31361572941144306 valid 0.7708794474601746\n",
      "EPOCH 100: LOSS train 0.28423810998598736 valid 0.7650997042655945\n",
      "EPOCH 110: LOSS train 0.26771198709805805 valid 0.7583284378051758\n",
      "EPOCH 120: LOSS train 0.24648148814837137 valid 0.7678614258766174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:05:04,217] Trial 14 pruned. \n",
      "[I 2023-08-24 16:05:04,507] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9149225950241089 valid 0.9238418936729431\n",
      "EPOCH 20: LOSS train 0.8410022258758545 valid 0.8766993880271912\n",
      "EPOCH 30: LOSS train 0.7531596223513285 valid 0.854283332824707\n",
      "EPOCH 40: LOSS train 0.6693061987559 valid 0.8387368321418762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:05:15,444] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9518271287282308 valid 0.9570673108100891\n",
      "EPOCH 20: LOSS train 0.8650016585985819 valid 0.9091325402259827\n",
      "EPOCH 30: LOSS train 0.7935507297515869 valid 0.8853872418403625\n",
      "EPOCH 40: LOSS train 0.7390687465667725 valid 0.8699078559875488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:05:27,197] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8319852550824484 valid 0.916589617729187\n",
      "EPOCH 20: LOSS train 0.7619730830192566 valid 0.8727059960365295\n",
      "EPOCH 30: LOSS train 0.6676624814669291 valid 0.8481240272521973\n",
      "EPOCH 40: LOSS train 0.6438319484392802 valid 0.8292450308799744\n",
      "EPOCH 50: LOSS train 0.5345362226168314 valid 0.8135040998458862\n",
      "EPOCH 60: LOSS train 0.4456770320733388 valid 0.8028504848480225\n",
      "EPOCH 70: LOSS train 0.39529483517011005 valid 0.7880486249923706\n",
      "EPOCH 80: LOSS train 0.34574905037879944 valid 0.7800403833389282\n",
      "EPOCH 90: LOSS train 0.3126489718755086 valid 0.7749796509742737\n",
      "EPOCH 100: LOSS train 0.26464299857616425 valid 0.7678115367889404\n",
      "EPOCH 110: LOSS train 0.24597961703936258 valid 0.763232409954071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:06:01,163] Trial 18 pruned. \n",
      "[I 2023-08-24 16:06:01,703] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8898886640866598 valid 0.9434162378311157\n",
      "EPOCH 20: LOSS train 0.8169260223706564 valid 0.8894577026367188\n",
      "EPOCH 30: LOSS train 0.7374055782953898 valid 0.8653506636619568\n",
      "EPOCH 40: LOSS train 0.6889314850171407 valid 0.8487825393676758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:06:14,009] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9994248946507772 valid 0.9840867519378662\n",
      "EPOCH 20: LOSS train 0.9728068510691324 valid 0.980194091796875\n",
      "EPOCH 30: LOSS train 0.9767831166585287 valid 0.9664987921714783\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "train_set_drug  = dataset(X_train_drug, y_train_drug)\n",
    "val_set_drug  = dataset(X_val_drug, y_val_drug)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=batch_size, shuffle=True)\n",
    "val_dl_drug = DataLoader(val_set_drug, batch_size=1, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'weight_decay': 1.4343876100314803e-06, 'learning_rate': 0.00045871647446227656, 'dropout': 0.05777989426470749}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
