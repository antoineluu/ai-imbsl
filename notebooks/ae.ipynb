{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\ai-imbsl\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "import torchviz\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_pp import replace_cell_names_with_id\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_feat 5.213636 (538, 2412)\n",
      "drug_feat 1.324686 (135, 2412)\n",
      "cell_feat 29.111364 (1450, 5011)\n"
     ]
    }
   ],
   "source": [
    "# columns = [\"cell_line\", \"drugA_name\", \"drugB_name\", \"drugA_conc\", \"drugB_conc\", \"target\"]\n",
    "# df_train = pd.read_csv(\"../data_raw/oneil.csv\", usecols=(1,2,3,4,5,12)).iloc[:,[0,1,3,2,4,5]].set_axis(columns, axis=1)\n",
    "# df_train[\"cell_line\"]\n",
    "# df_test = pd.read_csv(\"../data/test_yosua.csv\").set_axis(columns + [\"std\"], axis=1).convert_dtypes()\n",
    "\n",
    "drug_data = pd.read_pickle(\"../data/drug_data.pkl.compress\", compression=\"gzip\")\n",
    "cell_data = pd.read_pickle(\"../data/cell_line_data.pkl.compress\", compression=\"gzip\")\n",
    "drug_data_val = drug_data.copy()\n",
    "X_train_drug, X_val_drug, y_train_drug, y_val_drug = train_test_split(drug_data, drug_data_val, test_size=0.2, shuffle=True)\n",
    "# df_train = replace_cell_names_with_id(dataframe=df_train, mapping_file=\"../data/mappingccl.csv\")\n",
    "# df_test = replace_cell_names_with_id(dataframe=df_test, mapping_file=\"../data/mappingccl.csv\")\n",
    "# df_train = df_train[df_train.cell_line.isin(cell_data.index)]\n",
    "# df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True)\n",
    "# cell_data = cell_data[cell_data.index.isin(pd.concat([df_train.cell_line, df_test.cell_line]))]\n",
    "# print(\"oneil\", df_train.memory_usage().sum()/1e6, df_train.shape,\"\\n\", df_train.dtypes)\n",
    "print(\"drug_feat\", X_train_drug.memory_usage().sum()/1e6, X_train_drug.shape)\n",
    "print(\"drug_feat\", X_val_drug.memory_usage().sum()/1e6, X_val_drug.shape)\n",
    "print(\"cell_feat\", cell_data.memory_usage().sum()/1e6, cell_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2412]) torch.Size([64, 2412])\n"
     ]
    }
   ],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.dataset = data.to_numpy()\n",
    "        self.labels = labels.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx], self.labels[idx]\n",
    "                \n",
    "train_set_drug  = dataset(X_train_drug, y_train_drug)\n",
    "val_set_drug  = dataset(X_val_drug, y_val_drug)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=64, shuffle=True)\n",
    "val_dl_drug = DataLoader(val_set_drug, batch_size=64, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "print(xi.shape, yi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2412]) torch.Size([64, 2412])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a PyTorch class\n",
    "# 28*28 ==> 9 ==> 28*28\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, h_sizes, dropout=0.2):\n",
    "        super().__init__()\n",
    "         \n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[0], h_sizes[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[1], h_sizes[2]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[2], h_sizes[3])\n",
    "        )\n",
    "         \n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # The Sigmoid activation function\n",
    "        # outputs the value between 0 and 1\n",
    "        # 9 ==> 784\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[3], h_sizes[4]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[4], h_sizes[5]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[5], h_sizes[0])\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "ae = AE([xi.shape[1],512,512,256,512,512])\n",
    "yi = ae.forward(xi)\n",
    "print(xi.shape, yi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(dot.exe:14968): Pango-WARNING **: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Title: MLP autoencoder Pages: 1 -->\n",
       "<svg width=\"864pt\" height=\"16pt\"\n",
       " viewBox=\"0.00 0.00 864.00 16.45\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.31631 0.31631) rotate(0) translate(4 48)\">\n",
       "<title>MLP autoencoder</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-48 2727.5,-48 2727.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"116.75,-39 0,-39 0,-5 116.75,-5 116.75,-39\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-5 0,-39 61.75,-39 61.75,-5 0,-5\"/>\n",
       "<text text-anchor=\"start\" x=\"5\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"14.38\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"61.75,-5 61.75,-39 116.75,-39 116.75,-5 61.75,-5\"/>\n",
       "<text text-anchor=\"start\" x=\"66.75\" y=\"-18.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2412)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"296.75,-44 152.75,-44 152.75,0 296.75,0 296.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"152.75,0 152.75,-44 195.75,-44 195.75,0 152.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"161.12\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"157.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"195.75,-22 195.75,-44 238.75,-44 238.75,-22 195.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"204.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"238.75,-22 238.75,-44 296.75,-44 296.75,-22 238.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"243.75\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2412) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"195.75,0 195.75,-22 238.75,-22 238.75,0 195.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"200.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"238.75,0 238.75,-22 296.75,-22 296.75,0 238.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"246.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M116.4,-22C124.5,-22 132.96,-22 141.42,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.4,-25.5 151.4,-22 141.4,-18.5 141.4,-25.5\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"470.75,-44 332.75,-44 332.75,0 470.75,0 470.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"332.75,0 332.75,-44 375.75,-44 375.75,0 332.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"343.38\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">ReLU</text>\n",
       "<text text-anchor=\"start\" x=\"337.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"375.75,-22 375.75,-44 418.75,-44 418.75,-22 375.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"384.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"418.75,-22 418.75,-44 470.75,-44 470.75,-22 418.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"423.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"375.75,0 375.75,-22 418.75,-22 418.75,0 375.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"380.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"418.75,0 418.75,-22 470.75,-22 470.75,0 418.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"423.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M296.68,-22C304.88,-22 313.29,-22 321.6,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"321.33,-25.5 331.33,-22 321.33,-18.5 321.33,-25.5\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"644.75,-44 506.75,-44 506.75,0 644.75,0 644.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"506.75,0 506.75,-44 549.75,-44 549.75,0 506.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"511.38\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"511.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"549.75,-22 549.75,-44 592.75,-44 592.75,-22 549.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"558.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"592.75,-22 592.75,-44 644.75,-44 644.75,-22 592.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"597.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"549.75,0 549.75,-22 592.75,-22 592.75,0 549.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"554.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"592.75,0 592.75,-22 644.75,-22 644.75,0 592.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"597.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M470.73,-22C478.93,-22 487.36,-22 495.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"495.47,-25.5 505.47,-22 495.47,-18.5 495.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"818.75,-44 680.75,-44 680.75,0 818.75,0 818.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"680.75,0 680.75,-44 723.75,-44 723.75,0 680.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"689.12\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"685.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"723.75,-22 723.75,-44 766.75,-44 766.75,-22 723.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"732.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"766.75,-22 766.75,-44 818.75,-44 818.75,-22 766.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"771.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"723.75,0 723.75,-22 766.75,-22 766.75,0 723.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"728.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"766.75,0 766.75,-22 818.75,-22 818.75,0 766.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"771.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M644.73,-22C652.93,-22 661.36,-22 669.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"669.47,-25.5 679.47,-22 669.47,-18.5 669.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"992.75,-44 854.75,-44 854.75,0 992.75,0 992.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"854.75,0 854.75,-44 897.75,-44 897.75,0 854.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"865.38\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">ReLU</text>\n",
       "<text text-anchor=\"start\" x=\"859.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"897.75,-22 897.75,-44 940.75,-44 940.75,-22 897.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"906.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"940.75,-22 940.75,-44 992.75,-44 992.75,-22 940.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"945.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"897.75,0 897.75,-22 940.75,-22 940.75,0 897.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"902.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"940.75,0 940.75,-22 992.75,-22 992.75,0 940.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"945.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M818.73,-22C826.93,-22 835.36,-22 843.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"843.47,-25.5 853.47,-22 843.47,-18.5 843.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"1166.75,-44 1028.75,-44 1028.75,0 1166.75,0 1166.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1028.75,0 1028.75,-44 1071.75,-44 1071.75,0 1028.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1033.38\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"1033.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1071.75,-22 1071.75,-44 1114.75,-44 1114.75,-22 1071.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1080.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1114.75,-22 1114.75,-44 1166.75,-44 1166.75,-22 1114.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1119.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1071.75,0 1071.75,-22 1114.75,-22 1114.75,0 1071.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1076.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1114.75,0 1114.75,-22 1166.75,-22 1166.75,0 1114.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1119.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M992.73,-22C1000.93,-22 1009.36,-22 1017.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1017.47,-25.5 1027.47,-22 1017.47,-18.5 1017.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"1340.75,-44 1202.75,-44 1202.75,0 1340.75,0 1340.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1202.75,0 1202.75,-44 1245.75,-44 1245.75,0 1202.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1211.12\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"1207.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1245.75,-22 1245.75,-44 1288.75,-44 1288.75,-22 1245.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1254.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1288.75,-22 1288.75,-44 1340.75,-44 1340.75,-22 1288.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1293.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1245.75,0 1245.75,-22 1288.75,-22 1288.75,0 1245.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1250.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1288.75,0 1288.75,-22 1340.75,-22 1340.75,0 1288.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1293.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 256) </text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1166.73,-22C1174.93,-22 1183.36,-22 1191.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1191.47,-25.5 1201.47,-22 1191.47,-18.5 1191.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"1514.75,-44 1376.75,-44 1376.75,0 1514.75,0 1514.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1376.75,0 1376.75,-44 1419.75,-44 1419.75,0 1376.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1385.12\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"1381.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1419.75,-22 1419.75,-44 1462.75,-44 1462.75,-22 1419.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1428.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1462.75,-22 1462.75,-44 1514.75,-44 1514.75,-22 1462.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1467.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1419.75,0 1419.75,-22 1462.75,-22 1462.75,0 1419.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1424.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1462.75,0 1462.75,-22 1514.75,-22 1514.75,0 1462.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1467.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1340.73,-22C1348.93,-22 1357.36,-22 1365.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1365.47,-25.5 1375.47,-22 1365.47,-18.5 1365.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"1688.75,-44 1550.75,-44 1550.75,0 1688.75,0 1688.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1550.75,0 1550.75,-44 1593.75,-44 1593.75,0 1550.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1561.38\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">ReLU</text>\n",
       "<text text-anchor=\"start\" x=\"1555.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1593.75,-22 1593.75,-44 1636.75,-44 1636.75,-22 1593.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1602.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1636.75,-22 1636.75,-44 1688.75,-44 1688.75,-22 1636.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1641.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1593.75,0 1593.75,-22 1636.75,-22 1636.75,0 1593.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1598.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1636.75,0 1636.75,-22 1688.75,-22 1688.75,0 1636.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1641.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1514.73,-22C1522.93,-22 1531.36,-22 1539.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1539.47,-25.5 1549.47,-22 1539.47,-18.5 1539.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"1862.75,-44 1724.75,-44 1724.75,0 1862.75,0 1862.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1724.75,0 1724.75,-44 1767.75,-44 1767.75,0 1724.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1729.38\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"1729.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1767.75,-22 1767.75,-44 1810.75,-44 1810.75,-22 1767.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1776.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1810.75,-22 1810.75,-44 1862.75,-44 1862.75,-22 1810.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1815.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1767.75,0 1767.75,-22 1810.75,-22 1810.75,0 1767.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1772.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1810.75,0 1810.75,-22 1862.75,-22 1862.75,0 1810.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1815.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1688.73,-22C1696.93,-22 1705.36,-22 1713.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1713.47,-25.5 1723.47,-22 1713.47,-18.5 1713.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"2036.75,-44 1898.75,-44 1898.75,0 2036.75,0 2036.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1898.75,0 1898.75,-44 1941.75,-44 1941.75,0 1898.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1907.12\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"1903.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1941.75,-22 1941.75,-44 1984.75,-44 1984.75,-22 1941.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1950.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1984.75,-22 1984.75,-44 2036.75,-44 2036.75,-22 1984.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"1989.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1941.75,0 1941.75,-22 1984.75,-22 1984.75,0 1941.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1946.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1984.75,0 1984.75,-22 2036.75,-22 2036.75,0 1984.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"1989.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1862.73,-22C1870.93,-22 1879.36,-22 1887.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1887.47,-25.5 1897.47,-22 1887.47,-18.5 1887.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"2210.75,-44 2072.75,-44 2072.75,0 2210.75,0 2210.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2072.75,0 2072.75,-44 2115.75,-44 2115.75,0 2072.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"2083.38\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">ReLU</text>\n",
       "<text text-anchor=\"start\" x=\"2077.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2115.75,-22 2115.75,-44 2158.75,-44 2158.75,-22 2115.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"2124.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2158.75,-22 2158.75,-44 2210.75,-44 2210.75,-22 2158.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"2163.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2115.75,0 2115.75,-22 2158.75,-22 2158.75,0 2115.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"2120.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2158.75,0 2158.75,-22 2210.75,-22 2210.75,0 2158.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"2163.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2036.73,-22C2044.93,-22 2053.36,-22 2061.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2061.47,-25.5 2071.47,-22 2061.47,-18.5 2061.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"2384.75,-44 2246.75,-44 2246.75,0 2384.75,0 2384.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2246.75,0 2246.75,-44 2289.75,-44 2289.75,0 2246.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"2251.38\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Dropout</text>\n",
       "<text text-anchor=\"start\" x=\"2251.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2289.75,-22 2289.75,-44 2332.75,-44 2332.75,-22 2289.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"2298.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2332.75,-22 2332.75,-44 2384.75,-44 2384.75,-22 2332.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"2337.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2289.75,0 2289.75,-22 2332.75,-22 2332.75,0 2289.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"2294.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2332.75,0 2332.75,-22 2384.75,-22 2384.75,0 2332.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"2337.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2210.73,-22C2218.93,-22 2227.36,-22 2235.69,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2235.47,-25.5 2245.47,-22 2235.47,-18.5 2235.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"2564.75,-44 2420.75,-44 2420.75,0 2564.75,0 2564.75,-44\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2420.75,0 2420.75,-44 2463.75,-44 2463.75,0 2420.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"2429.12\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"2425.75\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2463.75,-22 2463.75,-44 2506.75,-44 2506.75,-22 2463.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"2472.88\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2506.75,-22 2506.75,-44 2564.75,-44 2564.75,-22 2506.75,-22\"/>\n",
       "<text text-anchor=\"start\" x=\"2514.38\" y=\"-29.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2463.75,0 2463.75,-22 2506.75,-22 2506.75,0 2463.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"2468.38\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2506.75,0 2506.75,-22 2564.75,-22 2564.75,0 2506.75,0\"/>\n",
       "<text text-anchor=\"start\" x=\"2511.75\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2412) </text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2384.66,-22C2392.85,-22 2401.28,-22 2409.65,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2409.47,-25.5 2419.47,-22 2409.47,-18.5 2409.47,-25.5\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"2723.5,-39 2600.75,-39 2600.75,-5 2723.5,-5 2723.5,-39\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2600.75,-5 2600.75,-39 2668.5,-39 2668.5,-5 2600.75,-5\"/>\n",
       "<text text-anchor=\"start\" x=\"2605.75\" y=\"-24.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"2618.12\" y=\"-12.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"2668.5,-5 2668.5,-39 2723.5,-39 2723.5,-5 2668.5,-5\"/>\n",
       "<text text-anchor=\"start\" x=\"2673.5\" y=\"-18.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(64, 2412)</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2564.73,-22C2572.91,-22 2581.25,-22 2589.43,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2589.35,-25.5 2599.35,-22 2589.35,-18.5 2589.35,-25.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x11cc29116a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "# device='meta' -> no memory is consumed for visualization\n",
    "model_graph = draw_graph(ae, input_size=(64, 2412), device='meta', graph_name='MLP autoencoder', graph_dir='LR')\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, epoch_index, tb_writer, training_loader, optimizer, loss_fn, device, L1, verbose=False):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    model = model.to(device)\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(inputs)\n",
    "\n",
    "        params = torch.cat([x.view(-1) for x in model.parameters()])\n",
    "        l1_regularization = L1 * torch.linalg.vector_norm(params, 1)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels) + l1_regularization\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # last_loss =  loss.item() / inputs.shape[0]  # loss per sample\n",
    "        # if verbose:print('  batch {} loss: {}'.format(i + 1, last_loss), outputs[0][0].item(), labels[0][0].item())\n",
    "        # tb_x = epoch_index * len(training_loader) + i + 1\n",
    "        # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "    \n",
    "    return running_loss / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "batch_size=256\n",
    "train_set_drug  = dataset(X_train_drug, y_train_drug)\n",
    "val_set_drug  = dataset(X_val_drug, y_val_drug)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=batch_size, shuffle=True)\n",
    "val_dl_drug = DataLoader(val_set_drug, batch_size=batch_size, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "\n",
    "def objective(trial=None):\n",
    "    if trial is None:\n",
    "        lr = 1e-3\n",
    "        dropout=0.2\n",
    "        weight_decay=1e-4\n",
    "        L1 = 1e-6\n",
    "        first_layer=1024\n",
    "    else:\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "        L1 = trial.suggest_float(\"L1\", 1e-8, 1e-4, log=True)\n",
    "\n",
    "        lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.3) \n",
    "        first_layer = trial.suggest_categorical(\"first_layer\", [1024, 512,256,128])\n",
    "\n",
    "    ae = AE([xi.shape[1],first_layer,512,256,512,first_layer], dropout=dropout)\n",
    "    # print(summary(ae.to(\"cuda\"), xi.shape))\n",
    "    early_stopper = EarlyStopper(patience=20)\n",
    "    optimizer = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, verbose=True, patience=10, min_lr=1e-7)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    epoch_number = 0\n",
    "\n",
    "    EPOCHS = 500\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        ae.train(True)\n",
    "        ae = ae.to(device=device)\n",
    "\n",
    "        avg_loss = train_one_epoch(ae, epoch_number, \"writer\", train_dl_drug, optimizer, loss_fn, L1=L1,device=device)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        # Set the ae to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        ae.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_dl_drug):\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs = vinputs.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "                voutputs = ae(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss\n",
    "        avg_vloss = running_vloss / (i + 1) \n",
    "        print(vinputs.shape[0])\n",
    "\n",
    "        if epoch_number%10==9:print('EPOCH {}:'.format(epoch_number + 1),'LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if early_stopper.early_stop(avg_vloss):             \n",
    "            break\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(avg_vloss, epoch_number)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        epoch_number += 1\n",
    "\n",
    "    return avg_vloss\n",
    "\n",
    "# objective()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, data in enumerate(train_dl_drug):\n",
    "#         inputs, labels = data\n",
    "#         inputs = inputs.to(device=device)\n",
    "#         labels = labels.to(device=device)\n",
    "#         outputs = ae(inputs)\n",
    "# with open(\"output.txt\", mode=\"w\") as f: \n",
    "#     [f.write(\n",
    "#         str(outputs[0][i].to(\"cpu\").numpy().round(3))+\"   \"+\n",
    "#         str(inputs[0][i].to(\"cpu\").numpy().round(3))+\"\\n\") for i in range(len(outputs[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 17:55:47,345] A new study created in memory with name: no-name-2300212f-d3ef-4223-ad49-b08e5bfafde9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9558379650115967 valid 0.9500653147697449\n",
      "EPOCH 20: LOSS train 0.9222928285598755 valid 0.9241638779640198\n",
      "EPOCH 30: LOSS train 0.8798766930898031 valid 0.9081265330314636\n",
      "EPOCH 40: LOSS train 0.8756439487139384 valid 0.8905343413352966\n",
      "Epoch 00040: reducing learning rate of group 0 to 4.6252e-04.\n",
      "EPOCH 50: LOSS train 0.8701503674189249 valid 0.8774283528327942\n",
      "EPOCH 60: LOSS train 0.8431068460146586 valid 0.8735763430595398\n",
      "EPOCH 70: LOSS train 0.842602550983429 valid 0.8666717410087585\n",
      "EPOCH 80: LOSS train 0.8455469806989034 valid 0.8602144718170166\n",
      "Epoch 00089: reducing learning rate of group 0 to 2.3126e-04.\n",
      "EPOCH 90: LOSS train 0.8269160985946655 valid 0.8573508262634277\n",
      "EPOCH 100: LOSS train 0.8092964887619019 valid 0.852138876914978\n",
      "EPOCH 110: LOSS train 0.8178675174713135 valid 0.8505203723907471\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.1563e-04.\n",
      "EPOCH 120: LOSS train 0.7934661308924357 valid 0.8491114974021912\n",
      "EPOCH 130: LOSS train 0.805749515692393 valid 0.8467800617218018\n",
      "Epoch 00136: reducing learning rate of group 0 to 5.7815e-05.\n",
      "EPOCH 140: LOSS train 0.797316312789917 valid 0.847705066204071\n",
      "EPOCH 150: LOSS train 0.8048998912175497 valid 0.8459479808807373\n",
      "Epoch 00156: reducing learning rate of group 0 to 2.8908e-05.\n",
      "EPOCH 160: LOSS train 0.7848464846611023 valid 0.8449394702911377\n",
      "Epoch 00167: reducing learning rate of group 0 to 1.4454e-05.\n",
      "EPOCH 170: LOSS train 0.8085295756657919 valid 0.8453993201255798\n",
      "EPOCH 180: LOSS train 0.794176439444224 valid 0.8453289270401001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 17:56:25,683] Trial 0 finished with value: 0.8453511595726013 and parameters: {'weight_decay': 1.521503859743021e-07, 'L1': 3.0320881973172606e-08, 'learning_rate': 0.000925047971446074, 'dropout': 0.22367883161577556, 'first_layer': 128}. Best is trial 0 with value: 0.8453511595726013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 5.749079863230388 valid 0.9661383628845215\n",
      "EPOCH 20: LOSS train 4.242317358652751 valid 0.9660544991493225\n",
      "EPOCH 30: LOSS train 3.0814040501912436 valid 0.9660072326660156\n",
      "EPOCH 40: LOSS train 2.2409890492757163 valid 0.9659820199012756\n",
      "EPOCH 50: LOSS train 1.7390731970469158 valid 0.965957522392273\n",
      "EPOCH 60: LOSS train 1.3637458483378093 valid 0.9659403562545776\n",
      "EPOCH 70: LOSS train 1.1490124066670735 valid 0.96592777967453\n",
      "EPOCH 80: LOSS train 1.1213864882787068 valid 0.9659102559089661\n",
      "EPOCH 90: LOSS train 1.015136996905009 valid 0.9658880233764648\n",
      "EPOCH 100: LOSS train 0.9945140679677328 valid 0.9658615589141846\n",
      "EPOCH 110: LOSS train 0.9926746090253195 valid 0.965840756893158\n",
      "Epoch 00119: reducing learning rate of group 0 to 7.3863e-05.\n",
      "EPOCH 120: LOSS train 0.9918403029441833 valid 0.9658300280570984\n",
      "EPOCH 130: LOSS train 1.0183249513308208 valid 0.9658229947090149\n",
      "Epoch 00130: reducing learning rate of group 0 to 3.6932e-05.\n",
      "EPOCH 140: LOSS train 1.0059877038002014 valid 0.9658195972442627\n",
      "EPOCH 150: LOSS train 0.993163526058197 valid 0.965818464756012\n",
      "Epoch 00150: reducing learning rate of group 0 to 1.8466e-05.\n",
      "EPOCH 160: LOSS train 1.012269635995229 valid 0.9658174514770508\n",
      "Epoch 00161: reducing learning rate of group 0 to 9.2329e-06.\n",
      "EPOCH 170: LOSS train 1.0453964273134868 valid 0.9658166766166687\n",
      "Epoch 00172: reducing learning rate of group 0 to 4.6164e-06.\n",
      "EPOCH 180: LOSS train 1.009662648042043 valid 0.9658161401748657\n",
      "Epoch 00183: reducing learning rate of group 0 to 2.3082e-06.\n",
      "EPOCH 190: LOSS train 0.9815130035082499 valid 0.9658157825469971\n",
      "Epoch 00194: reducing learning rate of group 0 to 1.1541e-06.\n",
      "EPOCH 200: LOSS train 1.0097787976264954 valid 0.9658156633377075\n",
      "Epoch 00205: reducing learning rate of group 0 to 5.7706e-07.\n",
      "EPOCH 210: LOSS train 0.9831401705741882 valid 0.9658157825469971\n",
      "Epoch 00216: reducing learning rate of group 0 to 2.8853e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 17:57:17,551] Trial 1 finished with value: 0.9658156633377075 and parameters: {'weight_decay': 9.397287637605942e-07, 'L1': 7.178700058262388e-05, 'learning_rate': 0.00014772615898265935, 'dropout': 0.05251964917304939, 'first_layer': 1024}. Best is trial 0 with value: 0.8453511595726013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 1.1384373505910237 valid 0.9664527773857117\n",
      "EPOCH 20: LOSS train 1.1708380778630574 valid 0.9664140343666077\n",
      "EPOCH 30: LOSS train 1.1499603589375813 valid 0.9663842916488647\n",
      "Epoch 00036: reducing learning rate of group 0 to 9.6666e-06.\n",
      "EPOCH 40: LOSS train 1.1418819427490234 valid 0.9663536548614502\n",
      "Epoch 00047: reducing learning rate of group 0 to 4.8333e-06.\n",
      "EPOCH 50: LOSS train 1.1522841453552246 valid 0.9663284420967102\n",
      "Epoch 00058: reducing learning rate of group 0 to 2.4166e-06.\n",
      "EPOCH 60: LOSS train 1.1534196535746257 valid 0.9663165807723999\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.2083e-06.\n",
      "EPOCH 70: LOSS train 1.137129823366801 valid 0.9663088321685791\n",
      "EPOCH 80: LOSS train 1.165955384572347 valid 0.9663040637969971\n",
      "Epoch 00086: reducing learning rate of group 0 to 6.0416e-07.\n",
      "EPOCH 90: LOSS train 1.1174464623133342 valid 0.9663010239601135\n",
      "Epoch 00097: reducing learning rate of group 0 to 3.0208e-07.\n",
      "EPOCH 100: LOSS train 1.129121979077657 valid 0.9662988781929016\n",
      "Epoch 00108: reducing learning rate of group 0 to 1.5104e-07.\n",
      "EPOCH 110: LOSS train 1.1422386566797893 valid 0.9662975072860718\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 120: LOSS train 1.1556634505589802 valid 0.9662971496582031\n",
      "EPOCH 130: LOSS train 1.1793475945790608 valid 0.9662967324256897\n",
      "EPOCH 140: LOSS train 1.144975225130717 valid 0.966296374797821\n",
      "EPOCH 150: LOSS train 1.1875230471293132 valid 0.9662961363792419\n",
      "EPOCH 160: LOSS train 1.1527519623438518 valid 0.9662957787513733\n",
      "EPOCH 170: LOSS train 1.1466262340545654 valid 0.9662952423095703\n",
      "EPOCH 180: LOSS train 1.1915987730026245 valid 0.9662946462631226\n",
      "EPOCH 190: LOSS train 1.1798282464345295 valid 0.9662945866584778\n",
      "EPOCH 200: LOSS train 1.1758225758870442 valid 0.9662941098213196\n",
      "EPOCH 210: LOSS train 1.1345634857813518 valid 0.9662937521934509\n",
      "EPOCH 220: LOSS train 1.1710521777470906 valid 0.9662933349609375\n",
      "EPOCH 230: LOSS train 1.2221564054489136 valid 0.9662929773330688\n",
      "EPOCH 240: LOSS train 1.2110960086186726 valid 0.9662924408912659\n",
      "EPOCH 250: LOSS train 1.1278996070226033 valid 0.9662920832633972\n",
      "EPOCH 260: LOSS train 1.2332322597503662 valid 0.9662919640541077\n",
      "EPOCH 270: LOSS train 1.1012909213701885 valid 0.9662913680076599\n",
      "EPOCH 280: LOSS train 1.1931862433751423 valid 0.9662909507751465\n",
      "EPOCH 290: LOSS train 1.1613328059514363 valid 0.9662904739379883\n",
      "EPOCH 300: LOSS train 1.1447380383809407 valid 0.966290295124054\n",
      "EPOCH 310: LOSS train 1.1334771315256755 valid 0.9662898182868958\n",
      "EPOCH 320: LOSS train 1.126167853673299 valid 0.966289222240448\n",
      "EPOCH 330: LOSS train 1.1280744075775146 valid 0.9662891626358032\n",
      "EPOCH 340: LOSS train 1.1587484280268352 valid 0.9662884473800659\n",
      "EPOCH 350: LOSS train 1.1372494300206502 valid 0.9662880897521973\n",
      "EPOCH 360: LOSS train 1.1561623414357503 valid 0.9662875533103943\n",
      "EPOCH 370: LOSS train 1.149709979693095 valid 0.9662874341011047\n",
      "EPOCH 380: LOSS train 1.131327231725057 valid 0.9662868976593018\n",
      "EPOCH 390: LOSS train 1.1363065640131633 valid 0.9662864208221436\n",
      "EPOCH 400: LOSS train 1.1607527335484822 valid 0.9662861824035645\n",
      "EPOCH 410: LOSS train 1.158433437347412 valid 0.9662859439849854\n",
      "EPOCH 420: LOSS train 1.2177399396896362 valid 0.9662851691246033\n",
      "EPOCH 430: LOSS train 1.204135576883952 valid 0.9662849307060242\n",
      "EPOCH 440: LOSS train 1.14298415184021 valid 0.9662843942642212\n",
      "EPOCH 450: LOSS train 1.1513563791910808 valid 0.9662840366363525\n",
      "EPOCH 460: LOSS train 1.2374906937281291 valid 0.9662837386131287\n",
      "EPOCH 470: LOSS train 1.1335783004760742 valid 0.9662827849388123\n",
      "EPOCH 480: LOSS train 1.1644421418507893 valid 0.9662826657295227\n",
      "EPOCH 490: LOSS train 1.1451212167739868 valid 0.9662821292877197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 17:59:14,538] Trial 2 finished with value: 0.966281533241272 and parameters: {'weight_decay': 1.6240761465405707e-05, 'L1': 2.9492281095963077e-06, 'learning_rate': 1.933316116923945e-05, 'dropout': 0.29945242490204527, 'first_layer': 512}. Best is trial 0 with value: 0.8453511595726013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 1.1449491580327351 valid 0.966281533241272\n",
      "EPOCH 10: LOSS train 1.8318997621536255 valid 0.9662110209465027\n",
      "EPOCH 20: LOSS train 1.7712084849675496 valid 0.966213047504425\n",
      "EPOCH 30: LOSS train 1.7456130981445312 valid 0.9662207365036011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 17:59:25,163] Trial 3 finished with value: 0.9662222266197205 and parameters: {'weight_decay': 3.510340887224134e-08, 'L1': 9.69207298297527e-06, 'learning_rate': 3.9309802788086826e-05, 'dropout': 0.23769606897011575, 'first_layer': 1024}. Best is trial 0 with value: 0.8453511595726013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 4.273407459259033 valid 0.9662054777145386\n",
      "EPOCH 20: LOSS train 3.167867422103882 valid 0.9661333560943604\n",
      "EPOCH 30: LOSS train 2.306013027826945 valid 0.9660952091217041\n",
      "EPOCH 40: LOSS train 1.7366260687510173 valid 0.9660723209381104\n",
      "EPOCH 50: LOSS train 1.3925496737162273 valid 0.9660578966140747\n",
      "EPOCH 60: LOSS train 1.1678822835286458 valid 0.9660438895225525\n",
      "EPOCH 70: LOSS train 1.1061400175094604 valid 0.9660329222679138\n",
      "EPOCH 80: LOSS train 1.007727026939392 valid 0.9660121202468872\n",
      "EPOCH 90: LOSS train 1.0368239482243855 valid 0.9659856557846069\n",
      "EPOCH 100: LOSS train 1.019333263238271 valid 0.9659576416015625\n",
      "Epoch 00100: reducing learning rate of group 0 to 8.0215e-05.\n",
      "EPOCH 110: LOSS train 0.9919237891832987 valid 0.9659404158592224\n",
      "Epoch 00118: reducing learning rate of group 0 to 4.0108e-05.\n",
      "EPOCH 120: LOSS train 0.9756018122037252 valid 0.9659237265586853\n",
      "Epoch 00129: reducing learning rate of group 0 to 2.0054e-05.\n",
      "EPOCH 130: LOSS train 1.0426649848620098 valid 0.9659152626991272\n",
      "EPOCH 140: LOSS train 1.0212231079737346 valid 0.9659103751182556\n",
      "EPOCH 150: LOSS train 0.9732348918914795 valid 0.9659067392349243\n",
      "Epoch 00156: reducing learning rate of group 0 to 1.0027e-05.\n",
      "EPOCH 160: LOSS train 0.9616250991821289 valid 0.9659035801887512\n",
      "EPOCH 170: LOSS train 1.0445493857065837 valid 0.965901792049408\n",
      "Epoch 00170: reducing learning rate of group 0 to 5.0135e-06.\n",
      "EPOCH 180: LOSS train 1.0442577997843425 valid 0.9659008979797363\n",
      "Epoch 00181: reducing learning rate of group 0 to 2.5067e-06.\n",
      "EPOCH 190: LOSS train 1.0255456169446309 valid 0.9659000635147095\n",
      "Epoch 00192: reducing learning rate of group 0 to 1.2534e-06.\n",
      "EPOCH 200: LOSS train 1.0083179076512654 valid 0.9659000039100647\n",
      "EPOCH 210: LOSS train 1.0089476903279622 valid 0.9658992886543274\n",
      "Epoch 00213: reducing learning rate of group 0 to 6.2668e-07.\n",
      "EPOCH 220: LOSS train 1.0052618384361267 valid 0.9658994078636169\n",
      "Epoch 00224: reducing learning rate of group 0 to 3.1334e-07.\n",
      "EPOCH 230: LOSS train 0.9704623818397522 valid 0.9658994078636169\n",
      "Epoch 00235: reducing learning rate of group 0 to 1.5667e-07.\n",
      "EPOCH 240: LOSS train 0.9880425333976746 valid 0.9658990502357483\n",
      "Epoch 00246: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 250: LOSS train 0.9893997510274252 valid 0.9658991694450378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:00:33,750] Trial 4 finished with value: 0.9658992886543274 and parameters: {'weight_decay': 4.9846727226524615e-05, 'L1': 5.030064085963517e-05, 'learning_rate': 0.00016043060667855465, 'dropout': 0.2522979002588282, 'first_layer': 1024}. Best is trial 0 with value: 0.8453511595726013.\n",
      "[I 2023-08-24 18:00:34,089] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 1.047809640566508 valid 0.9599978923797607\n",
      "EPOCH 20: LOSS train 0.9628181457519531 valid 0.9323472380638123\n",
      "EPOCH 30: LOSS train 0.9691176215807596 valid 0.9083560705184937\n",
      "EPOCH 40: LOSS train 0.8822894295056661 valid 0.8918082118034363\n",
      "EPOCH 50: LOSS train 0.8120157321294149 valid 0.8714638948440552\n",
      "EPOCH 60: LOSS train 0.8417049447695414 valid 0.8524872660636902\n",
      "Epoch 00061: reducing learning rate of group 0 to 4.9029e-05.\n",
      "EPOCH 70: LOSS train 0.8119808236757914 valid 0.8425993919372559\n",
      "EPOCH 80: LOSS train 0.7880351940790812 valid 0.8333290815353394\n",
      "EPOCH 90: LOSS train 0.7463201681772867 valid 0.8256587982177734\n",
      "EPOCH 100: LOSS train 0.7749769886334738 valid 0.8184805512428284\n",
      "Epoch 00101: reducing learning rate of group 0 to 2.4515e-05.\n",
      "EPOCH 110: LOSS train 0.7644139925638834 valid 0.8152925372123718\n",
      "EPOCH 120: LOSS train 0.7472295165061951 valid 0.8128119111061096\n",
      "EPOCH 130: LOSS train 0.7593441208203634 valid 0.8100350499153137\n",
      "Epoch 00130: reducing learning rate of group 0 to 1.2257e-05.\n",
      "EPOCH 140: LOSS train 0.7714752554893494 valid 0.8090698719024658\n",
      "Epoch 00141: reducing learning rate of group 0 to 6.1287e-06.\n",
      "EPOCH 150: LOSS train 0.742725650469462 valid 0.8081275224685669\n",
      "EPOCH 160: LOSS train 0.7205449938774109 valid 0.8073796033859253\n",
      "Epoch 00169: reducing learning rate of group 0 to 3.0643e-06.\n",
      "EPOCH 170: LOSS train 0.726284384727478 valid 0.8068877458572388\n",
      "EPOCH 180: LOSS train 0.7397015492121378 valid 0.806629478931427\n",
      "Epoch 00180: reducing learning rate of group 0 to 1.5322e-06.\n",
      "EPOCH 190: LOSS train 0.7579075495402018 valid 0.8064749240875244\n",
      "Epoch 00196: reducing learning rate of group 0 to 7.6609e-07.\n",
      "EPOCH 200: LOSS train 0.7819063663482666 valid 0.8064002990722656\n",
      "Epoch 00207: reducing learning rate of group 0 to 3.8304e-07.\n",
      "EPOCH 210: LOSS train 0.7325914899508158 valid 0.806325376033783\n",
      "Epoch 00218: reducing learning rate of group 0 to 1.9152e-07.\n",
      "EPOCH 220: LOSS train 0.7414738933245341 valid 0.8062868118286133\n",
      "Epoch 00229: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 230: LOSS train 0.727697491645813 valid 0.8062730431556702\n",
      "EPOCH 240: LOSS train 0.7518389225006104 valid 0.8062629103660583\n",
      "EPOCH 250: LOSS train 0.7251418034235636 valid 0.8062533736228943\n",
      "EPOCH 260: LOSS train 0.7561875780423483 valid 0.8062437772750854\n",
      "EPOCH 270: LOSS train 0.7476851145426432 valid 0.8062358498573303\n",
      "EPOCH 280: LOSS train 0.7276521722475687 valid 0.8062241673469543\n",
      "EPOCH 290: LOSS train 0.7394618590672811 valid 0.8062121868133545\n",
      "EPOCH 300: LOSS train 0.7290889819463094 valid 0.8062009215354919\n",
      "EPOCH 310: LOSS train 0.7350875735282898 valid 0.8061899542808533\n",
      "EPOCH 320: LOSS train 0.7295043269793192 valid 0.8061791658401489\n",
      "EPOCH 330: LOSS train 0.736422042051951 valid 0.8061692118644714\n",
      "EPOCH 340: LOSS train 0.7218476931254069 valid 0.8061567544937134\n",
      "EPOCH 350: LOSS train 0.7572338779767355 valid 0.8061472177505493\n",
      "EPOCH 360: LOSS train 0.7455061872800192 valid 0.8061356544494629\n",
      "EPOCH 370: LOSS train 0.7546514471371969 valid 0.8061266541481018\n",
      "EPOCH 380: LOSS train 0.7503366867701212 valid 0.8061200976371765\n",
      "EPOCH 390: LOSS train 0.7308219869931539 valid 0.8061097860336304\n",
      "EPOCH 400: LOSS train 0.750701387723287 valid 0.8061007857322693\n",
      "EPOCH 410: LOSS train 0.770848015944163 valid 0.8060925602912903\n",
      "EPOCH 420: LOSS train 0.7543997367223104 valid 0.8060796856880188\n",
      "EPOCH 430: LOSS train 0.7464176416397095 valid 0.8060672283172607\n",
      "EPOCH 440: LOSS train 0.7042519052823385 valid 0.8060578107833862\n",
      "EPOCH 450: LOSS train 0.7354984482129415 valid 0.8060486316680908\n",
      "EPOCH 460: LOSS train 0.741517186164856 valid 0.8060422539710999\n",
      "EPOCH 470: LOSS train 0.7446584502855936 valid 0.806033194065094\n",
      "EPOCH 480: LOSS train 0.7397565046946207 valid 0.8060242533683777\n",
      "EPOCH 490: LOSS train 0.7643360098203024 valid 0.8060153126716614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:03:04,570] Trial 6 finished with value: 0.8060060143470764 and parameters: {'weight_decay': 5.684865449016882e-08, 'L1': 6.722646546589177e-07, 'learning_rate': 9.805891446863493e-05, 'dropout': 0.029666581999730046, 'first_layer': 1024}. Best is trial 6 with value: 0.8060060143470764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 0.7429449160893759 valid 0.8060060143470764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:03:06,167] Trial 7 pruned. \n",
      "[I 2023-08-24 18:03:06,523] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 1.1732579072316487 valid 0.9568621516227722\n",
      "EPOCH 20: LOSS train 1.0804816087086995 valid 0.9355418682098389\n",
      "EPOCH 30: LOSS train 1.0131855607032776 valid 0.9180835485458374\n",
      "EPOCH 40: LOSS train 0.9889011780420939 valid 0.9075998067855835\n",
      "EPOCH 50: LOSS train 0.9423128763834635 valid 0.8963865637779236\n",
      "EPOCH 60: LOSS train 0.895369827747345 valid 0.8817471265792847\n",
      "EPOCH 70: LOSS train 0.8920516769091288 valid 0.8708170652389526\n",
      "EPOCH 80: LOSS train 0.878963053226471 valid 0.8588021993637085\n",
      "EPOCH 90: LOSS train 0.8450976411501566 valid 0.8445824384689331\n",
      "EPOCH 100: LOSS train 0.8467490077018738 valid 0.8357324004173279\n",
      "EPOCH 110: LOSS train 0.8193745613098145 valid 0.8269793391227722\n",
      "EPOCH 120: LOSS train 0.7705751657485962 valid 0.8208762407302856\n",
      "EPOCH 130: LOSS train 0.7551621596018473 valid 0.8175712823867798\n",
      "EPOCH 140: LOSS train 0.696519116560618 valid 0.8146508932113647\n",
      "EPOCH 150: LOSS train 0.6941251357396444 valid 0.8106219172477722\n",
      "EPOCH 160: LOSS train 0.6885878046353658 valid 0.806019127368927\n",
      "EPOCH 170: LOSS train 0.6414763530095419 valid 0.8025646209716797\n",
      "EPOCH 180: LOSS train 0.6374777356783549 valid 0.7994416952133179\n",
      "EPOCH 190: LOSS train 0.5699747800827026 valid 0.7948592901229858\n",
      "EPOCH 200: LOSS train 0.58979598681132 valid 0.7927770018577576\n",
      "EPOCH 210: LOSS train 0.5722116827964783 valid 0.7930437922477722\n",
      "EPOCH 220: LOSS train 0.5439925193786621 valid 0.7908627390861511\n",
      "EPOCH 230: LOSS train 0.532591183980306 valid 0.791554868221283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:04:53,860] Trial 9 finished with value: 0.7910802364349365 and parameters: {'weight_decay': 1.7453468352246422e-06, 'L1': 2.546596158881757e-06, 'learning_rate': 0.00017668893819907996, 'dropout': 0.06109537336121707, 'first_layer': 1024}. Best is trial 9 with value: 0.7910802364349365.\n",
      "[I 2023-08-24 18:04:54,632] Trial 10 pruned. \n",
      "[I 2023-08-24 18:04:55,318] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 1.1204986969629924 valid 0.9635928273200989\n",
      "EPOCH 20: LOSS train 1.051390568415324 valid 0.9402180910110474\n",
      "EPOCH 30: LOSS train 0.9894433418909708 valid 0.916644275188446\n",
      "EPOCH 40: LOSS train 0.9738032619158427 valid 0.902249813079834\n",
      "EPOCH 50: LOSS train 0.9707181652386984 valid 0.8847669959068298\n",
      "EPOCH 60: LOSS train 0.9211507439613342 valid 0.8671302199363708\n",
      "EPOCH 70: LOSS train 0.8975914120674133 valid 0.8499242663383484\n",
      "Epoch 00077: reducing learning rate of group 0 to 5.3930e-05.\n",
      "EPOCH 80: LOSS train 0.8710804382960001 valid 0.8357982635498047\n",
      "EPOCH 90: LOSS train 0.8320570588111877 valid 0.8293763399124146\n",
      "EPOCH 100: LOSS train 0.8211499055226644 valid 0.8217598795890808\n",
      "EPOCH 110: LOSS train 0.8118125796318054 valid 0.8163885474205017\n",
      "EPOCH 120: LOSS train 0.7822912732760111 valid 0.8113059401512146\n",
      "EPOCH 130: LOSS train 0.7763382792472839 valid 0.8069292902946472\n",
      "EPOCH 140: LOSS train 0.7610127925872803 valid 0.8030408620834351\n",
      "EPOCH 150: LOSS train 0.7712078889211019 valid 0.8000784516334534\n",
      "Epoch 00156: reducing learning rate of group 0 to 2.6965e-05.\n",
      "EPOCH 160: LOSS train 0.7527134021123251 valid 0.796166718006134\n",
      "Epoch 00167: reducing learning rate of group 0 to 1.3482e-05.\n",
      "EPOCH 170: LOSS train 0.7485534946123759 valid 0.7946441769599915\n",
      "Epoch 00178: reducing learning rate of group 0 to 6.7412e-06.\n",
      "EPOCH 180: LOSS train 0.7390456199645996 valid 0.7938681840896606\n",
      "EPOCH 190: LOSS train 0.7148127555847168 valid 0.7932085990905762\n",
      "EPOCH 200: LOSS train 0.7423236767450968 valid 0.7929859757423401\n",
      "Epoch 00201: reducing learning rate of group 0 to 3.3706e-06.\n",
      "EPOCH 210: LOSS train 0.7405215501785278 valid 0.7927837371826172\n",
      "Epoch 00212: reducing learning rate of group 0 to 1.6853e-06.\n",
      "EPOCH 220: LOSS train 0.7328124046325684 valid 0.7925668954849243\n",
      "Epoch 00223: reducing learning rate of group 0 to 8.4265e-07.\n",
      "EPOCH 230: LOSS train 0.734813928604126 valid 0.7925405502319336\n",
      "Epoch 00234: reducing learning rate of group 0 to 4.2132e-07.\n",
      "EPOCH 240: LOSS train 0.7268717090288798 valid 0.7925143837928772\n",
      "Epoch 00245: reducing learning rate of group 0 to 2.1066e-07.\n",
      "EPOCH 250: LOSS train 0.7429362734158834 valid 0.7924960255622864\n",
      "EPOCH 260: LOSS train 0.7374815344810486 valid 0.7924832105636597\n",
      "Epoch 00265: reducing learning rate of group 0 to 1.0533e-07.\n",
      "EPOCH 270: LOSS train 0.7054073015848795 valid 0.7924667596817017\n",
      "EPOCH 280: LOSS train 0.7303482890129089 valid 0.7924607992172241\n",
      "EPOCH 290: LOSS train 0.7434448003768921 valid 0.7924531698226929\n",
      "EPOCH 300: LOSS train 0.7146734595298767 valid 0.7924489974975586\n",
      "EPOCH 310: LOSS train 0.7509751717249552 valid 0.7924401164054871\n",
      "EPOCH 320: LOSS train 0.7059807380040487 valid 0.792432427406311\n",
      "EPOCH 330: LOSS train 0.7299103538195292 valid 0.7924285531044006\n",
      "EPOCH 340: LOSS train 0.7465673486391703 valid 0.7924218773841858\n",
      "EPOCH 350: LOSS train 0.766167958577474 valid 0.7924166917800903\n",
      "EPOCH 360: LOSS train 0.7432242433230082 valid 0.7924121022224426\n",
      "EPOCH 370: LOSS train 0.7210703094800314 valid 0.7924079895019531\n",
      "EPOCH 380: LOSS train 0.7358017166455587 valid 0.7924041152000427\n",
      "EPOCH 390: LOSS train 0.7382738987604777 valid 0.792396605014801\n",
      "EPOCH 400: LOSS train 0.7381400068600973 valid 0.7923864722251892\n",
      "EPOCH 410: LOSS train 0.7450979153315226 valid 0.7923842668533325\n",
      "EPOCH 420: LOSS train 0.7379438877105713 valid 0.7923796772956848\n",
      "EPOCH 430: LOSS train 0.7304796179135641 valid 0.7923729419708252\n",
      "EPOCH 440: LOSS train 0.7165496548016866 valid 0.7923722863197327\n",
      "EPOCH 450: LOSS train 0.7171456416447958 valid 0.7923675179481506\n",
      "EPOCH 460: LOSS train 0.7441467642784119 valid 0.792364239692688\n",
      "EPOCH 470: LOSS train 0.7247827251752218 valid 0.792360782623291\n",
      "EPOCH 480: LOSS train 0.7323490182558695 valid 0.792353093624115\n",
      "EPOCH 490: LOSS train 0.6900213559468588 valid 0.7923466563224792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:08:29,800] Trial 12 finished with value: 0.7923423051834106 and parameters: {'weight_decay': 1.5217943899838894e-06, 'L1': 1.725779671867697e-06, 'learning_rate': 0.00010785913972200362, 'dropout': 0.006519897613204841, 'first_layer': 1024}. Best is trial 9 with value: 0.7910802364349365.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 0.7241134246190389 valid 0.7923423051834106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:08:30,333] Trial 13 pruned. \n",
      "[I 2023-08-24 18:08:30,900] Trial 14 pruned. \n",
      "[I 2023-08-24 18:08:31,437] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 1.0021655360857646 valid 0.9286516904830933\n",
      "EPOCH 20: LOSS train 0.9314760764439901 valid 0.8932445049285889\n",
      "EPOCH 30: LOSS train 0.9011655449867249 valid 0.8551510572433472\n",
      "EPOCH 40: LOSS train 0.8034077882766724 valid 0.8317286968231201\n",
      "EPOCH 50: LOSS train 0.7358217636744181 valid 0.8148636221885681\n",
      "EPOCH 60: LOSS train 0.7027788758277893 valid 0.7980794310569763\n",
      "EPOCH 70: LOSS train 0.657513439655304 valid 0.7938249111175537\n",
      "EPOCH 80: LOSS train 0.5956132610638937 valid 0.785187304019928\n",
      "EPOCH 90: LOSS train 0.5664336681365967 valid 0.7802191972732544\n",
      "EPOCH 100: LOSS train 0.5150036811828613 valid 0.77079838514328\n",
      "EPOCH 110: LOSS train 0.4582968056201935 valid 0.7645775675773621\n",
      "EPOCH 120: LOSS train 0.4320084750652313 valid 0.7586661577224731\n",
      "EPOCH 130: LOSS train 0.4106995761394501 valid 0.7571603655815125\n",
      "EPOCH 140: LOSS train 0.3904690245787303 valid 0.7545919418334961\n",
      "EPOCH 150: LOSS train 0.3738105098406474 valid 0.7505562901496887\n",
      "EPOCH 160: LOSS train 0.35203556219736737 valid 0.7475869655609131\n",
      "EPOCH 170: LOSS train 0.3390910128752391 valid 0.7515841126441956\n",
      "EPOCH 180: LOSS train 0.32190462946891785 valid 0.7442137598991394\n",
      "EPOCH 190: LOSS train 0.31295626362164813 valid 0.7464473247528076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:10:10,609] Trial 16 finished with value: 0.7448643445968628 and parameters: {'weight_decay': 9.380025628481598e-06, 'L1': 1.2372567753399294e-06, 'learning_rate': 0.00046976771950563617, 'dropout': 0.10656901671755459, 'first_layer': 1024}. Best is trial 16 with value: 0.7448643445968628.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 200: LOSS train 0.28906378149986267 valid 0.7448643445968628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:10:11,199] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8899222612380981 valid 0.9243428111076355\n",
      "EPOCH 20: LOSS train 0.8653959433237711 valid 0.8942312598228455\n",
      "EPOCH 30: LOSS train 0.797186533610026 valid 0.8607043027877808\n",
      "EPOCH 40: LOSS train 0.7084664702415466 valid 0.8297315835952759\n",
      "EPOCH 50: LOSS train 0.6780080397923788 valid 0.8142337203025818\n",
      "EPOCH 60: LOSS train 0.6442016363143921 valid 0.8045491576194763\n",
      "EPOCH 70: LOSS train 0.5734171072642008 valid 0.7954046726226807\n",
      "EPOCH 80: LOSS train 0.518924872080485 valid 0.7868221402168274\n",
      "EPOCH 90: LOSS train 0.49171683192253113 valid 0.7809946537017822\n",
      "EPOCH 100: LOSS train 0.47647590438524884 valid 0.7904627323150635\n",
      "EPOCH 110: LOSS train 0.45237721999486286 valid 0.7734705209732056\n",
      "EPOCH 120: LOSS train 0.41743209958076477 valid 0.7792816758155823\n",
      "EPOCH 130: LOSS train 0.38874679803848267 valid 0.7663649916648865\n",
      "EPOCH 140: LOSS train 0.38051145275433856 valid 0.7634707689285278\n",
      "EPOCH 150: LOSS train 0.3707665006319682 valid 0.7645427584648132\n",
      "EPOCH 160: LOSS train 0.34835325678189594 valid 0.7577757239341736\n",
      "EPOCH 170: LOSS train 0.34396826227506 valid 0.7543285489082336\n",
      "EPOCH 180: LOSS train 0.3387749195098877 valid 0.7540813684463501\n",
      "EPOCH 190: LOSS train 0.3131573100884755 valid 0.7539536356925964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:11:39,386] Trial 18 finished with value: 0.7527999877929688 and parameters: {'weight_decay': 6.519848521642752e-05, 'L1': 1.2015302818094368e-07, 'learning_rate': 0.0009079258692776367, 'dropout': 0.10572765025958578, 'first_layer': 512}. Best is trial 16 with value: 0.7448643445968628.\n",
      "[I 2023-08-24 18:11:39,854] Trial 19 pruned. \n",
      "[I 2023-08-24 18:11:40,370] Trial 20 pruned. \n",
      "[I 2023-08-24 18:11:40,883] Trial 21 pruned. \n",
      "[I 2023-08-24 18:11:41,403] Trial 22 pruned. \n",
      "[I 2023-08-24 18:11:43,080] Trial 23 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 1.026223103205363 valid 0.9534241557121277\n",
      "EPOCH 20: LOSS train 1.0279094576835632 valid 0.9300695061683655\n",
      "EPOCH 30: LOSS train 0.9602466821670532 valid 0.9107813239097595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:12:02,555] Trial 24 pruned. \n",
      "[I 2023-08-24 18:12:03,115] Trial 25 pruned. \n",
      "[I 2023-08-24 18:12:03,596] Trial 26 pruned. \n",
      "[I 2023-08-24 18:12:04,099] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9586419463157654 valid 0.9186251163482666\n",
      "EPOCH 20: LOSS train 0.8773513038953146 valid 0.8767983317375183\n",
      "EPOCH 30: LOSS train 0.8051541050275167 valid 0.8383063673973083\n",
      "EPOCH 40: LOSS train 0.711656411488851 valid 0.8066897988319397\n",
      "EPOCH 50: LOSS train 0.6457609931627909 valid 0.7880690693855286\n",
      "EPOCH 60: LOSS train 0.5778284470240275 valid 0.7805308103561401\n",
      "EPOCH 70: LOSS train 0.5155910154183706 valid 0.7678174376487732\n",
      "EPOCH 80: LOSS train 0.46478824814160663 valid 0.7621568441390991\n",
      "EPOCH 90: LOSS train 0.40013231833775836 valid 0.7541503310203552\n",
      "EPOCH 100: LOSS train 0.36814212799072266 valid 0.7551124095916748\n",
      "EPOCH 110: LOSS train 0.3505212167898814 valid 0.7551292777061462\n",
      "EPOCH 120: LOSS train 0.37082576751708984 valid 0.7493510842323303\n",
      "EPOCH 130: LOSS train 0.2823615272839864 valid 0.7463329434394836\n",
      "EPOCH 140: LOSS train 0.2712044616540273 valid 0.7444107532501221\n",
      "EPOCH 150: LOSS train 0.277225802342097 valid 0.7460790872573853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:13:25,607] Trial 28 finished with value: 0.7447375655174255 and parameters: {'weight_decay': 1.862633031918403e-05, 'L1': 9.183807387231902e-07, 'learning_rate': 0.0005784231010039403, 'dropout': 0.06784019566275354, 'first_layer': 1024}. Best is trial 28 with value: 0.7447375655174255.\n",
      "[I 2023-08-24 18:13:26,110] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9103397528330485 valid 0.9017821550369263\n",
      "EPOCH 20: LOSS train 0.7831994493802389 valid 0.8447605967521667\n",
      "EPOCH 30: LOSS train 0.7067689696947733 valid 0.8232313394546509\n",
      "EPOCH 40: LOSS train 0.6518688599268595 valid 0.8037476539611816\n",
      "EPOCH 50: LOSS train 0.5833984613418579 valid 0.7910440564155579\n",
      "EPOCH 60: LOSS train 0.5548954407374064 valid 0.7857874035835266\n",
      "EPOCH 70: LOSS train 0.5078741908073425 valid 0.7752193212509155\n",
      "EPOCH 80: LOSS train 0.46676652630170185 valid 0.7666313052177429\n",
      "EPOCH 90: LOSS train 0.44242117802302044 valid 0.7586417198181152\n",
      "EPOCH 100: LOSS train 0.42522692680358887 valid 0.7520553469657898\n",
      "EPOCH 110: LOSS train 0.4050217966238658 valid 0.754375696182251\n",
      "EPOCH 120: LOSS train 0.3704349795977275 valid 0.7459155917167664\n",
      "EPOCH 130: LOSS train 0.37182583411534625 valid 0.7413755059242249\n",
      "EPOCH 140: LOSS train 0.3524420956770579 valid 0.7407110929489136\n",
      "EPOCH 150: LOSS train 0.33875856796900433 valid 0.7421055436134338\n",
      "EPOCH 160: LOSS train 0.3374521533648173 valid 0.7415432929992676\n",
      "EPOCH 170: LOSS train 0.3201957046985626 valid 0.7400569915771484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:14:49,956] Trial 30 finished with value: 0.7386537194252014 and parameters: {'weight_decay': 1.6682723008704847e-05, 'L1': 1.3530228081540591e-07, 'learning_rate': 0.00099393827199095, 'dropout': 0.1056367381706223, 'first_layer': 512}. Best is trial 30 with value: 0.7386537194252014.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9082832535107931 valid 0.9128998517990112\n",
      "EPOCH 20: LOSS train 0.8315853079160055 valid 0.8622657060623169\n",
      "EPOCH 30: LOSS train 0.7592150966326395 valid 0.8260172009468079\n",
      "EPOCH 40: LOSS train 0.6643585960070292 valid 0.8026137351989746\n",
      "EPOCH 50: LOSS train 0.5995129346847534 valid 0.7865001559257507\n",
      "EPOCH 60: LOSS train 0.5698206226030985 valid 0.7792510390281677\n",
      "EPOCH 70: LOSS train 0.514446755250295 valid 0.7742030024528503\n",
      "EPOCH 80: LOSS train 0.4635828336079915 valid 0.7636206150054932\n",
      "EPOCH 90: LOSS train 0.4699137508869171 valid 0.7622263431549072\n",
      "EPOCH 100: LOSS train 0.4143070379892985 valid 0.7594430446624756\n",
      "EPOCH 110: LOSS train 0.37820470333099365 valid 0.75084388256073\n",
      "EPOCH 120: LOSS train 0.3798874020576477 valid 0.7450903058052063\n",
      "EPOCH 130: LOSS train 0.36118871966997784 valid 0.7445464730262756\n",
      "EPOCH 140: LOSS train 0.3364576995372772 valid 0.7394722700119019\n",
      "EPOCH 150: LOSS train 0.3344399134318034 valid 0.7373407483100891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:15:56,563] Trial 31 finished with value: 0.7392796277999878 and parameters: {'weight_decay': 2.1820176746105766e-05, 'L1': 1.4421387528391943e-07, 'learning_rate': 0.0009758510120178325, 'dropout': 0.0989267048550777, 'first_layer': 512}. Best is trial 30 with value: 0.7386537194252014.\n",
      "[I 2023-08-24 18:15:57,291] Trial 32 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9194466670354208 valid 0.9262292385101318\n",
      "EPOCH 20: LOSS train 0.8559478322664896 valid 0.878812313079834\n",
      "EPOCH 30: LOSS train 0.7707009315490723 valid 0.8435406684875488\n",
      "EPOCH 40: LOSS train 0.6897880832354227 valid 0.814767062664032\n",
      "EPOCH 50: LOSS train 0.6925752758979797 valid 0.8004013299942017\n",
      "EPOCH 60: LOSS train 0.6088069081306458 valid 0.7847372889518738\n",
      "EPOCH 70: LOSS train 0.5299608906110128 valid 0.7740110754966736\n",
      "EPOCH 80: LOSS train 0.5297724008560181 valid 0.7645931839942932\n",
      "EPOCH 90: LOSS train 0.4814288914203644 valid 0.7601331472396851\n",
      "EPOCH 100: LOSS train 0.4394838909308116 valid 0.7562472224235535\n",
      "EPOCH 110: LOSS train 0.42795732617378235 valid 0.7483182549476624\n",
      "EPOCH 120: LOSS train 0.3966328203678131 valid 0.7438737750053406\n",
      "EPOCH 130: LOSS train 0.3756937285264333 valid 0.7389339208602905\n",
      "EPOCH 140: LOSS train 0.3654259542624156 valid 0.7361639142036438\n",
      "EPOCH 150: LOSS train 0.34477369983990985 valid 0.7339457273483276\n",
      "EPOCH 160: LOSS train 0.32818931341171265 valid 0.732274055480957\n",
      "EPOCH 170: LOSS train 0.321238507827123 valid 0.7285043001174927\n",
      "EPOCH 180: LOSS train 0.31653910875320435 valid 0.7266078591346741\n",
      "EPOCH 190: LOSS train 0.29463110367457074 valid 0.7252769470214844\n",
      "Epoch 00198: reducing learning rate of group 0 to 3.5460e-04.\n",
      "EPOCH 200: LOSS train 0.2898549387852351 valid 0.7231307625770569\n",
      "EPOCH 210: LOSS train 0.2690025866031647 valid 0.7195281982421875\n",
      "EPOCH 220: LOSS train 0.2712610165278117 valid 0.7188907265663147\n",
      "Epoch 00227: reducing learning rate of group 0 to 1.7730e-04.\n",
      "EPOCH 230: LOSS train 0.2579394777615865 valid 0.7177814245223999\n",
      "EPOCH 240: LOSS train 0.24898429214954376 valid 0.7171015739440918\n",
      "EPOCH 250: LOSS train 0.251484880844752 valid 0.7160900235176086\n",
      "EPOCH 260: LOSS train 0.2548897365729014 valid 0.715678334236145\n",
      "Epoch 00264: reducing learning rate of group 0 to 8.8650e-05.\n",
      "EPOCH 270: LOSS train 0.2534950375556946 valid 0.7155870199203491\n",
      "Epoch 00275: reducing learning rate of group 0 to 4.4325e-05.\n",
      "EPOCH 280: LOSS train 0.24389852583408356 valid 0.7150653004646301\n",
      "EPOCH 290: LOSS train 0.24041730662186941 valid 0.7148688435554504\n",
      "Epoch 00297: reducing learning rate of group 0 to 2.2163e-05.\n",
      "EPOCH 300: LOSS train 0.23837445676326752 valid 0.7149356007575989\n",
      "Epoch 00308: reducing learning rate of group 0 to 1.1081e-05.\n",
      "EPOCH 310: LOSS train 0.24183396498362222 valid 0.714765191078186\n",
      "Epoch 00319: reducing learning rate of group 0 to 5.5407e-06.\n",
      "EPOCH 320: LOSS train 0.25005881985028583 valid 0.7148229479789734\n",
      "EPOCH 330: LOSS train 0.253595436612765 valid 0.7148037552833557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:18:05,240] Trial 33 finished with value: 0.7147747278213501 and parameters: {'weight_decay': 1.1985132865365866e-05, 'L1': 3.3967165965563384e-07, 'learning_rate': 0.0007092038435659098, 'dropout': 0.07809298462256418, 'first_layer': 512}. Best is trial 33 with value: 0.7147747278213501.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9197025299072266 valid 0.9121640920639038\n",
      "EPOCH 20: LOSS train 0.8090057571729025 valid 0.8590270280838013\n",
      "EPOCH 30: LOSS train 0.7364276051521301 valid 0.8173599243164062\n",
      "EPOCH 40: LOSS train 0.6817459662755331 valid 0.797257661819458\n",
      "EPOCH 50: LOSS train 0.5876881877581278 valid 0.781710684299469\n",
      "EPOCH 60: LOSS train 0.5222284098466238 valid 0.7677462697029114\n",
      "EPOCH 70: LOSS train 0.4907721181710561 valid 0.7614993453025818\n",
      "EPOCH 80: LOSS train 0.4509786168734233 valid 0.7558192610740662\n",
      "EPOCH 90: LOSS train 0.40563832720120746 valid 0.7470263838768005\n",
      "EPOCH 100: LOSS train 0.4010372857252757 valid 0.7420593500137329\n",
      "EPOCH 110: LOSS train 0.3544096151987712 valid 0.7380011081695557\n",
      "EPOCH 120: LOSS train 0.34333325425783795 valid 0.7372170090675354\n",
      "EPOCH 130: LOSS train 0.320507417122523 valid 0.7334194183349609\n",
      "EPOCH 140: LOSS train 0.296654611825943 valid 0.7333149313926697\n",
      "EPOCH 150: LOSS train 0.28177767992019653 valid 0.7319819331169128\n",
      "EPOCH 160: LOSS train 0.27260421713193256 valid 0.7300741076469421\n",
      "EPOCH 170: LOSS train 0.27448256810506183 valid 0.7272515892982483\n",
      "EPOCH 180: LOSS train 0.26158976058165234 valid 0.7244904637336731\n",
      "EPOCH 190: LOSS train 0.2527795135974884 valid 0.7252619862556458\n",
      "Epoch 00195: reducing learning rate of group 0 to 3.7923e-04.\n",
      "EPOCH 200: LOSS train 0.23485882580280304 valid 0.7228701710700989\n",
      "EPOCH 210: LOSS train 0.21762458483378092 valid 0.7196441292762756\n",
      "EPOCH 220: LOSS train 0.2137337028980255 valid 0.7187247276306152\n",
      "EPOCH 230: LOSS train 0.21190311014652252 valid 0.7171190977096558\n",
      "Epoch 00235: reducing learning rate of group 0 to 1.8962e-04.\n",
      "EPOCH 240: LOSS train 0.21229771276315054 valid 0.7159820199012756\n",
      "EPOCH 250: LOSS train 0.20239106317361197 valid 0.7156931161880493\n",
      "EPOCH 260: LOSS train 0.19995903968811035 valid 0.7151378989219666\n",
      "Epoch 00262: reducing learning rate of group 0 to 9.4808e-05.\n",
      "EPOCH 270: LOSS train 0.1940834124883016 valid 0.7150071859359741\n",
      "Epoch 00273: reducing learning rate of group 0 to 4.7404e-05.\n",
      "EPOCH 280: LOSS train 0.20404973129431406 valid 0.7145395278930664\n",
      "EPOCH 290: LOSS train 0.19715430835882822 valid 0.714195728302002\n",
      "EPOCH 300: LOSS train 0.1911051074663798 valid 0.7140870690345764\n",
      "EPOCH 310: LOSS train 0.1859233578046163 valid 0.7139783501625061\n",
      "Epoch 00316: reducing learning rate of group 0 to 2.3702e-05.\n",
      "EPOCH 320: LOSS train 0.19053933024406433 valid 0.7137219905853271\n",
      "Epoch 00327: reducing learning rate of group 0 to 1.1851e-05.\n",
      "EPOCH 330: LOSS train 0.18930738170941672 valid 0.7137947678565979\n",
      "Epoch 00338: reducing learning rate of group 0 to 5.9255e-06.\n",
      "EPOCH 340: LOSS train 0.19090622663497925 valid 0.7136763334274292\n",
      "Epoch 00349: reducing learning rate of group 0 to 2.9627e-06.\n",
      "EPOCH 350: LOSS train 0.19826293488343558 valid 0.7135725021362305\n",
      "EPOCH 360: LOSS train 0.19950262705485025 valid 0.7135326266288757\n",
      "Epoch 00360: reducing learning rate of group 0 to 1.4814e-06.\n",
      "EPOCH 370: LOSS train 0.19330523908138275 valid 0.7135252356529236\n",
      "Epoch 00371: reducing learning rate of group 0 to 7.4068e-07.\n",
      "EPOCH 380: LOSS train 0.18870960175991058 valid 0.71352219581604\n",
      "Epoch 00382: reducing learning rate of group 0 to 3.7034e-07.\n",
      "EPOCH 390: LOSS train 0.1981552243232727 valid 0.7135158777236938\n",
      "Epoch 00393: reducing learning rate of group 0 to 1.8517e-07.\n",
      "EPOCH 400: LOSS train 0.191453884045283 valid 0.7135117053985596\n",
      "Epoch 00404: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 410: LOSS train 0.18980915347735086 valid 0.7135106921195984\n",
      "EPOCH 420: LOSS train 0.1948036402463913 valid 0.7135095596313477\n",
      "EPOCH 430: LOSS train 0.19145743052164713 valid 0.7135084271430969\n",
      "EPOCH 440: LOSS train 0.19060554603735605 valid 0.7135084867477417\n",
      "EPOCH 450: LOSS train 0.18987682461738586 valid 0.7135077118873596\n",
      "EPOCH 460: LOSS train 0.18426024417082468 valid 0.7135071158409119\n",
      "EPOCH 470: LOSS train 0.19678265849749246 valid 0.7135059237480164\n",
      "EPOCH 480: LOSS train 0.1875754694143931 valid 0.7135055661201477\n",
      "EPOCH 490: LOSS train 0.18847499787807465 valid 0.7135039567947388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:21:10,405] Trial 34 finished with value: 0.7135037779808044 and parameters: {'weight_decay': 1.5949621641126727e-05, 'L1': 2.0214217845240095e-07, 'learning_rate': 0.0007584607894578174, 'dropout': 0.05293127067160355, 'first_layer': 512}. Best is trial 34 with value: 0.7135037779808044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 0.1923274447520574 valid 0.7135037779808044\n",
      "EPOCH 10: LOSS train 0.8922967712084452 valid 0.9070898294448853\n",
      "EPOCH 20: LOSS train 0.7718491752942404 valid 0.845486044883728\n",
      "EPOCH 30: LOSS train 0.7177057266235352 valid 0.8074396252632141\n",
      "EPOCH 40: LOSS train 0.626485546429952 valid 0.7950010895729065\n",
      "EPOCH 50: LOSS train 0.5809065500895182 valid 0.7838720679283142\n",
      "EPOCH 60: LOSS train 0.4881657063961029 valid 0.7701390385627747\n",
      "EPOCH 70: LOSS train 0.45178980628649396 valid 0.7586055994033813\n",
      "EPOCH 80: LOSS train 0.4197208185990651 valid 0.7537974715232849\n",
      "EPOCH 90: LOSS train 0.384647270043691 valid 0.7468516230583191\n",
      "EPOCH 100: LOSS train 0.3757113218307495 valid 0.7416362762451172\n",
      "EPOCH 110: LOSS train 0.32462047537167865 valid 0.7384058833122253\n",
      "EPOCH 120: LOSS train 0.29923659563064575 valid 0.7352893948554993\n",
      "EPOCH 130: LOSS train 0.2906966010729472 valid 0.7312088012695312\n",
      "EPOCH 140: LOSS train 0.25948192675908405 valid 0.7291670441627502\n",
      "EPOCH 150: LOSS train 0.2708158791065216 valid 0.734929621219635\n",
      "EPOCH 160: LOSS train 0.2567780464887619 valid 0.7277922034263611\n",
      "EPOCH 170: LOSS train 0.25234223902225494 valid 0.7332192659378052\n",
      "Epoch 00173: reducing learning rate of group 0 to 4.8790e-04.\n",
      "EPOCH 180: LOSS train 0.21787984669208527 valid 0.7239769697189331\n",
      "EPOCH 190: LOSS train 0.2087156375249227 valid 0.7211678624153137\n",
      "EPOCH 200: LOSS train 0.19885540505250296 valid 0.7195069193840027\n",
      "EPOCH 210: LOSS train 0.2023055205742518 valid 0.7192317843437195\n",
      "EPOCH 220: LOSS train 0.19851389527320862 valid 0.7174180746078491\n",
      "EPOCH 230: LOSS train 0.19319497545560202 valid 0.7179149985313416\n",
      "EPOCH 240: LOSS train 0.19040216008822122 valid 0.7173570394515991\n",
      "Epoch 00243: reducing learning rate of group 0 to 2.4395e-04.\n",
      "EPOCH 250: LOSS train 0.18609554568926492 valid 0.7162008285522461\n",
      "EPOCH 260: LOSS train 0.1820054550965627 valid 0.7161715030670166\n",
      "Epoch 00267: reducing learning rate of group 0 to 1.2198e-04.\n",
      "EPOCH 270: LOSS train 0.18675467868645987 valid 0.7151907682418823\n",
      "EPOCH 280: LOSS train 0.1737202654282252 valid 0.7149196267127991\n",
      "Epoch 00289: reducing learning rate of group 0 to 6.0988e-05.\n",
      "EPOCH 290: LOSS train 0.17461011310418448 valid 0.7149409651756287\n",
      "EPOCH 300: LOSS train 0.17636011044184366 valid 0.7145512700080872\n",
      "Epoch 00300: reducing learning rate of group 0 to 3.0494e-05.\n",
      "EPOCH 310: LOSS train 0.16844685872395834 valid 0.7143496870994568\n",
      "Epoch 00314: reducing learning rate of group 0 to 1.5247e-05.\n",
      "EPOCH 320: LOSS train 0.17050040264924368 valid 0.7142244577407837\n",
      "Epoch 00329: reducing learning rate of group 0 to 7.6234e-06.\n",
      "EPOCH 330: LOSS train 0.17547489205996195 valid 0.7142006158828735\n",
      "EPOCH 340: LOSS train 0.17168425023555756 valid 0.7141745090484619\n",
      "Epoch 00340: reducing learning rate of group 0 to 3.8117e-06.\n",
      "EPOCH 350: LOSS train 0.17140906552473703 valid 0.7141552567481995\n",
      "EPOCH 360: LOSS train 0.16913804411888123 valid 0.7141178250312805\n",
      "Epoch 00364: reducing learning rate of group 0 to 1.9059e-06.\n",
      "EPOCH 370: LOSS train 0.16879932085673013 valid 0.7140923142433167\n",
      "EPOCH 380: LOSS train 0.16243502497673035 valid 0.7140779495239258\n",
      "Epoch 00383: reducing learning rate of group 0 to 9.5293e-07.\n",
      "EPOCH 390: LOSS train 0.17289337515830994 valid 0.7140751481056213\n",
      "Epoch 00394: reducing learning rate of group 0 to 4.7647e-07.\n",
      "EPOCH 400: LOSS train 0.1609822909037272 valid 0.7140710949897766\n",
      "Epoch 00405: reducing learning rate of group 0 to 2.3823e-07.\n",
      "EPOCH 410: LOSS train 0.16904170314470926 valid 0.7140694856643677\n",
      "Epoch 00416: reducing learning rate of group 0 to 1.1912e-07.\n",
      "EPOCH 420: LOSS train 0.1634312868118286 valid 0.7140682935714722\n",
      "Epoch 00427: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 430: LOSS train 0.169724702835083 valid 0.7140671610832214\n",
      "EPOCH 440: LOSS train 0.16770824790000916 valid 0.7140669822692871\n",
      "EPOCH 450: LOSS train 0.1702936738729477 valid 0.7140668034553528\n",
      "EPOCH 460: LOSS train 0.1734333336353302 valid 0.7140666246414185\n",
      "EPOCH 470: LOSS train 0.16692406932512918 valid 0.7140665650367737\n",
      "EPOCH 480: LOSS train 0.1730267455180486 valid 0.7140661478042603\n",
      "EPOCH 490: LOSS train 0.1806240677833557 valid 0.714066207408905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:24:16,148] Trial 35 finished with value: 0.7140654921531677 and parameters: {'weight_decay': 1.550307992128013e-05, 'L1': 1.998621204156801e-07, 'learning_rate': 0.0009758005371662377, 'dropout': 0.05066325414340786, 'first_layer': 512}. Best is trial 34 with value: 0.7135037779808044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 0.17180650432904562 valid 0.7140654921531677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:24:16,584] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9042191306749979 valid 0.9031804203987122\n",
      "EPOCH 20: LOSS train 0.7919440070788065 valid 0.8538695573806763\n",
      "EPOCH 30: LOSS train 0.7303068439165751 valid 0.8191690444946289\n",
      "EPOCH 40: LOSS train 0.6295322974522909 valid 0.7996503114700317\n",
      "EPOCH 50: LOSS train 0.6007658640543619 valid 0.7832791209220886\n",
      "EPOCH 60: LOSS train 0.49840736389160156 valid 0.7676497101783752\n",
      "EPOCH 70: LOSS train 0.49578021963437396 valid 0.7566865086555481\n",
      "EPOCH 80: LOSS train 0.4606778522332509 valid 0.7540603876113892\n",
      "EPOCH 90: LOSS train 0.43078423539797467 valid 0.7453635931015015\n",
      "EPOCH 100: LOSS train 0.3814437886079152 valid 0.7390702366828918\n",
      "EPOCH 110: LOSS train 0.3671200672785441 valid 0.7352911233901978\n",
      "EPOCH 120: LOSS train 0.3725954790910085 valid 0.7349314093589783\n",
      "EPOCH 130: LOSS train 0.3108553389708201 valid 0.7317197322845459\n",
      "EPOCH 140: LOSS train 0.28814150889714557 valid 0.7276955246925354\n",
      "EPOCH 150: LOSS train 0.29475775361061096 valid 0.7272536754608154\n",
      "Epoch 00150: reducing learning rate of group 0 to 3.9201e-04.\n",
      "EPOCH 160: LOSS train 0.262620210647583 valid 0.7234161496162415\n",
      "EPOCH 170: LOSS train 0.2585597336292267 valid 0.7233743071556091\n",
      "EPOCH 180: LOSS train 0.2527352174123128 valid 0.7219592332839966\n",
      "EPOCH 190: LOSS train 0.24074978629748026 valid 0.7206647396087646\n",
      "EPOCH 200: LOSS train 0.2270332674185435 valid 0.720348060131073\n",
      "EPOCH 210: LOSS train 0.23862790067990622 valid 0.7189453840255737\n",
      "Epoch 00211: reducing learning rate of group 0 to 1.9600e-04.\n",
      "EPOCH 220: LOSS train 0.22887647648652396 valid 0.7176737189292908\n",
      "EPOCH 230: LOSS train 0.23553908864657083 valid 0.7174451351165771\n",
      "EPOCH 240: LOSS train 0.21892890334129333 valid 0.7174071669578552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:25:39,529] Trial 37 finished with value: 0.7180107235908508 and parameters: {'weight_decay': 7.8471945413109e-06, 'L1': 3.890497592556043e-07, 'learning_rate': 0.0007840183525243987, 'dropout': 0.04407143804668643, 'first_layer': 512}. Best is trial 34 with value: 0.7135037779808044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9124179085095724 valid 0.9066482186317444\n",
      "EPOCH 20: LOSS train 0.8276390234629313 valid 0.8607890009880066\n",
      "EPOCH 30: LOSS train 0.7397639552752177 valid 0.8198691606521606\n",
      "EPOCH 40: LOSS train 0.646086057027181 valid 0.7995730638504028\n",
      "EPOCH 50: LOSS train 0.6035583217938741 valid 0.7813643217086792\n",
      "EPOCH 60: LOSS train 0.5522015889485677 valid 0.7701431512832642\n",
      "EPOCH 70: LOSS train 0.4831928809483846 valid 0.7581408619880676\n",
      "EPOCH 80: LOSS train 0.4620466927687327 valid 0.7531905174255371\n",
      "EPOCH 90: LOSS train 0.42090951402982074 valid 0.745550274848938\n",
      "EPOCH 100: LOSS train 0.3771424690882365 valid 0.7424588203430176\n",
      "EPOCH 110: LOSS train 0.3632831772168477 valid 0.7402634024620056\n",
      "EPOCH 120: LOSS train 0.3241419494152069 valid 0.7337726354598999\n",
      "EPOCH 130: LOSS train 0.3207644323507945 valid 0.7316882610321045\n",
      "EPOCH 140: LOSS train 0.29135356346766156 valid 0.7274731397628784\n",
      "EPOCH 150: LOSS train 0.2852950990200043 valid 0.7288544774055481\n",
      "EPOCH 160: LOSS train 0.27174737056096393 valid 0.7239910960197449\n",
      "EPOCH 170: LOSS train 0.2590896685918172 valid 0.7258129119873047\n",
      "EPOCH 180: LOSS train 0.2544596195220947 valid 0.7265745401382446\n",
      "EPOCH 190: LOSS train 0.23371178905169168 valid 0.7203164100646973\n",
      "EPOCH 200: LOSS train 0.2256603886683782 valid 0.720207154750824\n",
      "EPOCH 210: LOSS train 0.22435890138149261 valid 0.7215151786804199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:26:56,374] Trial 38 finished with value: 0.7204461097717285 and parameters: {'weight_decay': 7.156518559220984e-06, 'L1': 3.7459999640766877e-07, 'learning_rate': 0.0007501044428366838, 'dropout': 0.03634827995129279, 'first_layer': 512}. Best is trial 34 with value: 0.7135037779808044.\n",
      "[I 2023-08-24 18:26:56,837] Trial 39 pruned. \n",
      "[I 2023-08-24 18:26:57,290] Trial 40 pruned. \n",
      "[I 2023-08-24 18:26:57,719] Trial 41 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9012747406959534 valid 0.9106178879737854\n",
      "EPOCH 20: LOSS train 0.8468411962191263 valid 0.8644881844520569\n",
      "EPOCH 30: LOSS train 0.7112515568733215 valid 0.8207525610923767\n",
      "EPOCH 40: LOSS train 0.7143117785453796 valid 0.8001646399497986\n",
      "EPOCH 50: LOSS train 0.606796145439148 valid 0.7856647968292236\n",
      "EPOCH 60: LOSS train 0.5461216568946838 valid 0.7746065258979797\n",
      "EPOCH 70: LOSS train 0.5000840723514557 valid 0.7644763588905334\n",
      "EPOCH 80: LOSS train 0.4350704252719879 valid 0.7564968466758728\n",
      "EPOCH 90: LOSS train 0.41234975059827167 valid 0.7472479939460754\n",
      "EPOCH 100: LOSS train 0.3696114420890808 valid 0.7421981692314148\n",
      "EPOCH 110: LOSS train 0.34639204541842145 valid 0.7387531399726868\n",
      "EPOCH 120: LOSS train 0.32248695691426593 valid 0.7369387745857239\n",
      "EPOCH 130: LOSS train 0.30538636445999146 valid 0.7356178164482117\n",
      "EPOCH 140: LOSS train 0.29249484340349835 valid 0.7354936003684998\n",
      "EPOCH 150: LOSS train 0.2601241668065389 valid 0.7336897253990173\n",
      "EPOCH 160: LOSS train 0.26298627257347107 valid 0.7334909439086914\n",
      "EPOCH 170: LOSS train 0.24796344339847565 valid 0.7320722937583923\n",
      "EPOCH 180: LOSS train 0.23737367987632751 valid 0.7310488224029541\n",
      "EPOCH 190: LOSS train 0.2272462248802185 valid 0.7305091619491577\n",
      "EPOCH 200: LOSS train 0.218459352850914 valid 0.7288791537284851\n",
      "EPOCH 210: LOSS train 0.204754372437795 valid 0.7287998199462891\n",
      "EPOCH 220: LOSS train 0.20486992597579956 valid 0.7292969226837158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:28:16,355] Trial 42 finished with value: 0.7283526659011841 and parameters: {'weight_decay': 7.499312315753741e-06, 'L1': 5.742000283110726e-07, 'learning_rate': 0.0007244390302470492, 'dropout': 0.022604685539095365, 'first_layer': 512}. Best is trial 34 with value: 0.7135037779808044.\n",
      "[I 2023-08-24 18:28:16,849] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8959304690361023 valid 0.9140670895576477\n",
      "EPOCH 20: LOSS train 0.820981522401174 valid 0.8658169507980347\n",
      "EPOCH 30: LOSS train 0.7342444658279419 valid 0.8263528943061829\n",
      "EPOCH 40: LOSS train 0.6530331174532572 valid 0.8027204871177673\n",
      "EPOCH 50: LOSS train 0.6058071057001749 valid 0.787722647190094\n",
      "EPOCH 60: LOSS train 0.5321473181247711 valid 0.7752447128295898\n",
      "EPOCH 70: LOSS train 0.5028611918290457 valid 0.7667570114135742\n",
      "EPOCH 80: LOSS train 0.4488985240459442 valid 0.7611554265022278\n",
      "EPOCH 90: LOSS train 0.45397090911865234 valid 0.7572913765907288\n",
      "EPOCH 100: LOSS train 0.4173881908257802 valid 0.7543937563896179\n",
      "EPOCH 110: LOSS train 0.3937179346879323 valid 0.7521092295646667\n",
      "EPOCH 120: LOSS train 0.3456466794013977 valid 0.7437588572502136\n",
      "EPOCH 130: LOSS train 0.33777090907096863 valid 0.7425805926322937\n",
      "EPOCH 140: LOSS train 0.32493462165196735 valid 0.7361398339271545\n",
      "EPOCH 150: LOSS train 0.31132789452870685 valid 0.7325406670570374\n",
      "EPOCH 160: LOSS train 0.2972963750362396 valid 0.7326757311820984\n",
      "EPOCH 170: LOSS train 0.29812700549761456 valid 0.7305229902267456\n",
      "Epoch 00173: reducing learning rate of group 0 to 3.9300e-04.\n",
      "EPOCH 180: LOSS train 0.2557906707127889 valid 0.7257789373397827\n",
      "EPOCH 190: LOSS train 0.26024575034777325 valid 0.7235513925552368\n",
      "EPOCH 200: LOSS train 0.24573489526907602 valid 0.7229260802268982\n",
      "EPOCH 210: LOSS train 0.24544203778107962 valid 0.722165584564209\n",
      "EPOCH 220: LOSS train 0.24835950136184692 valid 0.7220646739006042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:29:32,801] Trial 44 finished with value: 0.7227436304092407 and parameters: {'weight_decay': 1.147423448204942e-05, 'L1': 3.44249534741533e-07, 'learning_rate': 0.0007859939117545289, 'dropout': 0.057857215441214964, 'first_layer': 512}. Best is trial 34 with value: 0.7135037779808044.\n",
      "[I 2023-08-24 18:29:33,304] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.866858184337616 valid 0.9058825373649597\n",
      "EPOCH 20: LOSS train 0.7746893763542175 valid 0.8462609648704529\n",
      "EPOCH 30: LOSS train 0.6832562883694967 valid 0.8126391768455505\n",
      "EPOCH 40: LOSS train 0.6310449242591858 valid 0.7852272987365723\n",
      "EPOCH 50: LOSS train 0.5587361653645834 valid 0.7710937261581421\n",
      "EPOCH 60: LOSS train 0.5262277623017629 valid 0.7613551020622253\n",
      "EPOCH 70: LOSS train 0.4493722915649414 valid 0.7548185586929321\n",
      "EPOCH 80: LOSS train 0.4160238206386566 valid 0.7469772100448608\n",
      "EPOCH 90: LOSS train 0.3770069380601247 valid 0.7442516088485718\n",
      "EPOCH 100: LOSS train 0.3686426480611165 valid 0.7383729219436646\n",
      "EPOCH 110: LOSS train 0.3261450429757436 valid 0.7334924936294556\n",
      "EPOCH 120: LOSS train 0.31308544675509137 valid 0.7322433590888977\n",
      "EPOCH 130: LOSS train 0.2786072591940562 valid 0.7297483682632446\n",
      "EPOCH 140: LOSS train 0.2648897022008896 valid 0.7283986806869507\n",
      "EPOCH 150: LOSS train 0.2516171485185623 valid 0.7264068126678467\n",
      "EPOCH 160: LOSS train 0.24104947845141092 valid 0.7264277338981628\n",
      "EPOCH 170: LOSS train 0.22763484716415405 valid 0.7252697348594666\n",
      "Epoch 00173: reducing learning rate of group 0 to 4.1077e-04.\n",
      "EPOCH 180: LOSS train 0.22206629316012064 valid 0.7199761271476746\n",
      "EPOCH 190: LOSS train 0.2044296662012736 valid 0.7199687957763672\n",
      "EPOCH 200: LOSS train 0.20229282478491464 valid 0.7183871865272522\n",
      "EPOCH 210: LOSS train 0.20213491221268973 valid 0.7168421149253845\n",
      "EPOCH 220: LOSS train 0.18269705772399902 valid 0.7177329063415527\n",
      "EPOCH 230: LOSS train 0.2014033943414688 valid 0.7166205048561096\n",
      "Epoch 00231: reducing learning rate of group 0 to 2.0539e-04.\n",
      "EPOCH 240: LOSS train 0.18361026048660278 valid 0.7150129079818726\n",
      "EPOCH 250: LOSS train 0.18065924445788065 valid 0.7136150002479553\n",
      "EPOCH 260: LOSS train 0.1723916232585907 valid 0.7137902975082397\n",
      "EPOCH 270: LOSS train 0.17861526211102804 valid 0.7135915160179138\n",
      "Epoch 00275: reducing learning rate of group 0 to 1.0269e-04.\n",
      "EPOCH 280: LOSS train 0.15916183590888977 valid 0.7130776643753052\n",
      "EPOCH 290: LOSS train 0.17121503750483194 valid 0.7124813795089722\n",
      "Epoch 00291: reducing learning rate of group 0 to 5.1347e-05.\n",
      "EPOCH 300: LOSS train 0.17133862276872 valid 0.7121703028678894\n",
      "Epoch 00302: reducing learning rate of group 0 to 2.5673e-05.\n",
      "EPOCH 310: LOSS train 0.16276275118192038 valid 0.7119481563568115\n",
      "Epoch 00313: reducing learning rate of group 0 to 1.2837e-05.\n",
      "EPOCH 320: LOSS train 0.16802441577116647 valid 0.712053656578064\n",
      "Epoch 00324: reducing learning rate of group 0 to 6.4183e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:31:24,526] Trial 46 finished with value: 0.7119879126548767 and parameters: {'weight_decay': 1.4940271869850511e-05, 'L1': 2.035023742123312e-07, 'learning_rate': 0.0008215476035324593, 'dropout': 0.038985292466368573, 'first_layer': 512}. Best is trial 46 with value: 0.7119879126548767.\n",
      "[I 2023-08-24 18:31:24,966] Trial 47 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8860469460487366 valid 0.9138746857643127\n",
      "EPOCH 20: LOSS train 0.8214063843091329 valid 0.8712638020515442\n",
      "EPOCH 30: LOSS train 0.7179014285405477 valid 0.8264384865760803\n",
      "EPOCH 40: LOSS train 0.6676048239072164 valid 0.8000107407569885\n",
      "EPOCH 50: LOSS train 0.614733616511027 valid 0.783824622631073\n",
      "EPOCH 60: LOSS train 0.5618464152018229 valid 0.773956835269928\n",
      "EPOCH 70: LOSS train 0.5124070445696512 valid 0.7636771202087402\n",
      "EPOCH 80: LOSS train 0.4725618859132131 valid 0.7577674388885498\n",
      "EPOCH 90: LOSS train 0.43642841776212055 valid 0.7531576752662659\n",
      "EPOCH 100: LOSS train 0.387978990872701 valid 0.7519984841346741\n",
      "EPOCH 110: LOSS train 0.36213914553324383 valid 0.7468695640563965\n",
      "EPOCH 120: LOSS train 0.34490182002385456 valid 0.740726888179779\n",
      "EPOCH 130: LOSS train 0.33226460218429565 valid 0.7372552156448364\n",
      "EPOCH 140: LOSS train 0.3238537112871806 valid 0.7373639345169067\n",
      "EPOCH 150: LOSS train 0.2955984075864156 valid 0.7370460033416748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:32:22,890] Trial 48 finished with value: 0.7361067533493042 and parameters: {'weight_decay': 3.546954683655734e-05, 'L1': 1.9314976128819466e-07, 'learning_rate': 0.0008146464925654635, 'dropout': 0.058519036812877956, 'first_layer': 512}. Best is trial 46 with value: 0.7119879126548767.\n",
      "[I 2023-08-24 18:32:23,747] Trial 49 pruned. \n",
      "[I 2023-08-24 18:32:24,230] Trial 50 pruned. \n",
      "[I 2023-08-24 18:32:24,621] Trial 51 pruned. \n",
      "[I 2023-08-24 18:32:25,091] Trial 52 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9479749798774719 valid 0.9165550470352173\n",
      "EPOCH 20: LOSS train 0.8741223017374674 valid 0.8677172660827637\n",
      "EPOCH 30: LOSS train 0.7505629062652588 valid 0.8244380950927734\n",
      "EPOCH 40: LOSS train 0.6551577051480612 valid 0.8017144203186035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:32:42,567] Trial 53 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8824093143145243 valid 0.9013755917549133\n",
      "EPOCH 20: LOSS train 0.769973357518514 valid 0.8464995622634888\n",
      "EPOCH 30: LOSS train 0.6891368627548218 valid 0.811931312084198\n",
      "EPOCH 40: LOSS train 0.548008014758428 valid 0.7879669666290283\n",
      "EPOCH 50: LOSS train 0.5078734159469604 valid 0.7721530795097351\n",
      "EPOCH 60: LOSS train 0.44410940011342365 valid 0.7630273699760437\n",
      "EPOCH 70: LOSS train 0.3829171558221181 valid 0.7553988099098206\n",
      "EPOCH 80: LOSS train 0.33065929015477497 valid 0.7541736364364624\n",
      "EPOCH 90: LOSS train 0.3070412874221802 valid 0.7529414296150208\n",
      "EPOCH 100: LOSS train 0.2881207565466563 valid 0.7447722554206848\n",
      "EPOCH 110: LOSS train 0.2530130495627721 valid 0.7414244413375854\n",
      "EPOCH 120: LOSS train 0.23203115165233612 valid 0.7365875840187073\n",
      "EPOCH 130: LOSS train 0.21470544735590616 valid 0.7376076579093933\n",
      "EPOCH 140: LOSS train 0.19106311599413553 valid 0.7366167902946472\n",
      "EPOCH 150: LOSS train 0.17804328600565592 valid 0.7332500219345093\n",
      "EPOCH 160: LOSS train 0.17628316084543863 valid 0.7319496870040894\n",
      "EPOCH 170: LOSS train 0.1583515852689743 valid 0.7293097972869873\n",
      "EPOCH 180: LOSS train 0.15578007698059082 valid 0.7318896651268005\n",
      "Epoch 00183: reducing learning rate of group 0 to 4.9306e-04.\n",
      "EPOCH 190: LOSS train 0.1356040984392166 valid 0.725929319858551\n",
      "EPOCH 200: LOSS train 0.12201413760582606 valid 0.7261518239974976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:33:55,839] Trial 54 pruned. \n",
      "[I 2023-08-24 18:33:56,626] Trial 55 pruned. \n",
      "[I 2023-08-24 18:33:57,079] Trial 56 pruned. \n",
      "[I 2023-08-24 18:33:57,469] Trial 57 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8912999033927917 valid 0.9039905071258545\n",
      "EPOCH 20: LOSS train 0.7767078081766764 valid 0.8505973815917969\n",
      "EPOCH 30: LOSS train 0.7082468271255493 valid 0.8106752038002014\n",
      "EPOCH 40: LOSS train 0.6069199045499166 valid 0.7892186045646667\n",
      "EPOCH 50: LOSS train 0.5292413632074991 valid 0.7716809511184692\n",
      "EPOCH 60: LOSS train 0.4822157820065816 valid 0.7586771845817566\n",
      "EPOCH 70: LOSS train 0.4090122977892558 valid 0.7533639669418335\n",
      "EPOCH 80: LOSS train 0.3827071189880371 valid 0.7482008337974548\n",
      "EPOCH 90: LOSS train 0.3398101131121318 valid 0.7449358105659485\n",
      "EPOCH 100: LOSS train 0.29390568534533185 valid 0.7438536882400513\n",
      "EPOCH 110: LOSS train 0.25645774106184643 valid 0.7423612475395203\n",
      "EPOCH 120: LOSS train 0.25454555451869965 valid 0.7413328289985657\n",
      "EPOCH 130: LOSS train 0.2214554399251938 valid 0.7402817010879517\n",
      "EPOCH 140: LOSS train 0.20553493003050485 valid 0.7375492453575134\n",
      "EPOCH 150: LOSS train 0.18943184117476145 valid 0.7362127304077148\n",
      "EPOCH 160: LOSS train 0.17829452951749167 valid 0.7388341426849365\n",
      "Epoch 00168: reducing learning rate of group 0 to 4.1847e-04.\n",
      "EPOCH 170: LOSS train 0.16541488965352377 valid 0.7351217269897461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:34:53,599] Trial 58 finished with value: 0.73493891954422 and parameters: {'weight_decay': 5.198074467117965e-07, 'L1': 5.656200933968382e-07, 'learning_rate': 0.000836937327766405, 'dropout': 0.0030395405874737574, 'first_layer': 512}. Best is trial 46 with value: 0.7119879126548767.\n",
      "[I 2023-08-24 18:34:54,141] Trial 59 pruned. \n",
      "[I 2023-08-24 18:34:54,517] Trial 60 pruned. \n",
      "[I 2023-08-24 18:34:54,944] Trial 61 pruned. \n",
      "[I 2023-08-24 18:34:55,346] Trial 62 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8988607128461202 valid 0.9114614725112915\n",
      "EPOCH 20: LOSS train 0.8145141204198202 valid 0.8545764088630676\n",
      "EPOCH 30: LOSS train 0.6938326756159464 valid 0.8146694898605347\n",
      "EPOCH 40: LOSS train 0.6313369671503702 valid 0.7976307272911072\n",
      "EPOCH 50: LOSS train 0.5812210440635681 valid 0.7812067270278931\n",
      "EPOCH 60: LOSS train 0.5338974595069885 valid 0.7679672241210938\n",
      "EPOCH 70: LOSS train 0.4686526854832967 valid 0.7585920095443726\n",
      "EPOCH 80: LOSS train 0.44200393557548523 valid 0.7541367411613464\n",
      "EPOCH 90: LOSS train 0.39529698093732196 valid 0.7460145950317383\n",
      "EPOCH 100: LOSS train 0.36515213052431744 valid 0.7400283217430115\n",
      "EPOCH 110: LOSS train 0.34423767526944477 valid 0.7333242893218994\n",
      "EPOCH 120: LOSS train 0.32708563407262164 valid 0.7308077812194824\n",
      "EPOCH 130: LOSS train 0.2996971507867177 valid 0.7271284461021423\n",
      "EPOCH 140: LOSS train 0.27896374464035034 valid 0.7310165762901306\n",
      "EPOCH 150: LOSS train 0.2832535703976949 valid 0.726352334022522\n",
      "EPOCH 160: LOSS train 0.26253361503283185 valid 0.7290307283401489\n",
      "EPOCH 170: LOSS train 0.2532765517632167 valid 0.7247933149337769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:35:51,689] Trial 63 finished with value: 0.7227454781532288 and parameters: {'weight_decay': 1.186187470050044e-05, 'L1': 2.226315641548646e-07, 'learning_rate': 0.0008469529110750521, 'dropout': 0.05201127415331942, 'first_layer': 512}. Best is trial 46 with value: 0.7119879126548767.\n",
      "[I 2023-08-24 18:35:52,692] Trial 64 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9335634509722391 valid 0.9100880026817322\n",
      "EPOCH 20: LOSS train 0.779105524222056 valid 0.8591024875640869\n",
      "EPOCH 30: LOSS train 0.7163725892702738 valid 0.8193677067756653\n",
      "EPOCH 40: LOSS train 0.6110265453656515 valid 0.797309935092926\n",
      "EPOCH 50: LOSS train 0.5470525026321411 valid 0.7834463715553284\n",
      "EPOCH 60: LOSS train 0.470855971177419 valid 0.7750712037086487\n",
      "EPOCH 70: LOSS train 0.42167993386586505 valid 0.7693486213684082\n",
      "EPOCH 80: LOSS train 0.38048507769902545 valid 0.763298511505127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:36:19,578] Trial 65 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9039716521898905 valid 0.9106667041778564\n",
      "EPOCH 20: LOSS train 0.8071785370508829 valid 0.8567987084388733\n",
      "EPOCH 30: LOSS train 0.7133159041404724 valid 0.8199130892753601\n",
      "EPOCH 40: LOSS train 0.645749588807424 valid 0.7983964085578918\n",
      "EPOCH 50: LOSS train 0.6036480267842611 valid 0.7813734412193298\n",
      "EPOCH 60: LOSS train 0.5488343437512716 valid 0.7692495584487915\n",
      "EPOCH 70: LOSS train 0.49289939800898236 valid 0.7657128572463989\n",
      "EPOCH 80: LOSS train 0.4542081852753957 valid 0.762260913848877\n",
      "EPOCH 90: LOSS train 0.44099393486976624 valid 0.751393735408783\n",
      "EPOCH 100: LOSS train 0.4304613570372264 valid 0.748737633228302\n",
      "EPOCH 110: LOSS train 0.41448234518369037 valid 0.7422487139701843\n",
      "EPOCH 120: LOSS train 0.3877659738063812 valid 0.7395757436752319\n",
      "EPOCH 130: LOSS train 0.35479555527369183 valid 0.7371573448181152\n",
      "EPOCH 140: LOSS train 0.3480275273323059 valid 0.7339234948158264\n",
      "EPOCH 150: LOSS train 0.3351975778738658 valid 0.7320995926856995\n",
      "EPOCH 160: LOSS train 0.3187544147173564 valid 0.729684054851532\n",
      "EPOCH 170: LOSS train 0.31160608927408856 valid 0.7258157134056091\n",
      "EPOCH 180: LOSS train 0.297427902619044 valid 0.7261751890182495\n",
      "EPOCH 190: LOSS train 0.2821047604084015 valid 0.7265036106109619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:37:23,663] Trial 66 finished with value: 0.7238898277282715 and parameters: {'weight_decay': 4.218513607922955e-06, 'L1': 1.0273771761849944e-07, 'learning_rate': 0.0008356933867661293, 'dropout': 0.09041186518397398, 'first_layer': 512}. Best is trial 46 with value: 0.7119879126548767.\n",
      "[I 2023-08-24 18:37:24,127] Trial 67 pruned. \n",
      "[I 2023-08-24 18:37:24,547] Trial 68 pruned. \n",
      "[I 2023-08-24 18:37:24,986] Trial 69 pruned. \n",
      "[I 2023-08-24 18:37:25,336] Trial 70 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9149656693140665 valid 0.8996393084526062\n",
      "EPOCH 20: LOSS train 0.7855455279350281 valid 0.8510788679122925\n",
      "EPOCH 30: LOSS train 0.7240825295448303 valid 0.8139052391052246\n",
      "EPOCH 40: LOSS train 0.6500369707743326 valid 0.794856071472168\n",
      "EPOCH 50: LOSS train 0.5741254885991415 valid 0.7827911376953125\n",
      "EPOCH 60: LOSS train 0.505685567855835 valid 0.7637555003166199\n",
      "EPOCH 70: LOSS train 0.4601116081078847 valid 0.7570879459381104\n",
      "EPOCH 80: LOSS train 0.4311160743236542 valid 0.7468356490135193\n",
      "EPOCH 90: LOSS train 0.4144830107688904 valid 0.742828369140625\n",
      "EPOCH 100: LOSS train 0.3572944402694702 valid 0.7399213314056396\n",
      "EPOCH 110: LOSS train 0.3458228011926015 valid 0.7390968203544617\n",
      "EPOCH 120: LOSS train 0.33657650152842206 valid 0.7400338053703308\n",
      "EPOCH 130: LOSS train 0.3311721980571747 valid 0.7395477890968323\n",
      "EPOCH 140: LOSS train 0.30228187640508014 valid 0.7356649041175842\n",
      "EPOCH 150: LOSS train 0.2698710064093272 valid 0.7295308709144592\n",
      "EPOCH 160: LOSS train 0.2689381241798401 valid 0.7282687425613403\n",
      "EPOCH 170: LOSS train 0.25944724182287854 valid 0.7299557328224182\n",
      "Epoch 00174: reducing learning rate of group 0 to 4.2497e-04.\n",
      "EPOCH 180: LOSS train 0.2526191174983978 valid 0.7233434319496155\n",
      "EPOCH 190: LOSS train 0.2232747127612432 valid 0.720544159412384\n",
      "EPOCH 200: LOSS train 0.22291592260201773 valid 0.720781683921814\n",
      "EPOCH 210: LOSS train 0.21527702609697977 valid 0.7191947102546692\n",
      "Epoch 00219: reducing learning rate of group 0 to 2.1249e-04.\n",
      "EPOCH 220: LOSS train 0.21366514762242636 valid 0.7174400091171265\n",
      "EPOCH 230: LOSS train 0.23426752289136252 valid 0.7155243754386902\n",
      "EPOCH 240: LOSS train 0.19944154222806296 valid 0.7156017422676086\n",
      "Epoch 00246: reducing learning rate of group 0 to 1.0624e-04.\n",
      "EPOCH 250: LOSS train 0.2093485544125239 valid 0.7142258286476135\n",
      "EPOCH 260: LOSS train 0.20561831692854562 valid 0.7142030596733093\n",
      "Epoch 00263: reducing learning rate of group 0 to 5.3121e-05.\n",
      "EPOCH 270: LOSS train 0.19656884173552194 valid 0.7136245369911194\n",
      "EPOCH 280: LOSS train 0.2045237123966217 valid 0.7140442728996277\n",
      "Epoch 00289: reducing learning rate of group 0 to 2.6561e-05.\n",
      "EPOCH 290: LOSS train 0.19310515622297922 valid 0.7139303684234619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:38:58,940] Trial 71 finished with value: 0.7138981223106384 and parameters: {'weight_decay': 1.3386192214471218e-05, 'L1': 2.422846335475917e-07, 'learning_rate': 0.0008499433086883709, 'dropout': 0.05261094875364654, 'first_layer': 512}. Best is trial 46 with value: 0.7119879126548767.\n",
      "[I 2023-08-24 18:38:59,352] Trial 72 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9196508924166361 valid 0.9051464200019836\n",
      "EPOCH 20: LOSS train 0.8142132957776388 valid 0.8516295552253723\n",
      "EPOCH 30: LOSS train 0.7238203088442484 valid 0.8166379332542419\n",
      "EPOCH 40: LOSS train 0.6051587263743082 valid 0.7981510162353516\n",
      "EPOCH 50: LOSS train 0.5527820984522501 valid 0.780709445476532\n",
      "EPOCH 60: LOSS train 0.5162262320518494 valid 0.7675822377204895\n",
      "EPOCH 70: LOSS train 0.46904389063517254 valid 0.7600014805793762\n",
      "EPOCH 80: LOSS train 0.42809396982192993 valid 0.7518849968910217\n",
      "EPOCH 90: LOSS train 0.3984873394171397 valid 0.7441558241844177\n",
      "EPOCH 100: LOSS train 0.36404362320899963 valid 0.7399866580963135\n",
      "EPOCH 110: LOSS train 0.3436455229918162 valid 0.7372130751609802\n",
      "EPOCH 120: LOSS train 0.31539220611254376 valid 0.7376956939697266\n",
      "EPOCH 130: LOSS train 0.314795861641566 valid 0.7318478226661682\n",
      "EPOCH 140: LOSS train 0.28139278292655945 valid 0.7290129065513611\n",
      "EPOCH 150: LOSS train 0.26375992099444073 valid 0.729756236076355\n",
      "EPOCH 160: LOSS train 0.27298834919929504 valid 0.7343571186065674\n",
      "EPOCH 170: LOSS train 0.25957610209782916 valid 0.7272006869316101\n",
      "EPOCH 180: LOSS train 0.24805172284444174 valid 0.7261896133422852\n",
      "EPOCH 190: LOSS train 0.25479815403620404 valid 0.7241857051849365\n",
      "EPOCH 200: LOSS train 0.2339236040910085 valid 0.7229641079902649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:40:05,530] Trial 73 pruned. \n",
      "[I 2023-08-24 18:40:05,980] Trial 74 pruned. \n",
      "[I 2023-08-24 18:40:06,385] Trial 75 pruned. \n",
      "[I 2023-08-24 18:40:06,792] Trial 76 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8872739871342977 valid 0.9074344635009766\n",
      "EPOCH 20: LOSS train 0.8207229177157084 valid 0.856856107711792\n",
      "EPOCH 30: LOSS train 0.706531306107839 valid 0.818343997001648\n",
      "EPOCH 40: LOSS train 0.6146284937858582 valid 0.7978942394256592\n",
      "EPOCH 50: LOSS train 0.5715483029683431 valid 0.7848488092422485\n",
      "EPOCH 60: LOSS train 0.49828847249348956 valid 0.7733974456787109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:40:30,286] Trial 77 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 70: LOSS train 0.46506399909655255 valid 0.7702329158782959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:40:30,808] Trial 78 pruned. \n",
      "[I 2023-08-24 18:40:31,370] Trial 79 pruned. \n",
      "[I 2023-08-24 18:40:31,763] Trial 80 pruned. \n",
      "[I 2023-08-24 18:40:32,158] Trial 81 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8919001817703247 valid 0.9083172082901001\n",
      "EPOCH 20: LOSS train 0.7843728065490723 valid 0.859086811542511\n",
      "EPOCH 30: LOSS train 0.7420137723286947 valid 0.8184338212013245\n",
      "EPOCH 40: LOSS train 0.6491098403930664 valid 0.7965169548988342\n",
      "EPOCH 50: LOSS train 0.5739620526631674 valid 0.7804542779922485\n",
      "EPOCH 60: LOSS train 0.5453996658325195 valid 0.7704106569290161\n",
      "EPOCH 70: LOSS train 0.49752811590830487 valid 0.7656659483909607\n",
      "EPOCH 80: LOSS train 0.440706471602122 valid 0.7558910250663757\n",
      "EPOCH 90: LOSS train 0.3916189869244893 valid 0.7507492303848267\n",
      "EPOCH 100: LOSS train 0.38056330879529315 valid 0.7432364225387573\n",
      "EPOCH 110: LOSS train 0.35213567813237506 valid 0.7406653165817261\n",
      "EPOCH 120: LOSS train 0.34444114565849304 valid 0.7363128066062927\n",
      "EPOCH 130: LOSS train 0.31825215617815655 valid 0.7352152466773987\n",
      "EPOCH 140: LOSS train 0.29881266752878827 valid 0.7316102385520935\n",
      "EPOCH 150: LOSS train 0.2802843749523163 valid 0.7280159592628479\n",
      "EPOCH 160: LOSS train 0.2810239295164744 valid 0.7237886786460876\n",
      "Epoch 00160: reducing learning rate of group 0 to 4.0527e-04.\n",
      "EPOCH 170: LOSS train 0.2433784157037735 valid 0.7196860909461975\n",
      "EPOCH 180: LOSS train 0.24383626381556192 valid 0.7183043956756592\n",
      "EPOCH 190: LOSS train 0.22647838791211447 valid 0.7178027629852295\n",
      "Epoch 00197: reducing learning rate of group 0 to 2.0264e-04.\n",
      "EPOCH 200: LOSS train 0.2195164958635966 valid 0.7175756692886353\n",
      "EPOCH 210: LOSS train 0.23130439718564352 valid 0.7173730731010437\n",
      "EPOCH 220: LOSS train 0.2229518344004949 valid 0.7165265083312988\n",
      "EPOCH 230: LOSS train 0.21268590788046518 valid 0.716318666934967\n",
      "EPOCH 240: LOSS train 0.2142542451620102 valid 0.7166778445243835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:42:00,404] Trial 82 finished with value: 0.7165442109107971 and parameters: {'weight_decay': 7.499835862199838e-06, 'L1': 2.886949549472956e-07, 'learning_rate': 0.0008105450750076353, 'dropout': 0.04716246002940919, 'first_layer': 512}. Best is trial 46 with value: 0.7119879126548767.\n",
      "[I 2023-08-24 18:42:00,951] Trial 83 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9063659509023031 valid 0.9062053561210632\n",
      "EPOCH 20: LOSS train 0.8080318371454874 valid 0.8493187427520752\n",
      "EPOCH 30: LOSS train 0.7059122721354166 valid 0.8168983459472656\n",
      "EPOCH 40: LOSS train 0.6612828969955444 valid 0.7966228127479553\n",
      "EPOCH 50: LOSS train 0.5604578653971354 valid 0.7761608958244324\n",
      "EPOCH 60: LOSS train 0.5136544307072958 valid 0.7649930119514465\n",
      "EPOCH 70: LOSS train 0.48617292443911236 valid 0.7583223581314087\n",
      "EPOCH 80: LOSS train 0.4363293449083964 valid 0.7517234683036804\n",
      "EPOCH 90: LOSS train 0.42490479350090027 valid 0.7424679398536682\n",
      "EPOCH 100: LOSS train 0.3991056978702545 valid 0.7370765209197998\n",
      "EPOCH 110: LOSS train 0.3538411458333333 valid 0.7336767315864563\n",
      "EPOCH 120: LOSS train 0.3200406034787496 valid 0.7311674952507019\n",
      "EPOCH 130: LOSS train 0.2994813621044159 valid 0.7289989590644836\n",
      "EPOCH 140: LOSS train 0.2959178189436595 valid 0.7281321287155151\n",
      "EPOCH 150: LOSS train 0.276040256023407 valid 0.7270005345344543\n",
      "EPOCH 160: LOSS train 0.26491891344388324 valid 0.7237836122512817\n",
      "EPOCH 170: LOSS train 0.25154226024945575 valid 0.7222672700881958\n",
      "EPOCH 180: LOSS train 0.25483331580956775 valid 0.7224308252334595\n",
      "Epoch 00186: reducing learning rate of group 0 to 3.9207e-04.\n",
      "EPOCH 190: LOSS train 0.21819248795509338 valid 0.7191381454467773\n",
      "EPOCH 200: LOSS train 0.21249572932720184 valid 0.7169421315193176\n",
      "EPOCH 210: LOSS train 0.20984035730361938 valid 0.7156211137771606\n",
      "EPOCH 220: LOSS train 0.19489267965157828 valid 0.7160501480102539\n",
      "EPOCH 230: LOSS train 0.19593976934750876 valid 0.715082049369812\n",
      "Epoch 00235: reducing learning rate of group 0 to 1.9604e-04.\n",
      "EPOCH 240: LOSS train 0.19181691110134125 valid 0.7134445905685425\n",
      "EPOCH 250: LOSS train 0.1858745664358139 valid 0.7123728394508362\n",
      "EPOCH 260: LOSS train 0.18800556659698486 valid 0.7125218510627747\n",
      "EPOCH 270: LOSS train 0.18106325467427573 valid 0.711428165435791\n",
      "Epoch 00274: reducing learning rate of group 0 to 9.8018e-05.\n",
      "EPOCH 280: LOSS train 0.18155096471309662 valid 0.7109066247940063\n",
      "Epoch 00285: reducing learning rate of group 0 to 4.9009e-05.\n",
      "EPOCH 290: LOSS train 0.16742215553919473 valid 0.7103627324104309\n",
      "EPOCH 300: LOSS train 0.17908810079097748 valid 0.7099622488021851\n",
      "Epoch 00308: reducing learning rate of group 0 to 2.4504e-05.\n",
      "EPOCH 310: LOSS train 0.16961111625035605 valid 0.7099369168281555\n",
      "Epoch 00319: reducing learning rate of group 0 to 1.2252e-05.\n",
      "EPOCH 320: LOSS train 0.17595058182875314 valid 0.7099950313568115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:44:08,907] Trial 84 finished with value: 0.7099388837814331 and parameters: {'weight_decay': 7.92019577790693e-06, 'L1': 1.696170709020603e-07, 'learning_rate': 0.0007841433738117765, 'dropout': 0.04435050647117976, 'first_layer': 512}. Best is trial 84 with value: 0.7099388837814331.\n",
      "[I 2023-08-24 18:44:09,398] Trial 85 pruned. \n",
      "[I 2023-08-24 18:44:09,888] Trial 86 pruned. \n",
      "[I 2023-08-24 18:44:10,308] Trial 87 pruned. \n",
      "[I 2023-08-24 18:44:10,712] Trial 88 pruned. \n",
      "[I 2023-08-24 18:44:11,125] Trial 89 pruned. \n",
      "[I 2023-08-24 18:44:11,515] Trial 90 pruned. \n",
      "[I 2023-08-24 18:44:15,058] Trial 91 pruned. \n",
      "[I 2023-08-24 18:44:15,893] Trial 92 pruned. \n",
      "[I 2023-08-24 18:44:16,403] Trial 93 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.899686594804128 valid 0.8953154683113098\n",
      "EPOCH 20: LOSS train 0.8183322548866272 valid 0.8469147086143494\n",
      "EPOCH 30: LOSS train 0.7127149105072021 valid 0.8194067478179932\n",
      "EPOCH 40: LOSS train 0.6619896292686462 valid 0.7913077473640442\n",
      "EPOCH 50: LOSS train 0.5936486124992371 valid 0.7840250134468079\n",
      "EPOCH 60: LOSS train 0.511844664812088 valid 0.7699636816978455\n",
      "EPOCH 70: LOSS train 0.47120201587677 valid 0.7545944452285767\n",
      "EPOCH 80: LOSS train 0.4247824350992839 valid 0.7477700114250183\n",
      "EPOCH 90: LOSS train 0.39837421973546344 valid 0.7475164532661438\n",
      "EPOCH 100: LOSS train 0.36102710167566937 valid 0.7442284226417542\n",
      "EPOCH 110: LOSS train 0.3677847683429718 valid 0.7421154379844666\n",
      "EPOCH 120: LOSS train 0.34151419003804523 valid 0.738457202911377\n",
      "EPOCH 130: LOSS train 0.3140309949715932 valid 0.7369002103805542\n",
      "EPOCH 140: LOSS train 0.31626012921333313 valid 0.7380295395851135\n",
      "EPOCH 150: LOSS train 0.2920709550380707 valid 0.7317113876342773\n",
      "Epoch 00155: reducing learning rate of group 0 to 4.9963e-04.\n",
      "EPOCH 160: LOSS train 0.26939475536346436 valid 0.7273122668266296\n",
      "EPOCH 170: LOSS train 0.25311705470085144 valid 0.7254826426506042\n",
      "EPOCH 180: LOSS train 0.2504298985004425 valid 0.722268283367157\n",
      "EPOCH 190: LOSS train 0.23684860269228616 valid 0.721859872341156\n",
      "EPOCH 200: LOSS train 0.23457682132720947 valid 0.722505509853363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:45:32,693] Trial 94 pruned. \n",
      "[I 2023-08-24 18:45:33,161] Trial 95 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.9015002051989237 valid 0.8991154432296753\n",
      "EPOCH 20: LOSS train 0.7830127278963724 valid 0.8367441892623901\n",
      "EPOCH 30: LOSS train 0.6777301033337911 valid 0.804577648639679\n",
      "EPOCH 40: LOSS train 0.5520719091097513 valid 0.7840337157249451\n",
      "EPOCH 50: LOSS train 0.4679309328397115 valid 0.7650690674781799\n",
      "EPOCH 60: LOSS train 0.42824816703796387 valid 0.7604491710662842\n",
      "EPOCH 70: LOSS train 0.3496352930863698 valid 0.7523722052574158\n",
      "EPOCH 80: LOSS train 0.3041362265745799 valid 0.744291365146637\n",
      "EPOCH 90: LOSS train 0.2670813699563344 valid 0.7400479912757874\n",
      "EPOCH 100: LOSS train 0.2536432445049286 valid 0.7438588738441467\n",
      "EPOCH 110: LOSS train 0.23398586610953012 valid 0.7390185594558716\n",
      "EPOCH 120: LOSS train 0.20583285888036093 valid 0.735120952129364\n",
      "EPOCH 130: LOSS train 0.20126953721046448 valid 0.7365003228187561\n",
      "EPOCH 140: LOSS train 0.1972067952156067 valid 0.7308589220046997\n",
      "EPOCH 150: LOSS train 0.19045096139113107 valid 0.7340288758277893\n",
      "EPOCH 160: LOSS train 0.19376103579998016 valid 0.736409604549408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 18:46:43,004] Trial 96 finished with value: 0.7411100268363953 and parameters: {'weight_decay': 1.3826991462510193e-05, 'L1': 6.521018687460175e-07, 'learning_rate': 0.0008790585193331017, 'dropout': 0.05262563814217478, 'first_layer': 1024}. Best is trial 84 with value: 0.7099388837814331.\n",
      "[I 2023-08-24 18:46:43,556] Trial 97 pruned. \n",
      "[I 2023-08-24 18:46:44,075] Trial 98 pruned. \n",
      "[I 2023-08-24 18:46:44,558] Trial 99 pruned. \n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "train_set_drug  = dataset(X_train_drug, y_train_drug)\n",
    "val_set_drug  = dataset(X_val_drug, y_val_drug)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=batch_size, shuffle=True)\n",
    "val_dl_drug = DataLoader(val_set_drug, batch_size=1, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673, 2412)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 256, 512]       1,235,456\n",
      "              ReLU-2             [-1, 256, 512]               0\n",
      "           Dropout-3             [-1, 256, 512]               0\n",
      "            Linear-4             [-1, 256, 512]         262,656\n",
      "              ReLU-5             [-1, 256, 512]               0\n",
      "           Dropout-6             [-1, 256, 512]               0\n",
      "            Linear-7             [-1, 256, 256]         131,328\n",
      "            Linear-8             [-1, 256, 512]         131,584\n",
      "              ReLU-9             [-1, 256, 512]               0\n",
      "          Dropout-10             [-1, 256, 512]               0\n",
      "           Linear-11             [-1, 256, 512]         262,656\n",
      "             ReLU-12             [-1, 256, 512]               0\n",
      "          Dropout-13             [-1, 256, 512]               0\n",
      "           Linear-14            [-1, 256, 2412]       1,237,356\n",
      "================================================================\n",
      "Total params: 3,261,036\n",
      "Trainable params: 3,261,036\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.36\n",
      "Forward/backward pass size (MB): 17.21\n",
      "Params size (MB): 12.44\n",
      "Estimated Total Size (MB): 32.01\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "EPOCH 10: LOSS train 0.8712390065193176\n",
      "EPOCH 20: LOSS train 0.7497995098431905\n",
      "EPOCH 30: LOSS train 0.6463123162587484\n",
      "EPOCH 40: LOSS train 0.5594772696495056\n",
      "EPOCH 50: LOSS train 0.48705510298411053\n",
      "EPOCH 60: LOSS train 0.42810608943303424\n",
      "EPOCH 70: LOSS train 0.38496069113413495\n",
      "EPOCH 80: LOSS train 0.3516508936882019\n",
      "EPOCH 90: LOSS train 0.31674930453300476\n",
      "EPOCH 100: LOSS train 0.2997869352499644\n",
      "EPOCH 110: LOSS train 0.2793225149313609\n",
      "EPOCH 120: LOSS train 0.2643078962961833\n",
      "EPOCH 130: LOSS train 0.2486264705657959\n",
      "EPOCH 140: LOSS train 0.23878054320812225\n",
      "EPOCH 150: LOSS train 0.2327403426170349\n",
      "EPOCH 160: LOSS train 0.22162201007207236\n",
      "EPOCH 170: LOSS train 0.2180475393931071\n",
      "EPOCH 180: LOSS train 0.21181230743726095\n",
      "EPOCH 190: LOSS train 0.20661812524000803\n",
      "EPOCH 200: LOSS train 0.19712986052036285\n",
      "EPOCH 210: LOSS train 0.19651345908641815\n",
      "EPOCH 220: LOSS train 0.19374563793341318\n",
      "EPOCH 230: LOSS train 0.19049554566542307\n",
      "EPOCH 240: LOSS train 0.1883616844813029\n",
      "EPOCH 250: LOSS train 0.18833517034848532\n",
      "EPOCH 260: LOSS train 0.1831128646930059\n",
      "EPOCH 270: LOSS train 0.1797365297873815\n",
      "EPOCH 280: LOSS train 0.17076184352238974\n",
      "Epoch 00284: reducing learning rate of group 0 to 3.9207e-04.\n",
      "EPOCH 290: LOSS train 0.16665111482143402\n",
      "EPOCH 300: LOSS train 0.1668273905913035\n",
      "EPOCH 310: LOSS train 0.16492159167925516\n",
      "Epoch 00314: reducing learning rate of group 0 to 1.9604e-04.\n",
      "EPOCH 320: LOSS train 0.16184940934181213\n",
      "EPOCH 330: LOSS train 0.1653558909893036\n",
      "EPOCH 340: LOSS train 0.16215160489082336\n",
      "EPOCH 350: LOSS train 0.15907445549964905\n",
      "Epoch 00354: reducing learning rate of group 0 to 9.8018e-05.\n",
      "EPOCH 360: LOSS train 0.16006946563720703\n",
      "EPOCH 370: LOSS train 0.157133420308431\n",
      "EPOCH 380: LOSS train 0.15795655051867166\n",
      "EPOCH 390: LOSS train 0.1591403434673945\n",
      "EPOCH 400: LOSS train 0.16257208585739136\n",
      "Epoch 00405: reducing learning rate of group 0 to 4.9009e-05.\n",
      "EPOCH 410: LOSS train 0.1584558387597402\n",
      "Epoch 00418: reducing learning rate of group 0 to 2.4504e-05.\n",
      "EPOCH 420: LOSS train 0.15755333999792734\n",
      "EPOCH 430: LOSS train 0.15638781090577444\n",
      "Epoch 00438: reducing learning rate of group 0 to 1.2252e-05.\n",
      "EPOCH 440: LOSS train 0.15923336148262024\n",
      "EPOCH 450: LOSS train 0.15618752439816794\n",
      "Epoch 00455: reducing learning rate of group 0 to 6.1261e-06.\n",
      "EPOCH 460: LOSS train 0.1548366198937098\n",
      "Epoch 00466: reducing learning rate of group 0 to 3.0631e-06.\n",
      "EPOCH 470: LOSS train 0.15941880146662393\n",
      "tensor(0.0673, device='cuda:0')\n",
      "tensor(0.0677, device='cuda:0')\n",
      "tensor(0.0715, device='cuda:0')\n",
      "torch.Size([161, 2412])\n"
     ]
    }
   ],
   "source": [
    "param_dict_34 = {'weight_decay': 1.5949621641126727e-05, 'L1': 2.0214217845240095e-07, 'learning_rate': 0.0007584607894578174, 'dropout': 0.05293127067160355, 'first_layer': 512}\n",
    "param_dict_46 = {'weight_decay': 1.4940271869850511e-05, 'L1': 2.035023742123312e-07, 'learning_rate': 0.0008215476035324593, 'dropout': 0.038985292466368573, 'first_layer': 512}\n",
    "# 84\n",
    "param_dict = {'weight_decay': 7.92019577790693e-06, 'L1': 1.696170709020603e-07, 'learning_rate': 0.0007841433738117765, 'dropout': 0.04435050647117976, 'first_layer': 512}\n",
    "\n",
    "batch_size=256\n",
    "X_drug, y_drug = drug_data, drug_data\n",
    "train_set_drug  = dataset(X_drug, y_drug)\n",
    "print(X_drug.shape)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=batch_size, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "\n",
    "def fit():\n",
    "    lr = param_dict[\"learning_rate\"]\n",
    "    dropout=param_dict[\"dropout\"]\n",
    "    weight_decay=param_dict[\"weight_decay\"]\n",
    "    L1 = param_dict[\"L1\"]\n",
    "    first_layer=param_dict[\"first_layer\"]\n",
    "\n",
    "    ae = AE([xi.shape[1],first_layer,512,256,512,first_layer], dropout=dropout)\n",
    "    print(summary(ae.to(\"cuda\"), xi.shape))\n",
    "    early_stopper = EarlyStopper(patience=30)\n",
    "    optimizer = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, verbose=True, patience=10, min_lr=1e-7)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    epoch_number = 0\n",
    "\n",
    "    EPOCHS = 500\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        ae.train(True)\n",
    "        ae = ae.to(device=device)\n",
    "\n",
    "        avg_loss = train_one_epoch(ae, epoch_number, \"writer\", train_dl_drug, optimizer, loss_fn, L1=L1,device=device)\n",
    "\n",
    "        # Set the ae to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        # ae.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "\n",
    "        if epoch_number%10==9:print('EPOCH {}:'.format(epoch_number + 1),'LOSS train {}'.format(avg_loss))\n",
    "        scheduler.step(avg_loss)\n",
    "        if early_stopper.early_stop(avg_loss):             \n",
    "            break\n",
    "        epoch_number+=1\n",
    "    return ae\n",
    "\n",
    "ae = fit()\n",
    "ae.eval()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_dl_drug):\n",
    "        ae = ae.to(device)\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = ae(inputs)\n",
    "        loss = loss_fn(outputs, labels) \n",
    "        print(loss)\n",
    "with open(\"output_drug1.txt\", mode=\"w\") as f: \n",
    "    print(outputs.shape)\n",
    "    [f.write(\n",
    "        str(outputs[0][i].to(\"cpu\").numpy().round(3))+\"   \"+\n",
    "        str(inputs[0][i].to(\"cpu\").numpy().round(3))+\"\\n\") for i in range(len(outputs[0]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m inputs_np \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mround(\u001b[39m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m outputs_np \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mround(\u001b[39m3\u001b[39m)\n\u001b[0;32m      4\u001b[0m inputs_0 \u001b[39m=\u001b[39m inputs_np[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "inputs_np = inputs.to(\"cpu\").numpy().round(3)\n",
    "outputs_np = outputs.to(\"cpu\").numpy().round(3)\n",
    "inputs_0 = inputs_np[0]\n",
    "outputs_0 = outputs_np[0]\n",
    "mask = inputs_0<1\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.scatter(inputs_0[mask][:100], outputs_0[mask][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, h_sizes, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[0], h_sizes[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[1], h_sizes[2]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[2], h_sizes[3])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  =\"../models/drug_encoder.pt\"\n",
    "torch.save(ae.encoder.state_dict(), path)\n",
    "\n",
    "encoder = Encoder([xi.shape[1],param_dict[\"first_layer\"],512,256], dropout=0)\n",
    "encoder.encoder.load_state_dict(torch.load(path))\n",
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_dl_drug):\n",
    "        encoder = encoder.to(device)\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = encoder(inputs)\n",
    "with open(\"output_encoder_drug.txt\", mode=\"w\") as f: \n",
    "    [f.write(\n",
    "        str(outputs[0][i].to(\"cpu\").numpy().round(3))+\"   \"+\n",
    "        str(inputs[0][i].to(\"cpu\").numpy().round(3))+\"\\n\") for i in range(len(outputs[0]))]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
