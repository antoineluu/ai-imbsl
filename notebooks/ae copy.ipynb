{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_pp import replace_cell_names_with_id\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_feat 23.298024 (1160, 5011)\n",
      "drug_feat 5.858004 (290, 5011)\n",
      "cell_feat 29.111364 (1450, 5011)\n"
     ]
    }
   ],
   "source": [
    "columns = [\"cell_line\", \"drugA_name\", \"drugB_name\", \"drugA_conc\", \"drugB_conc\", \"target\"]\n",
    "df_train = pd.read_csv(\"../data_raw/oneil.csv\", usecols=(1,2,3,4,5,12)).iloc[:,[0,1,3,2,4,5]].set_axis(columns, axis=1)\n",
    "df_train[\"cell_line\"]\n",
    "df_test = pd.read_csv(\"../data/test_yosua.csv\").set_axis(columns + [\"std\"], axis=1).convert_dtypes()\n",
    "\n",
    "# drug_data = pd.read_pickle(\"../data/drug_data.pkl.compress\", compression=\"gzip\")\n",
    "cell_data = pd.read_pickle(\"../data/cell_line_data.pkl.compress\", compression=\"gzip\")\n",
    "X_train_cell, X_val_cell, y_train_cell, y_val_cell = train_test_split(cell_data, cell_data, test_size=0.2, shuffle=True)\n",
    "df_train = replace_cell_names_with_id(dataframe=df_train, mapping_file=\"../data/mappingccl.csv\")\n",
    "df_test = replace_cell_names_with_id(dataframe=df_test, mapping_file=\"../data/mappingccl.csv\")\n",
    "# df_train = df_train[df_train.cell_line.isin(cell_data.index)]\n",
    "# df_train, df_val = train_test_split(df_train, test_size=0.2, shuffle=True)\n",
    "# cell_data = cell_data[cell_data.index.isin(pd.concat([df_train.cell_line, df_test.cell_line]))]\n",
    "# print(\"oneil\", df_train.memory_usage().sum()/1e6, df_train.shape,\"\\n\", df_train.dtypes)\n",
    "print(\"drug_feat\", X_train_cell.memory_usage().sum()/1e6, X_train_cell.shape)\n",
    "print(\"drug_feat\", X_val_cell.memory_usage().sum()/1e6, X_val_cell.shape)\n",
    "print(\"cell_feat\", cell_data.memory_usage().sum()/1e6, cell_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5011]) torch.Size([64, 5011])\n"
     ]
    }
   ],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.dataset = data.to_numpy()\n",
    "        self.labels = labels.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx], self.labels[idx]\n",
    "                \n",
    "train_set_cell  = dataset(X_train_cell, y_train_cell)\n",
    "val_set_cell  = dataset(X_val_cell, y_val_cell)\n",
    "train_dl_cell = DataLoader(train_set_cell, batch_size=64, shuffle=True)\n",
    "val_dl_cell = DataLoader(val_set_cell, batch_size=64, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_cell))\n",
    "print(xi.shape, yi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5011]) torch.Size([64, 5011])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a PyTorch class\n",
    "# 28*28 ==> 9 ==> 28*28\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, h_sizes, dropout=0.2):\n",
    "        super().__init__()\n",
    "         \n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[0], h_sizes[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[1], h_sizes[2]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[2], h_sizes[3])\n",
    "        )\n",
    "         \n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # The Sigmoid activation function\n",
    "        # outputs the value between 0 and 1\n",
    "        # 9 ==> 784\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[3], h_sizes[4]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[4], h_sizes[5]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[5], h_sizes[0])\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "ae = AE([xi.shape[1],512,512,256,512,512])\n",
    "yi = ae.forward(xi)\n",
    "print(xi.shape, yi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, epoch_index, tb_writer, training_loader, optimizer, loss_fn, device, L1,verbose=False):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    model = model.to(device)\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(inputs)\n",
    "\n",
    "        params = torch.cat([x.view(-1) for x in model.parameters()])\n",
    "        l1_regularization = L1 * torch.linalg.vector_norm(params, 1)\n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels) + l1_regularization\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # last_loss =  loss.item() / inputs.shape[0]  # loss per sample\n",
    "        # if verbose:print('  batch {} loss: {}'.format(i + 1, last_loss), outputs[0][0].item(), labels[0][0].item())\n",
    "        # tb_x = epoch_index * len(training_loader) + i + 1\n",
    "        # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "    \n",
    "    return running_loss / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "batch_size=256\n",
    "train_set_cell  = dataset(X_train_cell, y_train_cell)\n",
    "val_set_cell  = dataset(X_val_cell, y_val_cell)\n",
    "train_dl_cell = DataLoader(train_set_cell, batch_size=batch_size, shuffle=True)\n",
    "val_dl_cell = DataLoader(val_set_cell, batch_size=batch_size, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_cell))\n",
    "\n",
    "def objective(trial=None):\n",
    "    if trial is None:\n",
    "        lr = 1e-3\n",
    "        dropout=0.2\n",
    "        weight_decay=1e-4\n",
    "        first_layer=1024\n",
    "        L1=1e-6\n",
    "    else:\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "        L1 = trial.suggest_float(\"L1\", 1e-8, 1e-4, log=True)\n",
    "\n",
    "        lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.3) \n",
    "        first_layer = trial.suggest_categorical(\"first_layer\", [512,256,128])\n",
    "\n",
    "    ae = AE([xi.shape[1],first_layer,256,256,256,first_layer], dropout=dropout)\n",
    "    # print(summary(ae.to(\"cuda\"), xi.shape))\n",
    "    early_stopper = EarlyStopper(patience=20)\n",
    "    optimizer = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, verbose=True, patience=10, min_lr=1e-7)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    # timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    epoch_number = 0\n",
    "\n",
    "    EPOCHS = 500\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        ae.train(True)\n",
    "        ae = ae.to(device=device)\n",
    "\n",
    "        avg_loss = train_one_epoch(ae, epoch_number, \"writer\", train_dl_cell, optimizer, loss_fn, L1=L1,device=device)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        # Set the ae to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        ae.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_dl_cell):\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs = vinputs.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "                voutputs = ae(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss\n",
    "        avg_vloss = running_vloss / (i + 1) \n",
    "\n",
    "        if epoch_number%10==9:print('EPOCH {}:'.format(epoch_number + 1),'LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if early_stopper.early_stop(avg_vloss):             \n",
    "            break\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(avg_vloss, epoch_number)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        epoch_number += 1\n",
    "\n",
    "    return avg_vloss\n",
    "\n",
    "# objective()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, data in enumerate(train_dl_cell):\n",
    "#         inputs, labels = data\n",
    "#         inputs = inputs.to(device=device)\n",
    "#         labels = labels.to(device=device)\n",
    "#         outputs = ae(inputs)\n",
    "# with open(\"output_cell.txt\", mode=\"w\") as f: \n",
    "#     [f.write(\n",
    "#         str(outputs[0][i].to(\"cpu\").numpy().round(3))+\"   \"+\n",
    "#         str(inputs[0][i].to(\"cpu\").numpy().round(3))+\"\\n\") for i in range(len(outputs[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:29:32,886] A new study created in memory with name: no-name-70ba8cbd-c40b-487d-9d36-9591baa5c401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 1.0399535497029622 valid 1.0021177530288696\n",
      "EPOCH 20: LOSS train 1.013437271118164 valid 0.971034586429596\n",
      "EPOCH 30: LOSS train 0.9534526864687601 valid 0.9129434823989868\n",
      "EPOCH 40: LOSS train 0.8921734690666199 valid 0.850837230682373\n",
      "EPOCH 50: LOSS train 0.8492590586344401 valid 0.7979537844657898\n",
      "EPOCH 60: LOSS train 0.8078086773554484 valid 0.7579486966133118\n",
      "EPOCH 70: LOSS train 0.7870866854985555 valid 0.7291848659515381\n",
      "EPOCH 80: LOSS train 0.759340743223826 valid 0.7084951400756836\n",
      "EPOCH 90: LOSS train 0.7449156244595846 valid 0.6918842196464539\n",
      "EPOCH 100: LOSS train 0.7303123474121094 valid 0.6791356205940247\n",
      "EPOCH 110: LOSS train 0.7140375177065531 valid 0.6683578491210938\n",
      "EPOCH 120: LOSS train 0.7023919026056925 valid 0.6600361466407776\n",
      "EPOCH 130: LOSS train 0.6936354239781698 valid 0.6517137289047241\n",
      "EPOCH 140: LOSS train 0.6922606229782104 valid 0.6440206170082092\n",
      "Epoch 00146: reducing learning rate of group 0 to 3.6824e-05.\n",
      "EPOCH 150: LOSS train 0.6788631280263265 valid 0.6376215219497681\n",
      "EPOCH 160: LOSS train 0.6754691402117411 valid 0.6333644986152649\n",
      "EPOCH 170: LOSS train 0.6797877748807272 valid 0.6298696994781494\n",
      "EPOCH 180: LOSS train 0.6689949234326681 valid 0.6259918808937073\n",
      "EPOCH 190: LOSS train 0.6669700741767883 valid 0.6223935484886169\n",
      "EPOCH 200: LOSS train 0.6648300886154175 valid 0.618903636932373\n",
      "Epoch 00200: reducing learning rate of group 0 to 1.8412e-05.\n",
      "EPOCH 210: LOSS train 0.6551502148310343 valid 0.6167745590209961\n",
      "Epoch 00216: reducing learning rate of group 0 to 9.2061e-06.\n",
      "EPOCH 220: LOSS train 0.662686268488566 valid 0.6155582666397095\n",
      "Epoch 00227: reducing learning rate of group 0 to 4.6030e-06.\n",
      "EPOCH 230: LOSS train 0.6592394312222799 valid 0.6146015524864197\n",
      "EPOCH 240: LOSS train 0.6570255557696024 valid 0.6142014861106873\n",
      "Epoch 00240: reducing learning rate of group 0 to 2.3015e-06.\n",
      "EPOCH 250: LOSS train 0.6565120021502177 valid 0.6139801740646362\n",
      "Epoch 00251: reducing learning rate of group 0 to 1.1508e-06.\n",
      "EPOCH 260: LOSS train 0.6573523283004761 valid 0.613810658454895\n",
      "Epoch 00269: reducing learning rate of group 0 to 5.7538e-07.\n",
      "EPOCH 270: LOSS train 0.6584526300430298 valid 0.6136882305145264\n",
      "EPOCH 280: LOSS train 0.6534289518992106 valid 0.6136440634727478\n",
      "Epoch 00280: reducing learning rate of group 0 to 2.8769e-07.\n",
      "EPOCH 290: LOSS train 0.6576464374860128 valid 0.6136344075202942\n",
      "Epoch 00291: reducing learning rate of group 0 to 1.4384e-07.\n",
      "EPOCH 300: LOSS train 0.66007928053538 valid 0.613623321056366\n",
      "Epoch 00302: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 310: LOSS train 0.6533095836639404 valid 0.6136109828948975\n",
      "EPOCH 320: LOSS train 0.6613356073697408 valid 0.6136065125465393\n",
      "EPOCH 330: LOSS train 0.6575681964556376 valid 0.6135962009429932\n",
      "EPOCH 340: LOSS train 0.6543590823809305 valid 0.6135857701301575\n",
      "EPOCH 350: LOSS train 0.6586054762204488 valid 0.613576352596283\n",
      "EPOCH 360: LOSS train 0.657424807548523 valid 0.6135627627372742\n",
      "EPOCH 370: LOSS train 0.6568427483240763 valid 0.6135523319244385\n",
      "EPOCH 380: LOSS train 0.6550533572832743 valid 0.6135377287864685\n",
      "EPOCH 390: LOSS train 0.6599822839101156 valid 0.6135298013687134\n",
      "EPOCH 400: LOSS train 0.6513227820396423 valid 0.6135162711143494\n",
      "EPOCH 410: LOSS train 0.6555865804354349 valid 0.6135129332542419\n",
      "EPOCH 420: LOSS train 0.6505475044250488 valid 0.6135033965110779\n",
      "EPOCH 430: LOSS train 0.6541003187497457 valid 0.613495409488678\n",
      "EPOCH 440: LOSS train 0.6545476317405701 valid 0.6134856343269348\n",
      "EPOCH 450: LOSS train 0.656915565331777 valid 0.6134775280952454\n",
      "EPOCH 460: LOSS train 0.6508469780286154 valid 0.6134692430496216\n",
      "EPOCH 470: LOSS train 0.6602627436319987 valid 0.6134597063064575\n",
      "EPOCH 480: LOSS train 0.6571539839108785 valid 0.6134515404701233\n",
      "EPOCH 490: LOSS train 0.6564891537030538 valid 0.6134443879127502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:31:38,714] Trial 0 finished with value: 0.6134387254714966 and parameters: {'weight_decay': 3.908078684920513e-05, 'L1': 1.0644429752387261e-06, 'learning_rate': 7.364849091239817e-05, 'dropout': 0.06319706196670695, 'first_layer': 128}. Best is trial 0 with value: 0.6134387254714966.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 0.6557901700337728 valid 0.6134387254714966\n",
      "EPOCH 10: LOSS train 0.9963334600130717 valid 1.0087085962295532\n",
      "EPOCH 20: LOSS train 0.9996483127276102 valid 1.0079221725463867\n",
      "Epoch 00025: reducing learning rate of group 0 to 6.3664e-06.\n",
      "EPOCH 30: LOSS train 0.9950103561083475 valid 1.0068727731704712\n",
      "Epoch 00036: reducing learning rate of group 0 to 3.1832e-06.\n",
      "EPOCH 40: LOSS train 0.9929454525311788 valid 1.0060256719589233\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.5916e-06.\n",
      "EPOCH 50: LOSS train 1.0038802027702332 valid 1.0054866075515747\n",
      "EPOCH 60: LOSS train 1.0033925374348958 valid 1.0051544904708862\n",
      "Epoch 00068: reducing learning rate of group 0 to 7.9581e-07.\n",
      "EPOCH 70: LOSS train 1.0033976833025615 valid 1.004851222038269\n",
      "Epoch 00079: reducing learning rate of group 0 to 3.9790e-07.\n",
      "EPOCH 80: LOSS train 0.9899460275967916 valid 1.0046874284744263\n",
      "EPOCH 90: LOSS train 0.9947495261828104 valid 1.004602313041687\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.9895e-07.\n",
      "EPOCH 100: LOSS train 1.0011819402376811 valid 1.0045266151428223\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 110: LOSS train 1.003885289033254 valid 1.004486322402954\n",
      "EPOCH 120: LOSS train 0.985191543896993 valid 1.0044652223587036\n",
      "EPOCH 130: LOSS train 1.0021262764930725 valid 1.0044447183609009\n",
      "EPOCH 140: LOSS train 0.9932881792386373 valid 1.0044243335723877\n",
      "EPOCH 150: LOSS train 0.9988324244817098 valid 1.0044039487838745\n",
      "EPOCH 160: LOSS train 0.9873305956522623 valid 1.0043836832046509\n",
      "EPOCH 170: LOSS train 0.9947564999262491 valid 1.004363775253296\n",
      "EPOCH 180: LOSS train 0.9981818199157715 valid 1.0043437480926514\n",
      "EPOCH 190: LOSS train 0.993510882059733 valid 1.0043240785598755\n",
      "EPOCH 200: LOSS train 0.9988994399706522 valid 1.0043041706085205\n",
      "EPOCH 210: LOSS train 0.9984480142593384 valid 1.0042845010757446\n",
      "EPOCH 220: LOSS train 0.9887675642967224 valid 1.0042645931243896\n",
      "EPOCH 230: LOSS train 0.9902381499608358 valid 1.0042449235916138\n",
      "EPOCH 240: LOSS train 1.0004717111587524 valid 1.0042251348495483\n",
      "EPOCH 250: LOSS train 1.0060184200604756 valid 1.004205346107483\n",
      "EPOCH 260: LOSS train 0.9988895058631897 valid 1.004185438156128\n",
      "EPOCH 270: LOSS train 0.9901351531346639 valid 1.004165768623352\n",
      "EPOCH 280: LOSS train 0.9984927376111349 valid 1.0041460990905762\n",
      "EPOCH 290: LOSS train 0.9998287955919901 valid 1.0041263103485107\n",
      "EPOCH 300: LOSS train 1.002458353837331 valid 1.0041061639785767\n",
      "EPOCH 310: LOSS train 0.993167499701182 valid 1.0040863752365112\n",
      "EPOCH 320: LOSS train 1.002510945002238 valid 1.0040665864944458\n",
      "EPOCH 330: LOSS train 0.9874735275904337 valid 1.0040465593338013\n",
      "EPOCH 340: LOSS train 1.0055968364079793 valid 1.0040266513824463\n",
      "EPOCH 350: LOSS train 1.0033122102419536 valid 1.0040065050125122\n",
      "EPOCH 360: LOSS train 0.9990952610969543 valid 1.0039862394332886\n",
      "EPOCH 370: LOSS train 0.9947143395741781 valid 1.0039660930633545\n",
      "EPOCH 380: LOSS train 0.9874843756357828 valid 1.0039459466934204\n",
      "EPOCH 390: LOSS train 0.987756590048472 valid 1.0039260387420654\n",
      "EPOCH 400: LOSS train 0.9913162986437479 valid 1.0039061307907104\n",
      "EPOCH 410: LOSS train 0.9935379227002462 valid 1.0038859844207764\n",
      "EPOCH 420: LOSS train 1.0010129809379578 valid 1.0038655996322632\n",
      "EPOCH 430: LOSS train 0.9955385327339172 valid 1.0038450956344604\n",
      "EPOCH 440: LOSS train 0.9911868373552958 valid 1.003825068473816\n",
      "EPOCH 450: LOSS train 0.9981228510538737 valid 1.003805160522461\n",
      "EPOCH 460: LOSS train 0.9874927401542664 valid 1.003785252571106\n",
      "EPOCH 470: LOSS train 0.9970215360323588 valid 1.0037651062011719\n",
      "EPOCH 480: LOSS train 1.0020992358525593 valid 1.0037447214126587\n",
      "EPOCH 490: LOSS train 1.0000826120376587 valid 1.0037243366241455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:33:30,881] Trial 1 finished with value: 1.003704309463501 and parameters: {'weight_decay': 7.620705189515033e-06, 'L1': 1.6439494273361578e-08, 'learning_rate': 1.2732891632726681e-05, 'dropout': 0.2074769087491895, 'first_layer': 128}. Best is trial 0 with value: 0.6134387254714966.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 1.0099911093711853 valid 1.003704309463501\n",
      "EPOCH 10: LOSS train 0.9441302021344503 valid 0.8949863910675049\n",
      "EPOCH 20: LOSS train 0.8208324909210205 valid 0.7710355520248413\n",
      "EPOCH 30: LOSS train 0.7659608324368795 valid 0.7192613482475281\n",
      "EPOCH 40: LOSS train 0.7343475619951884 valid 0.7067320942878723\n",
      "EPOCH 50: LOSS train 0.7149730523427328 valid 0.685204029083252\n",
      "EPOCH 60: LOSS train 0.7008874217669169 valid 0.6748453378677368\n",
      "EPOCH 70: LOSS train 0.695732315381368 valid 0.6737874746322632\n",
      "EPOCH 80: LOSS train 0.6803900599479675 valid 0.65323805809021\n",
      "EPOCH 90: LOSS train 0.6676454941431681 valid 0.6397547125816345\n",
      "EPOCH 100: LOSS train 0.6554296414057413 valid 0.6295303106307983\n",
      "EPOCH 110: LOSS train 0.6517331004142761 valid 0.6218972206115723\n",
      "EPOCH 120: LOSS train 0.6410540739695231 valid 0.6133564710617065\n",
      "EPOCH 130: LOSS train 0.6394652922948202 valid 0.6100166440010071\n",
      "EPOCH 140: LOSS train 0.6259724100430807 valid 0.5963703989982605\n",
      "EPOCH 150: LOSS train 0.6283387343088785 valid 0.5812561511993408\n",
      "EPOCH 160: LOSS train 0.6120542486508688 valid 0.5848388671875\n",
      "EPOCH 170: LOSS train 0.6067656874656677 valid 0.578347384929657\n",
      "EPOCH 180: LOSS train 0.6055238048235575 valid 0.5704174637794495\n",
      "EPOCH 190: LOSS train 0.6087483565012614 valid 0.5707535743713379\n",
      "EPOCH 200: LOSS train 0.5983789364496866 valid 0.5591312646865845\n",
      "EPOCH 210: LOSS train 0.5911080638567606 valid 0.5575141906738281\n",
      "Epoch 00217: reducing learning rate of group 0 to 1.8529e-04.\n",
      "EPOCH 220: LOSS train 0.5941411058108012 valid 0.5539456009864807\n",
      "EPOCH 230: LOSS train 0.5922603011131287 valid 0.5512346625328064\n",
      "EPOCH 240: LOSS train 0.5798931121826172 valid 0.5507655143737793\n",
      "Epoch 00245: reducing learning rate of group 0 to 9.2645e-05.\n",
      "EPOCH 250: LOSS train 0.5897634029388428 valid 0.5484955310821533\n",
      "Epoch 00256: reducing learning rate of group 0 to 4.6322e-05.\n",
      "EPOCH 260: LOSS train 0.5785263975461324 valid 0.5478467345237732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:34:29,678] Trial 2 finished with value: 0.5471774935722351 and parameters: {'weight_decay': 1.501080306589278e-08, 'L1': 3.409887186652852e-07, 'learning_rate': 0.00037057986232507203, 'dropout': 0.29904751938164487, 'first_layer': 128}. Best is trial 2 with value: 0.5471774935722351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 2.914337158203125 valid 1.0070710182189941\n",
      "EPOCH 20: LOSS train 2.6506134668986 valid 1.0069704055786133\n",
      "EPOCH 30: LOSS train 2.40913979212443 valid 1.0068860054016113\n",
      "EPOCH 40: LOSS train 2.2014331817626953 valid 1.0067965984344482\n",
      "EPOCH 50: LOSS train 2.036083380381266 valid 1.0067178010940552\n",
      "EPOCH 60: LOSS train 1.901499072710673 valid 1.006655216217041\n",
      "EPOCH 70: LOSS train 1.769699255625407 valid 1.0066043138504028\n",
      "EPOCH 80: LOSS train 1.6712825695673625 valid 1.0065597295761108\n",
      "EPOCH 90: LOSS train 1.5802961587905884 valid 1.0065195560455322\n",
      "EPOCH 100: LOSS train 1.4823646942774455 valid 1.006481647491455\n",
      "EPOCH 110: LOSS train 1.4053330421447754 valid 1.0064506530761719\n",
      "EPOCH 120: LOSS train 1.3242533206939697 valid 1.006421685218811\n",
      "EPOCH 130: LOSS train 1.2623378038406372 valid 1.0063990354537964\n",
      "EPOCH 140: LOSS train 1.200508673985799 valid 1.0063772201538086\n",
      "EPOCH 150: LOSS train 1.1603472630182903 valid 1.0063598155975342\n",
      "EPOCH 160: LOSS train 1.111633022626241 valid 1.0063467025756836\n",
      "EPOCH 170: LOSS train 1.069092869758606 valid 1.0063345432281494\n",
      "EPOCH 180: LOSS train 1.050567348798116 valid 1.0063225030899048\n",
      "EPOCH 190: LOSS train 1.0368858973185222 valid 1.0063114166259766\n",
      "EPOCH 200: LOSS train 1.0179975430170696 valid 1.0063023567199707\n",
      "EPOCH 210: LOSS train 1.0155483484268188 valid 1.0062928199768066\n",
      "EPOCH 220: LOSS train 1.0041322708129883 valid 1.0062841176986694\n",
      "EPOCH 230: LOSS train 1.0055781205495198 valid 1.006274938583374\n",
      "Epoch 00231: reducing learning rate of group 0 to 3.6349e-05.\n",
      "EPOCH 240: LOSS train 1.0168283979098003 valid 1.0062698125839233\n",
      "Epoch 00248: reducing learning rate of group 0 to 1.8174e-05.\n",
      "EPOCH 250: LOSS train 1.0161320169766743 valid 1.0062663555145264\n",
      "Epoch 00259: reducing learning rate of group 0 to 9.0872e-06.\n",
      "EPOCH 260: LOSS train 1.0054507652918498 valid 1.006264567375183\n",
      "EPOCH 270: LOSS train 1.0004869500796 valid 1.0062634944915771\n",
      "EPOCH 280: LOSS train 1.002760112285614 valid 1.0062625408172607\n",
      "Epoch 00280: reducing learning rate of group 0 to 4.5436e-06.\n",
      "EPOCH 290: LOSS train 1.0083407958348591 valid 1.0062620639801025\n",
      "Epoch 00291: reducing learning rate of group 0 to 2.2718e-06.\n",
      "EPOCH 300: LOSS train 1.0005286931991577 valid 1.0062617063522339\n",
      "Epoch 00302: reducing learning rate of group 0 to 1.1359e-06.\n",
      "EPOCH 310: LOSS train 0.9920231898625692 valid 1.0062617063522339\n",
      "Epoch 00313: reducing learning rate of group 0 to 5.6795e-07.\n",
      "EPOCH 320: LOSS train 1.0056089957555134 valid 1.0062615871429443\n",
      "Epoch 00324: reducing learning rate of group 0 to 2.8398e-07.\n",
      "EPOCH 330: LOSS train 1.0037563840548198 valid 1.0062614679336548\n",
      "Epoch 00336: reducing learning rate of group 0 to 1.4199e-07.\n",
      "EPOCH 340: LOSS train 1.0079391996065776 valid 1.0062614679336548\n",
      "Epoch 00347: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 350: LOSS train 0.9959526062011719 valid 1.0062614679336548\n",
      "EPOCH 360: LOSS train 1.0015265345573425 valid 1.0062614679336548\n",
      "EPOCH 370: LOSS train 0.9997167388598124 valid 1.0062614679336548\n",
      "EPOCH 380: LOSS train 0.9980146686236063 valid 1.0062613487243652\n",
      "EPOCH 390: LOSS train 1.0012761950492859 valid 1.0062613487243652\n",
      "EPOCH 400: LOSS train 1.0065461198488872 valid 1.0062613487243652\n",
      "EPOCH 410: LOSS train 0.9985443154970804 valid 1.0062613487243652\n",
      "EPOCH 420: LOSS train 1.0003094871838887 valid 1.0062613487243652\n",
      "EPOCH 430: LOSS train 0.9978840947151184 valid 1.0062613487243652\n",
      "EPOCH 440: LOSS train 1.0096646149953206 valid 1.0062613487243652\n",
      "EPOCH 450: LOSS train 1.0011976559956868 valid 1.0062613487243652\n",
      "EPOCH 460: LOSS train 1.0050559838612874 valid 1.0062613487243652\n",
      "EPOCH 470: LOSS train 0.9959911902745565 valid 1.0062613487243652\n",
      "EPOCH 480: LOSS train 1.0162908832232158 valid 1.0062613487243652\n",
      "EPOCH 490: LOSS train 1.0101385712623596 valid 1.0062613487243652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:36:31,760] Trial 3 finished with value: 1.0062612295150757 and parameters: {'weight_decay': 3.7882230262288707e-06, 'L1': 2.539273381967826e-05, 'learning_rate': 7.269780468915152e-05, 'dropout': 0.17601422385262083, 'first_layer': 512}. Best is trial 2 with value: 0.5471774935722351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 1.0072216987609863 valid 1.0062612295150757\n",
      "EPOCH 10: LOSS train 1.0040526191393535 valid 1.006306767463684\n",
      "EPOCH 20: LOSS train 0.9996771216392517 valid 0.9993407726287842\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0915e-05.\n",
      "EPOCH 30: LOSS train 0.9794229865074158 valid 0.9895381927490234\n",
      "EPOCH 40: LOSS train 0.982856829961141 valid 0.9752549529075623\n",
      "EPOCH 50: LOSS train 0.9585199952125549 valid 0.9578486084938049\n",
      "EPOCH 60: LOSS train 0.9427702029546102 valid 0.9389966130256653\n",
      "EPOCH 70: LOSS train 0.9361519813537598 valid 0.9189887046813965\n",
      "EPOCH 80: LOSS train 0.921503484249115 valid 0.8998025059700012\n",
      "EPOCH 90: LOSS train 0.907498300075531 valid 0.8811278939247131\n",
      "EPOCH 100: LOSS train 0.8979941805203756 valid 0.8637794852256775\n",
      "EPOCH 110: LOSS train 0.8763161301612854 valid 0.8478327989578247\n",
      "EPOCH 120: LOSS train 0.8672784765561422 valid 0.8333770036697388\n",
      "EPOCH 130: LOSS train 0.8553104003270467 valid 0.8206698894500732\n",
      "EPOCH 140: LOSS train 0.8432207504908243 valid 0.8093137145042419\n",
      "EPOCH 150: LOSS train 0.8312344352404276 valid 0.7987161874771118\n",
      "EPOCH 160: LOSS train 0.8285223046938578 valid 0.7891919612884521\n",
      "EPOCH 170: LOSS train 0.8158198595046997 valid 0.7805246710777283\n",
      "EPOCH 180: LOSS train 0.8154616355895996 valid 0.7721168398857117\n",
      "EPOCH 190: LOSS train 0.7988355954488119 valid 0.7662160396575928\n",
      "EPOCH 200: LOSS train 0.794994572798411 valid 0.7603315711021423\n",
      "EPOCH 210: LOSS train 0.7965434590975443 valid 0.7545225620269775\n",
      "EPOCH 220: LOSS train 0.7852235635121664 valid 0.7486376166343689\n",
      "Epoch 00226: reducing learning rate of group 0 to 5.4577e-06.\n",
      "EPOCH 230: LOSS train 0.784340480963389 valid 0.7453133463859558\n",
      "Epoch 00237: reducing learning rate of group 0 to 2.7288e-06.\n",
      "EPOCH 240: LOSS train 0.786405881245931 valid 0.7434385418891907\n",
      "Epoch 00249: reducing learning rate of group 0 to 1.3644e-06.\n",
      "EPOCH 250: LOSS train 0.7821763753890991 valid 0.7423058152198792\n",
      "EPOCH 260: LOSS train 0.7853367726008097 valid 0.7418608069419861\n",
      "Epoch 00260: reducing learning rate of group 0 to 6.8221e-07.\n",
      "EPOCH 270: LOSS train 0.7809288501739502 valid 0.7417104840278625\n",
      "Epoch 00271: reducing learning rate of group 0 to 3.4110e-07.\n",
      "EPOCH 280: LOSS train 0.7816510001818339 valid 0.7416311502456665\n",
      "Epoch 00282: reducing learning rate of group 0 to 1.7055e-07.\n",
      "EPOCH 290: LOSS train 0.7765757044156393 valid 0.7415773272514343\n",
      "Epoch 00293: reducing learning rate of group 0 to 1.0000e-07.\n",
      "EPOCH 300: LOSS train 0.7825504342714945 valid 0.7415050864219666\n",
      "EPOCH 310: LOSS train 0.782300591468811 valid 0.741465151309967\n",
      "EPOCH 320: LOSS train 0.7772255937258402 valid 0.7414166927337646\n",
      "EPOCH 330: LOSS train 0.7812310457229614 valid 0.7413635849952698\n",
      "EPOCH 340: LOSS train 0.7830695907274882 valid 0.7413156628608704\n",
      "EPOCH 350: LOSS train 0.7797771493593851 valid 0.7412570118904114\n",
      "EPOCH 360: LOSS train 0.780087431271871 valid 0.7412284016609192\n",
      "EPOCH 370: LOSS train 0.7757125695546468 valid 0.7412151098251343\n",
      "EPOCH 380: LOSS train 0.7759346763292948 valid 0.7411840558052063\n",
      "EPOCH 390: LOSS train 0.7808499336242676 valid 0.7411445379257202\n",
      "EPOCH 400: LOSS train 0.7777300477027893 valid 0.741115391254425\n",
      "EPOCH 410: LOSS train 0.7811110615730286 valid 0.7410815358161926\n",
      "EPOCH 420: LOSS train 0.7809890707333883 valid 0.7410523295402527\n",
      "EPOCH 430: LOSS train 0.7727512121200562 valid 0.741012692451477\n",
      "EPOCH 440: LOSS train 0.7877637346585592 valid 0.7409650683403015\n",
      "EPOCH 450: LOSS train 0.7797550161679586 valid 0.7409045696258545\n",
      "EPOCH 460: LOSS train 0.7739858031272888 valid 0.7408715486526489\n",
      "EPOCH 470: LOSS train 0.7812679012616476 valid 0.7408387660980225\n",
      "EPOCH 480: LOSS train 0.7758174737294515 valid 0.7408050298690796\n",
      "EPOCH 490: LOSS train 0.7834079066912333 valid 0.740738570690155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:38:36,441] Trial 4 finished with value: 0.7406899333000183 and parameters: {'weight_decay': 4.8199930646767894e-08, 'L1': 2.602796910695003e-08, 'learning_rate': 2.1830709805200056e-05, 'dropout': 0.22155454998907181, 'first_layer': 256}. Best is trial 2 with value: 0.5471774935722351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 0.7784575422604879 valid 0.7406899333000183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:38:38,647] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 1.0465342998504639 valid 0.9877204298973083\n",
      "EPOCH 20: LOSS train 0.9295098582903544 valid 0.8320407271385193\n",
      "EPOCH 30: LOSS train 0.8440719048182169 valid 0.7426065802574158\n",
      "EPOCH 40: LOSS train 0.800281286239624 valid 0.7076164484024048\n",
      "EPOCH 50: LOSS train 0.7726508378982544 valid 0.6843662858009338\n",
      "EPOCH 60: LOSS train 0.7533137400945028 valid 0.6704912781715393\n",
      "EPOCH 70: LOSS train 0.7344982624053955 valid 0.6612252593040466\n",
      "EPOCH 80: LOSS train 0.7266733050346375 valid 0.652787983417511\n",
      "EPOCH 90: LOSS train 0.7115639646848043 valid 0.6427962779998779\n",
      "EPOCH 100: LOSS train 0.695933202902476 valid 0.6312980651855469\n",
      "EPOCH 110: LOSS train 0.6891065438588461 valid 0.6216166615486145\n",
      "EPOCH 120: LOSS train 0.6848931312561035 valid 0.613152801990509\n",
      "EPOCH 130: LOSS train 0.6770159800847372 valid 0.6056795716285706\n",
      "EPOCH 140: LOSS train 0.6659954984982809 valid 0.5993486046791077\n",
      "EPOCH 150: LOSS train 0.6619741916656494 valid 0.5922842025756836\n",
      "EPOCH 160: LOSS train 0.6464559038480123 valid 0.5854091644287109\n",
      "EPOCH 170: LOSS train 0.6383148034413656 valid 0.5792222619056702\n",
      "EPOCH 180: LOSS train 0.6369616389274597 valid 0.5732607841491699\n",
      "EPOCH 190: LOSS train 0.6325957377751669 valid 0.5671370625495911\n",
      "EPOCH 200: LOSS train 0.6236277421315511 valid 0.5621060132980347\n",
      "EPOCH 210: LOSS train 0.6174705624580383 valid 0.5564586520195007\n",
      "EPOCH 220: LOSS train 0.6117292245229086 valid 0.5530501008033752\n",
      "EPOCH 230: LOSS train 0.6146263678868612 valid 0.5483366847038269\n",
      "Epoch 00232: reducing learning rate of group 0 to 3.5972e-05.\n",
      "EPOCH 240: LOSS train 0.6007526318232218 valid 0.5455451607704163\n",
      "EPOCH 250: LOSS train 0.6024232308069865 valid 0.5434647798538208\n",
      "EPOCH 260: LOSS train 0.6010656555493673 valid 0.5415849685668945\n",
      "Epoch 00260: reducing learning rate of group 0 to 1.7986e-05.\n",
      "EPOCH 270: LOSS train 0.5985622406005859 valid 0.5405057072639465\n",
      "Epoch 00272: reducing learning rate of group 0 to 8.9930e-06.\n",
      "EPOCH 280: LOSS train 0.5967647631963094 valid 0.5397281050682068\n",
      "EPOCH 290: LOSS train 0.5969300866127014 valid 0.5396773219108582\n",
      "EPOCH 300: LOSS train 0.6010492245356241 valid 0.5390109419822693\n",
      "EPOCH 310: LOSS train 0.6002695163091024 valid 0.5384665131568909\n",
      "Epoch 00319: reducing learning rate of group 0 to 4.4965e-06.\n",
      "EPOCH 320: LOSS train 0.5966483553250631 valid 0.5382224321365356\n",
      "EPOCH 330: LOSS train 0.595411499341329 valid 0.5376856327056885\n",
      "Epoch 00330: reducing learning rate of group 0 to 2.2483e-06.\n",
      "EPOCH 340: LOSS train 0.5947001179059347 valid 0.5375752449035645\n",
      "Epoch 00341: reducing learning rate of group 0 to 1.1241e-06.\n",
      "EPOCH 350: LOSS train 0.5976456602414449 valid 0.5375505089759827\n",
      "Epoch 00352: reducing learning rate of group 0 to 5.6207e-07.\n",
      "EPOCH 360: LOSS train 0.5968039830525717 valid 0.5375372171401978\n",
      "Epoch 00363: reducing learning rate of group 0 to 2.8103e-07.\n",
      "EPOCH 370: LOSS train 0.5906889239947001 valid 0.5375497341156006\n",
      "Epoch 00374: reducing learning rate of group 0 to 1.4052e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:40:11,300] Trial 6 finished with value: 0.5375400185585022 and parameters: {'weight_decay': 6.507945186606959e-05, 'L1': 7.857846212139268e-07, 'learning_rate': 7.194438749527899e-05, 'dropout': 0.2708197706702147, 'first_layer': 512}. Best is trial 6 with value: 0.5375400185585022.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8776352008183798 valid 0.8455797433853149\n",
      "EPOCH 20: LOSS train 0.7303200562795004 valid 0.6993303298950195\n",
      "EPOCH 30: LOSS train 0.6650962034861246 valid 0.653278112411499\n",
      "EPOCH 40: LOSS train 0.6337554852167765 valid 0.6148905754089355\n",
      "EPOCH 50: LOSS train 0.5975545446077982 valid 0.5812624096870422\n",
      "EPOCH 60: LOSS train 0.5695037245750427 valid 0.5565420389175415\n",
      "EPOCH 70: LOSS train 0.5506419936815897 valid 0.5388692021369934\n",
      "EPOCH 80: LOSS train 0.5347418586413065 valid 0.5245106816291809\n",
      "EPOCH 90: LOSS train 0.5204384326934814 valid 0.5123250484466553\n",
      "EPOCH 100: LOSS train 0.5102590719858805 valid 0.5024912357330322\n",
      "EPOCH 110: LOSS train 0.4999377230803172 valid 0.4939729571342468\n",
      "EPOCH 120: LOSS train 0.4900975426038106 valid 0.4865984320640564\n",
      "EPOCH 130: LOSS train 0.48422301808993023 valid 0.4801118075847626\n",
      "EPOCH 140: LOSS train 0.4720040559768677 valid 0.47394928336143494\n",
      "EPOCH 150: LOSS train 0.46911896268526715 valid 0.4685250222682953\n",
      "EPOCH 160: LOSS train 0.45683274666468304 valid 0.46358776092529297\n",
      "EPOCH 170: LOSS train 0.4509611328442891 valid 0.45895916223526\n",
      "EPOCH 180: LOSS train 0.4545506437619527 valid 0.454847514629364\n",
      "EPOCH 190: LOSS train 0.44538547595342 valid 0.45083126425743103\n",
      "EPOCH 200: LOSS train 0.43676019708315533 valid 0.4472755491733551\n",
      "EPOCH 210: LOSS train 0.43481985727945965 valid 0.44404515624046326\n",
      "EPOCH 220: LOSS train 0.42612290382385254 valid 0.44156864285469055\n",
      "EPOCH 230: LOSS train 0.42475346724192303 valid 0.4383418560028076\n",
      "EPOCH 240: LOSS train 0.4199186861515045 valid 0.4358997344970703\n",
      "EPOCH 250: LOSS train 0.41372530659039813 valid 0.43342697620391846\n",
      "EPOCH 260: LOSS train 0.41402602195739746 valid 0.4313143789768219\n",
      "EPOCH 270: LOSS train 0.40850911537806195 valid 0.42935824394226074\n",
      "EPOCH 280: LOSS train 0.40189411242802936 valid 0.4273431897163391\n",
      "EPOCH 290: LOSS train 0.40133800109227497 valid 0.425417959690094\n",
      "EPOCH 300: LOSS train 0.3974299629529317 valid 0.4235095679759979\n",
      "Epoch 00309: reducing learning rate of group 0 to 5.6215e-05.\n",
      "EPOCH 310: LOSS train 0.39691805839538574 valid 0.4212338924407959\n",
      "EPOCH 320: LOSS train 0.3879573742548625 valid 0.4202582836151123\n",
      "EPOCH 330: LOSS train 0.39034875233968097 valid 0.4194667935371399\n",
      "EPOCH 340: LOSS train 0.3867454131444295 valid 0.41847649216651917\n",
      "EPOCH 350: LOSS train 0.38447291652361554 valid 0.41777676343917847\n",
      "EPOCH 360: LOSS train 0.38169097900390625 valid 0.41697123646736145\n",
      "EPOCH 370: LOSS train 0.38284305731455487 valid 0.4162810444831848\n",
      "Epoch 00375: reducing learning rate of group 0 to 2.8108e-05.\n",
      "EPOCH 380: LOSS train 0.38038191199302673 valid 0.4156070053577423\n",
      "EPOCH 390: LOSS train 0.3768853545188904 valid 0.4151158928871155\n",
      "EPOCH 400: LOSS train 0.3797134558359782 valid 0.4146241247653961\n",
      "Epoch 00407: reducing learning rate of group 0 to 1.4054e-05.\n",
      "EPOCH 410: LOSS train 0.3803625802199046 valid 0.4142727851867676\n",
      "EPOCH 420: LOSS train 0.37529562910397846 valid 0.4141468107700348\n",
      "Epoch 00423: reducing learning rate of group 0 to 7.0269e-06.\n",
      "EPOCH 430: LOSS train 0.3790357708930969 valid 0.4139317572116852\n",
      "Epoch 00434: reducing learning rate of group 0 to 3.5134e-06.\n",
      "EPOCH 440: LOSS train 0.3778315683205922 valid 0.4138857424259186\n",
      "Epoch 00445: reducing learning rate of group 0 to 1.7567e-06.\n",
      "EPOCH 450: LOSS train 0.37583374977111816 valid 0.4138508141040802\n",
      "EPOCH 460: LOSS train 0.3734472890694936 valid 0.4138389527797699\n",
      "Epoch 00467: reducing learning rate of group 0 to 8.7836e-07.\n",
      "EPOCH 470: LOSS train 0.37940075993537903 valid 0.4138115644454956\n",
      "Epoch 00478: reducing learning rate of group 0 to 4.3918e-07.\n",
      "EPOCH 480: LOSS train 0.37809718648592633 valid 0.41378235816955566\n",
      "Epoch 00489: reducing learning rate of group 0 to 2.1959e-07.\n",
      "EPOCH 490: LOSS train 0.3767223060131073 valid 0.413774311542511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:42:27,721] Trial 7 finished with value: 0.41377100348472595 and parameters: {'weight_decay': 1.516191366489868e-08, 'L1': 8.33059828523479e-08, 'learning_rate': 0.0001124303226955834, 'dropout': 0.05102692458044432, 'first_layer': 512}. Best is trial 7 with value: 0.41377100348472595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 500: LOSS train 0.3798980911572774 valid 0.41377100348472595\n",
      "Epoch 00500: reducing learning rate of group 0 to 1.0980e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:42:29,415] Trial 8 pruned. \n",
      "[I 2023-08-24 21:42:29,841] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6283361514409384 valid 0.619073748588562\n",
      "EPOCH 20: LOSS train 0.518434743086497 valid 0.5225434899330139\n",
      "EPOCH 30: LOSS train 0.4572836061318715 valid 0.47311732172966003\n",
      "EPOCH 40: LOSS train 0.41192235549290973 valid 0.44578486680984497\n",
      "EPOCH 50: LOSS train 0.3865756392478943 valid 0.42876577377319336\n",
      "EPOCH 60: LOSS train 0.35698866844177246 valid 0.41860294342041016\n",
      "EPOCH 70: LOSS train 0.337088406085968 valid 0.4138757586479187\n",
      "EPOCH 80: LOSS train 0.3165605068206787 valid 0.40791088342666626\n",
      "EPOCH 90: LOSS train 0.3016503055890401 valid 0.4064998924732208\n",
      "EPOCH 100: LOSS train 0.28623539209365845 valid 0.4044112265110016\n",
      "EPOCH 110: LOSS train 0.2728164792060852 valid 0.4049096703529358\n",
      "EPOCH 120: LOSS train 0.2618883053461711 valid 0.40581953525543213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:43:01,711] Trial 10 finished with value: 0.4051430821418762 and parameters: {'weight_decay': 2.1219223540896334e-07, 'L1': 8.586661113936183e-08, 'learning_rate': 0.0008191906963349316, 'dropout': 0.003108756735332646, 'first_layer': 512}. Best is trial 10 with value: 0.4051430821418762.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6257858673731486 valid 0.6220853328704834\n",
      "EPOCH 20: LOSS train 0.5184644261995951 valid 0.5250874757766724\n",
      "EPOCH 30: LOSS train 0.46255523959795636 valid 0.4778859615325928\n",
      "EPOCH 40: LOSS train 0.42014575004577637 valid 0.4493851363658905\n",
      "EPOCH 50: LOSS train 0.391960750023524 valid 0.4325098395347595\n",
      "EPOCH 60: LOSS train 0.3736726939678192 valid 0.4201580584049225\n",
      "EPOCH 70: LOSS train 0.3511684536933899 valid 0.4129047095775604\n",
      "EPOCH 80: LOSS train 0.3293749193350474 valid 0.4070153832435608\n",
      "EPOCH 90: LOSS train 0.3128468096256256 valid 0.40452826023101807\n",
      "EPOCH 100: LOSS train 0.3002268075942993 valid 0.4020925462245941\n",
      "EPOCH 110: LOSS train 0.28797318538029987 valid 0.40179213881492615\n",
      "EPOCH 120: LOSS train 0.2766427199045817 valid 0.4012174904346466\n",
      "EPOCH 130: LOSS train 0.2635909815629323 valid 0.4021979570388794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:43:36,486] Trial 11 finished with value: 0.40198081731796265 and parameters: {'weight_decay': 1.9715224340849373e-07, 'L1': 8.085369143861159e-08, 'learning_rate': 0.0007516243324851748, 'dropout': 0.007615054022446145, 'first_layer': 512}. Best is trial 11 with value: 0.40198081731796265.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6081677675247192 valid 0.6049374341964722\n",
      "EPOCH 20: LOSS train 0.5056520303090414 valid 0.5136488676071167\n",
      "EPOCH 30: LOSS train 0.45089223980903625 valid 0.468058705329895\n",
      "EPOCH 40: LOSS train 0.4045511583487193 valid 0.4411921203136444\n",
      "EPOCH 50: LOSS train 0.3804747263590495 valid 0.425428181886673\n",
      "EPOCH 60: LOSS train 0.3498562276363373 valid 0.4159837067127228\n",
      "EPOCH 70: LOSS train 0.3282684087753296 valid 0.4102972447872162\n",
      "EPOCH 80: LOSS train 0.30417219797770184 valid 0.40775951743125916\n",
      "EPOCH 90: LOSS train 0.28741833567619324 valid 0.4069548547267914\n",
      "EPOCH 100: LOSS train 0.2729806701342265 valid 0.4092433750629425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:44:05,944] Trial 12 finished with value: 0.41026413440704346 and parameters: {'weight_decay': 2.6631465417503364e-07, 'L1': 1.055082966987854e-07, 'learning_rate': 0.0009576817804945854, 'dropout': 0.003048503646370277, 'first_layer': 512}. Best is trial 11 with value: 0.40198081731796265.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 110: LOSS train 0.2583775619665782 valid 0.41026413440704346\n",
      "EPOCH 10: LOSS train 0.630102296670278 valid 0.6323235034942627\n",
      "EPOCH 20: LOSS train 0.52541450659434 valid 0.5362688302993774\n",
      "EPOCH 30: LOSS train 0.4669697682062785 valid 0.4849874973297119\n",
      "EPOCH 40: LOSS train 0.42860738436381024 valid 0.4566470980644226\n",
      "EPOCH 50: LOSS train 0.3995627264181773 valid 0.43713584542274475\n",
      "EPOCH 60: LOSS train 0.37938593824704486 valid 0.4228511154651642\n",
      "EPOCH 70: LOSS train 0.35595885912577313 valid 0.41418591141700745\n",
      "EPOCH 80: LOSS train 0.338872363169988 valid 0.40546727180480957\n",
      "EPOCH 90: LOSS train 0.32322519024213153 valid 0.40070757269859314\n",
      "EPOCH 100: LOSS train 0.30842628081639606 valid 0.39686456322669983\n",
      "EPOCH 110: LOSS train 0.2987699906031291 valid 0.3956332206726074\n",
      "EPOCH 120: LOSS train 0.2884153326352437 valid 0.39247646927833557\n",
      "EPOCH 130: LOSS train 0.27766093611717224 valid 0.3914840817451477\n",
      "EPOCH 140: LOSS train 0.26755137244860333 valid 0.3914276361465454\n",
      "EPOCH 150: LOSS train 0.25931750734647113 valid 0.39082857966423035\n",
      "EPOCH 160: LOSS train 0.25219786663850147 valid 0.39100250601768494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:44:50,432] Trial 13 finished with value: 0.3911953568458557 and parameters: {'weight_decay': 3.8115071306568935e-07, 'L1': 1.1354793800007698e-08, 'learning_rate': 0.0005962904416328184, 'dropout': 0.0027506894973574987, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.7086219588915507 valid 0.6890137195587158\n",
      "EPOCH 20: LOSS train 0.6350255409876505 valid 0.6241819858551025\n",
      "EPOCH 30: LOSS train 0.5808724363644918 valid 0.5722118020057678\n",
      "EPOCH 40: LOSS train 0.5465863744417826 valid 0.540557861328125\n",
      "EPOCH 50: LOSS train 0.5259804526964823 valid 0.5189912915229797\n",
      "EPOCH 60: LOSS train 0.5019987324873606 valid 0.5005415081977844\n",
      "EPOCH 70: LOSS train 0.48447171847025555 valid 0.4865182340145111\n",
      "EPOCH 80: LOSS train 0.4748984674612681 valid 0.4761160612106323\n",
      "EPOCH 90: LOSS train 0.46724825104077655 valid 0.467813104391098\n",
      "EPOCH 100: LOSS train 0.4572577973206838 valid 0.45979025959968567\n",
      "EPOCH 110: LOSS train 0.44201873739560443 valid 0.4537929594516754\n",
      "EPOCH 120: LOSS train 0.43326424558957416 valid 0.45077311992645264\n",
      "EPOCH 130: LOSS train 0.429251770178477 valid 0.4444451332092285\n",
      "EPOCH 140: LOSS train 0.42386027177174884 valid 0.44106045365333557\n",
      "EPOCH 150: LOSS train 0.4118083616097768 valid 0.43908268213272095\n",
      "EPOCH 160: LOSS train 0.40743814905484516 valid 0.43476247787475586\n",
      "EPOCH 170: LOSS train 0.3970819115638733 valid 0.4326212406158447\n",
      "EPOCH 180: LOSS train 0.3891487220923106 valid 0.43086451292037964\n",
      "EPOCH 190: LOSS train 0.3873813847700755 valid 0.42947646975517273\n",
      "EPOCH 200: LOSS train 0.3770986497402191 valid 0.4272044897079468\n",
      "EPOCH 210: LOSS train 0.3782397409280141 valid 0.4269223213195801\n",
      "EPOCH 220: LOSS train 0.36755117774009705 valid 0.4248741567134857\n",
      "EPOCH 230: LOSS train 0.368962029616038 valid 0.4243338704109192\n",
      "EPOCH 240: LOSS train 0.36070477962493896 valid 0.42461931705474854\n",
      "EPOCH 250: LOSS train 0.3570444583892822 valid 0.4240177571773529\n",
      "EPOCH 260: LOSS train 0.34955153862635296 valid 0.42227470874786377\n",
      "EPOCH 270: LOSS train 0.3456364870071411 valid 0.42183050513267517\n",
      "EPOCH 280: LOSS train 0.3436933259169261 valid 0.4219650328159332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:46:09,511] Trial 14 finished with value: 0.4226560592651367 and parameters: {'weight_decay': 4.7002067409701024e-07, 'L1': 1.0613730966036376e-08, 'learning_rate': 0.0003491457793416205, 'dropout': 0.11743256042947536, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.8497578899065653 valid 0.8313126564025879\n",
      "EPOCH 20: LOSS train 0.7054109374682108 valid 0.6903616189956665\n",
      "EPOCH 30: LOSS train 0.651772936185201 valid 0.6454691290855408\n",
      "EPOCH 40: LOSS train 0.6101376414299011 valid 0.603439211845398\n",
      "EPOCH 50: LOSS train 0.5819772680600485 valid 0.571724534034729\n",
      "EPOCH 60: LOSS train 0.5510450998942057 valid 0.5498930811882019\n",
      "EPOCH 70: LOSS train 0.5340875585873922 valid 0.5323087573051453\n",
      "EPOCH 80: LOSS train 0.5202241937319437 valid 0.5183740258216858\n",
      "EPOCH 90: LOSS train 0.510681668917338 valid 0.5068402290344238\n",
      "EPOCH 100: LOSS train 0.5030506551265717 valid 0.4978819787502289\n",
      "EPOCH 110: LOSS train 0.48370516300201416 valid 0.4898391664028168\n",
      "EPOCH 120: LOSS train 0.48058853546778363 valid 0.4831675589084625\n",
      "EPOCH 130: LOSS train 0.4726981619993846 valid 0.4770030975341797\n",
      "EPOCH 140: LOSS train 0.4641396403312683 valid 0.4713827073574066\n",
      "EPOCH 150: LOSS train 0.45986727873484295 valid 0.46665310859680176\n",
      "EPOCH 160: LOSS train 0.4521876275539398 valid 0.46206483244895935\n",
      "EPOCH 170: LOSS train 0.44915321469306946 valid 0.4580283761024475\n",
      "EPOCH 180: LOSS train 0.4430617193380992 valid 0.4545779526233673\n",
      "EPOCH 190: LOSS train 0.4378523826599121 valid 0.45125508308410645\n",
      "EPOCH 200: LOSS train 0.4333782394727071 valid 0.447988897562027\n",
      "EPOCH 210: LOSS train 0.42639941970507306 valid 0.445040225982666\n",
      "EPOCH 220: LOSS train 0.42668341596921283 valid 0.44211313128471375\n",
      "EPOCH 230: LOSS train 0.4197295407454173 valid 0.43930795788764954\n",
      "EPOCH 240: LOSS train 0.4171298344930013 valid 0.43742015957832336\n",
      "EPOCH 250: LOSS train 0.4126732250054677 valid 0.4348682761192322\n",
      "EPOCH 260: LOSS train 0.41064493854840595 valid 0.43217378854751587\n",
      "EPOCH 270: LOSS train 0.4064257740974426 valid 0.43023160099983215\n",
      "EPOCH 280: LOSS train 0.40309683481852215 valid 0.4287738502025604\n",
      "EPOCH 290: LOSS train 0.39746970931688946 valid 0.42663002014160156\n",
      "EPOCH 300: LOSS train 0.39497379461924237 valid 0.4254165291786194\n",
      "EPOCH 310: LOSS train 0.3927000363667806 valid 0.42362070083618164\n",
      "EPOCH 320: LOSS train 0.38932884732882184 valid 0.4221837818622589\n",
      "EPOCH 330: LOSS train 0.38801421721776325 valid 0.4206334948539734\n",
      "Epoch 00336: reducing learning rate of group 0 to 1.0824e-04.\n",
      "EPOCH 340: LOSS train 0.3860845963160197 valid 0.41912829875946045\n",
      "EPOCH 350: LOSS train 0.3831118444601695 valid 0.41835924983024597\n",
      "EPOCH 360: LOSS train 0.37972702582677204 valid 0.41777846217155457\n",
      "Epoch 00364: reducing learning rate of group 0 to 5.4118e-05.\n",
      "EPOCH 370: LOSS train 0.37790653109550476 valid 0.4171028733253479\n",
      "Epoch 00377: reducing learning rate of group 0 to 2.7059e-05.\n",
      "EPOCH 380: LOSS train 0.37812573711077374 valid 0.4167281985282898\n",
      "Epoch 00388: reducing learning rate of group 0 to 1.3529e-05.\n",
      "EPOCH 390: LOSS train 0.37716065843900043 valid 0.41647598147392273\n",
      "Epoch 00399: reducing learning rate of group 0 to 6.7647e-06.\n",
      "EPOCH 400: LOSS train 0.3760235806306203 valid 0.4163779318332672\n",
      "EPOCH 410: LOSS train 0.37793800234794617 valid 0.41636064648628235\n",
      "Epoch 00410: reducing learning rate of group 0 to 3.3824e-06.\n",
      "EPOCH 420: LOSS train 0.3814556101957957 valid 0.4163021147251129\n",
      "Epoch 00421: reducing learning rate of group 0 to 1.6912e-06.\n",
      "EPOCH 430: LOSS train 0.37918076912562054 valid 0.4162895381450653\n",
      "Epoch 00432: reducing learning rate of group 0 to 8.4559e-07.\n",
      "EPOCH 440: LOSS train 0.37746920188268024 valid 0.41627341508865356\n",
      "Epoch 00443: reducing learning rate of group 0 to 4.2280e-07.\n",
      "EPOCH 450: LOSS train 0.37904178102811176 valid 0.4162629544734955\n",
      "Epoch 00455: reducing learning rate of group 0 to 2.1140e-07.\n",
      "EPOCH 460: LOSS train 0.37594204147656757 valid 0.4162488281726837\n",
      "Epoch 00466: reducing learning rate of group 0 to 1.0570e-07.\n",
      "EPOCH 470: LOSS train 0.3784067630767822 valid 0.41624537110328674\n",
      "EPOCH 480: LOSS train 0.3759610454241435 valid 0.4162444770336151\n",
      "EPOCH 490: LOSS train 0.37509940067927044 valid 0.4162461459636688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:48:01,665] Trial 15 finished with value: 0.41624563932418823 and parameters: {'weight_decay': 7.997723568119919e-08, 'L1': 3.448752281481382e-08, 'learning_rate': 0.00021647165928768102, 'dropout': 0.040303353664567464, 'first_layer': 256}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6686643560727438 valid 0.6527604460716248\n",
      "EPOCH 20: LOSS train 0.5843929449717203 valid 0.5768197774887085\n",
      "EPOCH 30: LOSS train 0.5416168769200643 valid 0.531376302242279\n",
      "EPOCH 40: LOSS train 0.5016171634197235 valid 0.501101553440094\n",
      "EPOCH 50: LOSS train 0.4847195545832316 valid 0.4815389811992645\n",
      "EPOCH 60: LOSS train 0.45449894666671753 valid 0.4661445617675781\n",
      "EPOCH 70: LOSS train 0.43733619650204975 valid 0.4559389054775238\n",
      "EPOCH 80: LOSS train 0.42311836282412213 valid 0.4485446512699127\n",
      "EPOCH 90: LOSS train 0.41410409410794574 valid 0.4429503381252289\n",
      "EPOCH 100: LOSS train 0.39989317456881207 valid 0.43655285239219666\n",
      "EPOCH 110: LOSS train 0.39119293292363483 valid 0.4351850152015686\n",
      "EPOCH 120: LOSS train 0.38229626417160034 valid 0.43129613995552063\n",
      "EPOCH 130: LOSS train 0.37353604038556415 valid 0.42979615926742554\n",
      "EPOCH 140: LOSS train 0.36512431502342224 valid 0.42629751563072205\n",
      "EPOCH 150: LOSS train 0.35293182730674744 valid 0.4267103672027588\n",
      "EPOCH 160: LOSS train 0.34569384654362995 valid 0.426113486289978\n",
      "EPOCH 170: LOSS train 0.34004392226537067 valid 0.42664194107055664\n",
      "EPOCH 180: LOSS train 0.3332104484240214 valid 0.4250602126121521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:48:43,629] Trial 16 finished with value: 0.4257524013519287 and parameters: {'weight_decay': 1.1765657849032275e-06, 'L1': 1.036248190478141e-08, 'learning_rate': 0.0005894935906376176, 'dropout': 0.10059455002309657, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6226044694582621 valid 0.6188656687736511\n",
      "EPOCH 20: LOSS train 0.5309045910835266 valid 0.529849648475647\n",
      "EPOCH 30: LOSS train 0.47210877140363056 valid 0.48441487550735474\n",
      "EPOCH 40: LOSS train 0.43411622444788617 valid 0.4588643014431\n",
      "EPOCH 50: LOSS train 0.40304070711135864 valid 0.4420382082462311\n",
      "EPOCH 60: LOSS train 0.3794470429420471 valid 0.43363693356513977\n",
      "EPOCH 70: LOSS train 0.3603213131427765 valid 0.4258553683757782\n",
      "EPOCH 80: LOSS train 0.3415043254693349 valid 0.4225260019302368\n",
      "EPOCH 90: LOSS train 0.3252640167872111 valid 0.41847530007362366\n",
      "EPOCH 100: LOSS train 0.3109642763932546 valid 0.41856569051742554\n",
      "EPOCH 110: LOSS train 0.296718289454778 valid 0.4194652736186981\n",
      "EPOCH 120: LOSS train 0.2836491068204244 valid 0.41916313767433167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:49:19,863] Trial 17 finished with value: 0.4202786087989807 and parameters: {'weight_decay': 5.814974907820201e-07, 'L1': 3.944891979699699e-08, 'learning_rate': 0.0009093444626234499, 'dropout': 0.02969112547778877, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n",
      "[I 2023-08-24 21:49:20,224] Trial 18 pruned. \n",
      "[I 2023-08-24 21:49:20,528] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.631882111231486 valid 0.6305094957351685\n",
      "EPOCH 20: LOSS train 0.5356197953224182 valid 0.5427704453468323\n",
      "EPOCH 30: LOSS train 0.4807656606038411 valid 0.49361374974250793\n",
      "EPOCH 40: LOSS train 0.44175652662913006 valid 0.46546244621276855\n",
      "EPOCH 50: LOSS train 0.41730450590451557 valid 0.44658538699150085\n",
      "EPOCH 60: LOSS train 0.39791030685106915 valid 0.43317297101020813\n",
      "EPOCH 70: LOSS train 0.3779763678709666 valid 0.42603886127471924\n",
      "EPOCH 80: LOSS train 0.35861968994140625 valid 0.41900530457496643\n",
      "EPOCH 90: LOSS train 0.34466783205668133 valid 0.4133404791355133\n",
      "EPOCH 100: LOSS train 0.3314959506193797 valid 0.41099831461906433\n",
      "EPOCH 110: LOSS train 0.3151306410630544 valid 0.40790948271751404\n",
      "EPOCH 120: LOSS train 0.3051483730475108 valid 0.40690213441848755\n",
      "EPOCH 130: LOSS train 0.2965552310148875 valid 0.4058886766433716\n",
      "EPOCH 140: LOSS train 0.28578980763753253 valid 0.40515387058258057\n",
      "EPOCH 150: LOSS train 0.2806682586669922 valid 0.40606048703193665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:50:05,831] Trial 20 finished with value: 0.4062022566795349 and parameters: {'weight_decay': 6.298481571644835e-07, 'L1': 1.1166578306646927e-08, 'learning_rate': 0.0006321016901450013, 'dropout': 0.022509285850158905, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 160: LOSS train 0.269846111536026 valid 0.4062022566795349\n",
      "EPOCH 10: LOSS train 0.6079598665237427 valid 0.6073250770568848\n",
      "EPOCH 20: LOSS train 0.5060423413912455 valid 0.5138141512870789\n",
      "EPOCH 30: LOSS train 0.44595369696617126 valid 0.4680634140968323\n",
      "EPOCH 40: LOSS train 0.403664767742157 valid 0.44145819544792175\n",
      "EPOCH 50: LOSS train 0.3685372273127238 valid 0.4257655441761017\n",
      "EPOCH 60: LOSS train 0.3435991406440735 valid 0.41844281554222107\n",
      "EPOCH 70: LOSS train 0.3196689188480377 valid 0.4113118052482605\n",
      "EPOCH 80: LOSS train 0.3027604917685191 valid 0.4086734652519226\n",
      "EPOCH 90: LOSS train 0.28130688269933063 valid 0.4097416400909424\n",
      "EPOCH 100: LOSS train 0.2645522554715474 valid 0.4089932143688202\n",
      "EPOCH 110: LOSS train 0.25348388652006787 valid 0.4105704426765442\n",
      "EPOCH 120: LOSS train 0.23994754254817963 valid 0.4139299988746643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:50:39,516] Trial 21 finished with value: 0.4128230810165405 and parameters: {'weight_decay': 2.761490571554157e-07, 'L1': 7.705848093872681e-08, 'learning_rate': 0.0009636872696479253, 'dropout': 0.0002877978700196416, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6381293535232544 valid 0.624055802822113\n",
      "EPOCH 20: LOSS train 0.5239364504814148 valid 0.5256878137588501\n",
      "EPOCH 30: LOSS train 0.46644266446431476 valid 0.478557825088501\n",
      "EPOCH 40: LOSS train 0.4279176692167918 valid 0.4502856135368347\n",
      "EPOCH 50: LOSS train 0.39825884501139325 valid 0.4329407215118408\n",
      "EPOCH 60: LOSS train 0.38133831818898517 valid 0.4224635362625122\n",
      "EPOCH 70: LOSS train 0.3569872975349426 valid 0.4095044434070587\n",
      "EPOCH 80: LOSS train 0.34217673540115356 valid 0.40462687611579895\n",
      "EPOCH 90: LOSS train 0.3262186845143636 valid 0.3981681764125824\n",
      "EPOCH 100: LOSS train 0.3093000253041585 valid 0.39401358366012573\n",
      "EPOCH 110: LOSS train 0.2952780822912852 valid 0.3929896354675293\n",
      "EPOCH 120: LOSS train 0.28362807631492615 valid 0.3912481665611267\n",
      "EPOCH 130: LOSS train 0.27260133624076843 valid 0.3918532729148865\n",
      "EPOCH 140: LOSS train 0.2663783133029938 valid 0.3916362524032593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:51:21,295] Trial 22 finished with value: 0.39214444160461426 and parameters: {'weight_decay': 1.6973924854402177e-07, 'L1': 1.6246664489384396e-07, 'learning_rate': 0.0006680926261339925, 'dropout': 0.0005846404238424069, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6623870730400085 valid 0.6397314667701721\n",
      "EPOCH 20: LOSS train 0.5609356363614401 valid 0.5522814989089966\n",
      "EPOCH 30: LOSS train 0.5065678854783376 valid 0.5047808289527893\n",
      "EPOCH 40: LOSS train 0.47261524200439453 valid 0.47378474473953247\n",
      "EPOCH 50: LOSS train 0.443811555703481 valid 0.4549380838871002\n",
      "EPOCH 60: LOSS train 0.4268540342648824 valid 0.4411410391330719\n",
      "EPOCH 70: LOSS train 0.40421510736147565 valid 0.43159154057502747\n",
      "EPOCH 80: LOSS train 0.3887524902820587 valid 0.424735963344574\n",
      "EPOCH 90: LOSS train 0.37262309590975445 valid 0.4200734496116638\n",
      "EPOCH 100: LOSS train 0.3651300072669983 valid 0.41529783606529236\n",
      "EPOCH 110: LOSS train 0.34623422225316364 valid 0.41384750604629517\n",
      "EPOCH 120: LOSS train 0.33266304930051166 valid 0.411880761384964\n",
      "EPOCH 130: LOSS train 0.32573898633321124 valid 0.4110897481441498\n",
      "EPOCH 140: LOSS train 0.31800610820452374 valid 0.4107155203819275\n",
      "EPOCH 150: LOSS train 0.3082004984219869 valid 0.41056889295578003\n",
      "EPOCH 160: LOSS train 0.2977159221967061 valid 0.4111255705356598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:52:02,182] Trial 23 finished with value: 0.41211625933647156 and parameters: {'weight_decay': 6.026703146422046e-08, 'L1': 1.7340794521182084e-07, 'learning_rate': 0.0006118405495897285, 'dropout': 0.0347290803489301, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n",
      "[I 2023-08-24 21:52:02,504] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6456206838289896 valid 0.6480118632316589\n",
      "EPOCH 20: LOSS train 0.5506360729535421 valid 0.5526760816574097\n",
      "EPOCH 30: LOSS train 0.48791781067848206 valid 0.49929749965667725\n",
      "EPOCH 40: LOSS train 0.45160365104675293 valid 0.4692620038986206\n",
      "EPOCH 50: LOSS train 0.4214920699596405 valid 0.44879573583602905\n",
      "EPOCH 60: LOSS train 0.3999970257282257 valid 0.43568140268325806\n",
      "EPOCH 70: LOSS train 0.3834262291590373 valid 0.42662620544433594\n",
      "EPOCH 80: LOSS train 0.36762284239133197 valid 0.4197871685028076\n",
      "EPOCH 90: LOSS train 0.34955884019533795 valid 0.41458067297935486\n",
      "EPOCH 100: LOSS train 0.33858662843704224 valid 0.4113181233406067\n",
      "EPOCH 110: LOSS train 0.3239527642726898 valid 0.4090931713581085\n",
      "EPOCH 120: LOSS train 0.3142154812812805 valid 0.40800368785858154\n",
      "EPOCH 130: LOSS train 0.2994340856870015 valid 0.40743204951286316\n",
      "EPOCH 140: LOSS train 0.2901194095611572 valid 0.40634801983833313\n",
      "EPOCH 150: LOSS train 0.28426775336265564 valid 0.40694552659988403\n",
      "EPOCH 160: LOSS train 0.27528853217760724 valid 0.4072226285934448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:52:46,558] Trial 25 finished with value: 0.40800949931144714 and parameters: {'weight_decay': 4.1048522866394844e-07, 'L1': 2.0566533985932336e-08, 'learning_rate': 0.0006231228503538326, 'dropout': 0.02418870091552588, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n",
      "[I 2023-08-24 21:52:46,864] Trial 26 pruned. \n",
      "[I 2023-08-24 21:52:47,177] Trial 27 pruned. \n",
      "[I 2023-08-24 21:52:47,426] Trial 28 pruned. \n",
      "[I 2023-08-24 21:52:47,696] Trial 29 pruned. \n",
      "[I 2023-08-24 21:52:47,929] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6324581305185953 valid 0.6250680088996887\n",
      "EPOCH 20: LOSS train 0.5118051668008169 valid 0.5231593251228333\n",
      "EPOCH 30: LOSS train 0.4535118043422699 valid 0.4725326597690582\n",
      "EPOCH 40: LOSS train 0.41815970341364544 valid 0.44539323449134827\n",
      "EPOCH 50: LOSS train 0.3906843562920888 valid 0.42732810974121094\n",
      "EPOCH 60: LOSS train 0.36315996448198956 valid 0.41651690006256104\n",
      "EPOCH 70: LOSS train 0.34447258710861206 valid 0.409331351518631\n",
      "EPOCH 80: LOSS train 0.3238576451937358 valid 0.4040077030658722\n",
      "EPOCH 90: LOSS train 0.30921581387519836 valid 0.4003598093986511\n",
      "EPOCH 100: LOSS train 0.29557982087135315 valid 0.3990262746810913\n",
      "EPOCH 110: LOSS train 0.28103447953859967 valid 0.39771372079849243\n",
      "EPOCH 120: LOSS train 0.2702057560284932 valid 0.39673423767089844\n",
      "EPOCH 130: LOSS train 0.258805513381958 valid 0.3986113369464874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:53:22,002] Trial 31 finished with value: 0.39934584498405457 and parameters: {'weight_decay': 1.962306522354136e-07, 'L1': 1.0503983645308674e-07, 'learning_rate': 0.0007632769062839019, 'dropout': 0.0017718976563925182, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 140: LOSS train 0.2494902859131495 valid 0.39934584498405457\n",
      "EPOCH 10: LOSS train 0.6284323533376058 valid 0.6265348792076111\n",
      "EPOCH 20: LOSS train 0.528694768746694 valid 0.5363796949386597\n",
      "EPOCH 30: LOSS train 0.471309353907903 valid 0.4847276508808136\n",
      "EPOCH 40: LOSS train 0.4317329327265422 valid 0.45766615867614746\n",
      "EPOCH 50: LOSS train 0.403682937224706 valid 0.44006288051605225\n",
      "EPOCH 60: LOSS train 0.378517746925354 valid 0.4280896782875061\n",
      "EPOCH 70: LOSS train 0.3594586451848348 valid 0.4202587902545929\n",
      "EPOCH 80: LOSS train 0.3374687035878499 valid 0.41476020216941833\n",
      "EPOCH 90: LOSS train 0.32540865739186603 valid 0.41100671887397766\n",
      "EPOCH 100: LOSS train 0.3091992835203807 valid 0.40917202830314636\n",
      "EPOCH 110: LOSS train 0.29538455605506897 valid 0.4073950946331024\n",
      "EPOCH 120: LOSS train 0.28357937932014465 valid 0.4069823622703552\n",
      "EPOCH 130: LOSS train 0.27312230070432025 valid 0.4070819616317749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:53:54,847] Trial 32 finished with value: 0.4065963923931122 and parameters: {'weight_decay': 2.1172139176525261e-07, 'L1': 1.9577703276048573e-08, 'learning_rate': 0.0007445837147800758, 'dropout': 0.015356662179541762, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6447734832763672 valid 0.6135453581809998\n",
      "EPOCH 20: LOSS train 0.538857102394104 valid 0.5242220163345337\n",
      "EPOCH 30: LOSS train 0.4896782537301381 valid 0.48152580857276917\n",
      "EPOCH 40: LOSS train 0.45404399434725445 valid 0.4593186676502228\n",
      "EPOCH 50: LOSS train 0.4203694959481557 valid 0.4420911371707916\n",
      "EPOCH 60: LOSS train 0.39776456356048584 valid 0.4333708882331848\n",
      "EPOCH 70: LOSS train 0.38261550664901733 valid 0.4280956983566284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:54:13,562] Trial 33 pruned. \n",
      "[I 2023-08-24 21:54:13,892] Trial 34 pruned. \n",
      "[I 2023-08-24 21:54:14,127] Trial 35 pruned. \n",
      "[I 2023-08-24 21:54:16,800] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6563314994176229 valid 0.6441168785095215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:54:17,128] Trial 37 pruned. \n",
      "[I 2023-08-24 21:54:17,441] Trial 38 pruned. \n",
      "[I 2023-08-24 21:54:17,733] Trial 39 pruned. \n",
      "[I 2023-08-24 21:54:18,031] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6394370396931967 valid 0.6289570927619934\n",
      "EPOCH 20: LOSS train 0.5145539144674937 valid 0.5259408950805664\n",
      "EPOCH 30: LOSS train 0.4565290609995524 valid 0.4775499999523163\n",
      "EPOCH 40: LOSS train 0.4223994513352712 valid 0.44895291328430176\n",
      "EPOCH 50: LOSS train 0.3891648550828298 valid 0.43074512481689453\n",
      "EPOCH 60: LOSS train 0.36263622840245563 valid 0.4175402820110321\n",
      "EPOCH 70: LOSS train 0.3428766230742137 valid 0.410062313079834\n",
      "EPOCH 80: LOSS train 0.32215993603070575 valid 0.4041674733161926\n",
      "EPOCH 90: LOSS train 0.3084024290243785 valid 0.4012495279312134\n",
      "EPOCH 100: LOSS train 0.2924221058686574 valid 0.400654673576355\n",
      "EPOCH 110: LOSS train 0.2800936996936798 valid 0.39774763584136963\n",
      "EPOCH 120: LOSS train 0.2681881586710612 valid 0.39840900897979736\n",
      "EPOCH 130: LOSS train 0.2555217146873474 valid 0.39832836389541626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:54:49,982] Trial 41 finished with value: 0.39926958084106445 and parameters: {'weight_decay': 2.0870458398683053e-07, 'L1': 9.002538502484614e-08, 'learning_rate': 0.0007605085192101285, 'dropout': 0.002636665042906193, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6294752955436707 valid 0.6250953674316406\n",
      "EPOCH 20: LOSS train 0.5223088065783182 valid 0.5323207378387451\n",
      "EPOCH 30: LOSS train 0.46572141846021015 valid 0.48461413383483887\n",
      "EPOCH 40: LOSS train 0.42889564236005145 valid 0.45512107014656067\n",
      "EPOCH 50: LOSS train 0.39815754691759747 valid 0.4378722012042999\n",
      "EPOCH 60: LOSS train 0.3710996409257253 valid 0.42573583126068115\n",
      "EPOCH 70: LOSS train 0.35693126916885376 valid 0.41887596249580383\n",
      "EPOCH 80: LOSS train 0.3348369797070821 valid 0.4134483337402344\n",
      "EPOCH 90: LOSS train 0.3174629708131154 valid 0.40951213240623474\n",
      "EPOCH 100: LOSS train 0.30394433935483295 valid 0.40892940759658813\n",
      "EPOCH 110: LOSS train 0.290462464094162 valid 0.40829896926879883\n",
      "EPOCH 120: LOSS train 0.2772870560487111 valid 0.4077192544937134\n",
      "EPOCH 130: LOSS train 0.267378310362498 valid 0.4080237150192261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:55:23,595] Trial 42 finished with value: 0.4089176654815674 and parameters: {'weight_decay': 2.636064129468762e-07, 'L1': 2.9032844355225014e-08, 'learning_rate': 0.0007626000024037866, 'dropout': 0.013118139682285545, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n",
      "[I 2023-08-24 21:55:23,981] Trial 43 pruned. \n",
      "[I 2023-08-24 21:55:24,391] Trial 44 pruned. \n",
      "[I 2023-08-24 21:55:24,749] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6192524234453837 valid 0.618553638458252\n",
      "EPOCH 20: LOSS train 0.5076037645339966 valid 0.5209411382675171\n",
      "EPOCH 30: LOSS train 0.4483563502629598 valid 0.47307342290878296\n",
      "EPOCH 40: LOSS train 0.41038306554158527 valid 0.448738157749176\n",
      "EPOCH 50: LOSS train 0.3767482837041219 valid 0.4318057894706726\n",
      "EPOCH 60: LOSS train 0.35172930359840393 valid 0.4231354892253876\n",
      "EPOCH 70: LOSS train 0.3295711974302928 valid 0.4193982183933258\n",
      "EPOCH 80: LOSS train 0.3085784912109375 valid 0.41522741317749023\n",
      "EPOCH 90: LOSS train 0.29404740532239276 valid 0.41414788365364075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:55:50,807] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 100: LOSS train 0.27680959304173786 valid 0.4155488908290863\n",
      "EPOCH 10: LOSS train 0.6371941963831583 valid 0.624918520450592\n",
      "EPOCH 20: LOSS train 0.5400800903638204 valid 0.5363131165504456\n",
      "EPOCH 30: LOSS train 0.4876168966293335 valid 0.49375030398368835\n",
      "EPOCH 40: LOSS train 0.44789331158002216 valid 0.4650830924510956\n",
      "EPOCH 50: LOSS train 0.41801275809605914 valid 0.4464053213596344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:56:05,972] Trial 47 pruned. \n",
      "[I 2023-08-24 21:56:06,390] Trial 48 pruned. \n",
      "[I 2023-08-24 21:56:06,713] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6598265767097473 valid 0.6308480501174927\n",
      "EPOCH 20: LOSS train 0.5590315858523051 valid 0.5416960120201111\n",
      "EPOCH 30: LOSS train 0.5054305990537008 valid 0.4962679147720337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:56:15,696] Trial 50 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6256306966145834 valid 0.6175841689109802\n",
      "EPOCH 20: LOSS train 0.5178792476654053 valid 0.5223252773284912\n",
      "EPOCH 30: LOSS train 0.45373472571372986 valid 0.4730153977870941\n",
      "EPOCH 40: LOSS train 0.41882432500521344 valid 0.44608622789382935\n",
      "EPOCH 50: LOSS train 0.3876707951227824 valid 0.42872530221939087\n",
      "EPOCH 60: LOSS train 0.36449962854385376 valid 0.42017820477485657\n",
      "EPOCH 70: LOSS train 0.3419276773929596 valid 0.411562442779541\n",
      "EPOCH 80: LOSS train 0.32472965121269226 valid 0.406670480966568\n",
      "EPOCH 90: LOSS train 0.30704137682914734 valid 0.4040522873401642\n",
      "EPOCH 100: LOSS train 0.2906274696191152 valid 0.403359591960907\n",
      "EPOCH 110: LOSS train 0.27509045600891113 valid 0.40246155858039856\n",
      "EPOCH 120: LOSS train 0.2626897990703583 valid 0.40324732661247253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:56:46,470] Trial 51 finished with value: 0.4040413200855255 and parameters: {'weight_decay': 2.3311057945598987e-07, 'L1': 8.901340807110119e-08, 'learning_rate': 0.0008153011536167536, 'dropout': 0.0029755269813741443, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n",
      "[I 2023-08-24 21:56:47,651] Trial 52 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6267229318618774 valid 0.6178288459777832\n",
      "EPOCH 20: LOSS train 0.5286202629407247 valid 0.5287619829177856\n",
      "EPOCH 30: LOSS train 0.4720926781495412 valid 0.48421135544776917\n",
      "EPOCH 40: LOSS train 0.43466852108637494 valid 0.45893627405166626\n",
      "EPOCH 50: LOSS train 0.4057137866814931 valid 0.44168317317962646\n",
      "EPOCH 60: LOSS train 0.3796170949935913 valid 0.4310523569583893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:57:04,566] Trial 53 pruned. \n",
      "[I 2023-08-24 21:57:04,840] Trial 54 pruned. \n",
      "[I 2023-08-24 21:57:05,140] Trial 55 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.642032523949941 valid 0.6258377432823181\n",
      "EPOCH 20: LOSS train 0.5331521034240723 valid 0.5322948694229126\n",
      "EPOCH 30: LOSS train 0.4788399438063304 valid 0.4827268421649933\n",
      "EPOCH 40: LOSS train 0.43788357575734455 valid 0.45582348108291626\n",
      "EPOCH 50: LOSS train 0.41342129309972125 valid 0.43905550241470337\n",
      "EPOCH 60: LOSS train 0.3884260455767314 valid 0.4257964789867401\n",
      "EPOCH 70: LOSS train 0.36825597286224365 valid 0.4169403612613678\n",
      "EPOCH 80: LOSS train 0.34769582748413086 valid 0.4113596975803375\n",
      "EPOCH 90: LOSS train 0.33454858263333637 valid 0.4065614640712738\n",
      "EPOCH 100: LOSS train 0.3211129903793335 valid 0.4043794274330139\n",
      "EPOCH 110: LOSS train 0.3073416252930959 valid 0.4023796021938324\n",
      "EPOCH 120: LOSS train 0.297418346007665 valid 0.40200236439704895\n",
      "EPOCH 130: LOSS train 0.2872553765773773 valid 0.402283251285553\n",
      "EPOCH 140: LOSS train 0.27519330382347107 valid 0.40344181656837463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:57:41,907] Trial 56 finished with value: 0.4035259485244751 and parameters: {'weight_decay': 1.658089509972467e-07, 'L1': 1.6866041053003887e-07, 'learning_rate': 0.0006696250536264054, 'dropout': 0.010328644632225535, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n",
      "[I 2023-08-24 21:57:42,434] Trial 57 pruned. \n",
      "[I 2023-08-24 21:57:42,770] Trial 58 pruned. \n",
      "[I 2023-08-24 21:57:43,057] Trial 59 pruned. \n",
      "[I 2023-08-24 21:57:43,375] Trial 60 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6319735646247864 valid 0.6146162748336792\n",
      "EPOCH 20: LOSS train 0.524250864982605 valid 0.5234568119049072\n",
      "EPOCH 30: LOSS train 0.46202461918195087 valid 0.47331979870796204\n",
      "EPOCH 40: LOSS train 0.4233780999978383 valid 0.4484238922595978\n",
      "EPOCH 50: LOSS train 0.3930823604265849 valid 0.4302985966205597\n",
      "EPOCH 60: LOSS train 0.36651692787806195 valid 0.42001837491989136\n",
      "EPOCH 70: LOSS train 0.34237948060035706 valid 0.41290372610092163\n",
      "EPOCH 80: LOSS train 0.3239177664120992 valid 0.4103584587574005\n",
      "EPOCH 90: LOSS train 0.3064379890759786 valid 0.40858712792396545\n",
      "EPOCH 100: LOSS train 0.2915061016877492 valid 0.40827885270118713\n",
      "EPOCH 110: LOSS train 0.28040088216463727 valid 0.40899181365966797\n",
      "EPOCH 120: LOSS train 0.26660726467768353 valid 0.41064924001693726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:58:12,243] Trial 61 finished with value: 0.4110654890537262 and parameters: {'weight_decay': 2.2329791996356612e-07, 'L1': 1.4291414488165944e-07, 'learning_rate': 0.0008789476710464244, 'dropout': 0.008060395058331938, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6040686766306559 valid 0.6076944470405579\n",
      "EPOCH 20: LOSS train 0.5093909899393717 valid 0.516568660736084\n",
      "EPOCH 30: LOSS train 0.4435020883878072 valid 0.46762821078300476\n",
      "EPOCH 40: LOSS train 0.4043686588605245 valid 0.44198060035705566\n",
      "EPOCH 50: LOSS train 0.37292394042015076 valid 0.42703065276145935\n",
      "EPOCH 60: LOSS train 0.34805480639139813 valid 0.41450291872024536\n",
      "EPOCH 70: LOSS train 0.3269401689370473 valid 0.4091286063194275\n",
      "EPOCH 80: LOSS train 0.3048108418782552 valid 0.40538525581359863\n",
      "EPOCH 90: LOSS train 0.2906913061936696 valid 0.4039570987224579\n",
      "EPOCH 100: LOSS train 0.2733008662859599 valid 0.40442126989364624\n",
      "EPOCH 110: LOSS train 0.26489655176798504 valid 0.40592068433761597\n",
      "EPOCH 120: LOSS train 0.24882473051548004 valid 0.4057954251766205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:58:40,913] Trial 62 finished with value: 0.4067037105560303 and parameters: {'weight_decay': 1.6467477923976647e-07, 'L1': 7.34572483824844e-08, 'learning_rate': 0.0008563971074230542, 'dropout': 0.00010423625812137514, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6337786118189493 valid 0.6242548823356628\n",
      "EPOCH 20: LOSS train 0.5273471275965372 valid 0.5319624543190002\n",
      "EPOCH 30: LOSS train 0.4687144259611766 valid 0.4818156063556671\n",
      "EPOCH 40: LOSS train 0.43417859077453613 valid 0.4538640081882477\n",
      "EPOCH 50: LOSS train 0.4047516683737437 valid 0.43757063150405884\n",
      "EPOCH 60: LOSS train 0.3811921775341034 valid 0.42532142996788025\n",
      "EPOCH 70: LOSS train 0.3611614803473155 valid 0.4170287549495697\n",
      "EPOCH 80: LOSS train 0.3472476800282796 valid 0.4115954041481018\n",
      "EPOCH 90: LOSS train 0.32973681886990863 valid 0.40807268023490906\n",
      "EPOCH 100: LOSS train 0.31621737281481427 valid 0.4062768816947937\n",
      "EPOCH 110: LOSS train 0.3022686541080475 valid 0.40492770075798035\n",
      "EPOCH 120: LOSS train 0.2872263689835866 valid 0.4061491787433624\n",
      "EPOCH 130: LOSS train 0.27673303087552387 valid 0.4053869843482971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:59:13,998] Trial 63 finished with value: 0.4053548574447632 and parameters: {'weight_decay': 3.47963052774754e-07, 'L1': 9.55651711705913e-08, 'learning_rate': 0.0007199939357427945, 'dropout': 0.01279043240126556, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6311711470286051 valid 0.6113293170928955\n",
      "EPOCH 20: LOSS train 0.5442869464556376 valid 0.529919445514679\n",
      "EPOCH 30: LOSS train 0.48382503787676495 valid 0.48254966735839844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:59:22,727] Trial 64 pruned. \n",
      "[I 2023-08-24 21:59:23,032] Trial 65 pruned. \n",
      "[I 2023-08-24 21:59:23,286] Trial 66 pruned. \n",
      "[I 2023-08-24 21:59:24,786] Trial 67 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6228011250495911 valid 0.6205479502677917\n",
      "EPOCH 20: LOSS train 0.5231424570083618 valid 0.5329585075378418\n",
      "EPOCH 30: LOSS train 0.46597859263420105 valid 0.4834781885147095\n",
      "EPOCH 40: LOSS train 0.4297878642876943 valid 0.4561118483543396\n",
      "EPOCH 50: LOSS train 0.40085580945014954 valid 0.43922534584999084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 21:59:36,990] Trial 68 pruned. \n",
      "[I 2023-08-24 21:59:37,318] Trial 69 pruned. \n",
      "[I 2023-08-24 21:59:37,623] Trial 70 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6265184879302979 valid 0.6194261312484741\n",
      "EPOCH 20: LOSS train 0.5200760761896769 valid 0.5273032188415527\n",
      "EPOCH 30: LOSS train 0.45464921991030377 valid 0.47537779808044434\n",
      "EPOCH 40: LOSS train 0.41572291652361554 valid 0.447244256734848\n",
      "EPOCH 50: LOSS train 0.38762789964675903 valid 0.4306472837924957\n",
      "EPOCH 60: LOSS train 0.36082205176353455 valid 0.4161018133163452\n",
      "EPOCH 70: LOSS train 0.34530265132586163 valid 0.40985533595085144\n",
      "EPOCH 80: LOSS train 0.3216087222099304 valid 0.40318790078163147\n",
      "EPOCH 90: LOSS train 0.3076844612757365 valid 0.4010891020298004\n",
      "EPOCH 100: LOSS train 0.2924336791038513 valid 0.40019285678863525\n",
      "EPOCH 110: LOSS train 0.2753509084383647 valid 0.39859679341316223\n",
      "EPOCH 120: LOSS train 0.2649606068929036 valid 0.39894765615463257\n",
      "EPOCH 130: LOSS train 0.2523377438386281 valid 0.39955270290374756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:00:10,355] Trial 71 finished with value: 0.4007708728313446 and parameters: {'weight_decay': 1.0146649242723905e-07, 'L1': 9.045260038502405e-08, 'learning_rate': 0.0007767858957150347, 'dropout': 0.00015374873512121064, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6263367732365926 valid 0.6204580664634705\n",
      "EPOCH 20: LOSS train 0.5264261364936829 valid 0.526315450668335\n",
      "EPOCH 30: LOSS train 0.4659140606721242 valid 0.4786776006221771\n",
      "EPOCH 40: LOSS train 0.4213111400604248 valid 0.45017769932746887\n",
      "EPOCH 50: LOSS train 0.39720630645751953 valid 0.43242236971855164\n",
      "EPOCH 60: LOSS train 0.37020906805992126 valid 0.41996490955352783\n",
      "EPOCH 70: LOSS train 0.35002220670382184 valid 0.41287294030189514\n",
      "EPOCH 80: LOSS train 0.328451265891393 valid 0.4073587656021118\n",
      "EPOCH 90: LOSS train 0.31466519832611084 valid 0.4045867919921875\n",
      "EPOCH 100: LOSS train 0.3015454014142354 valid 0.40416955947875977\n",
      "EPOCH 110: LOSS train 0.28643622001012164 valid 0.4025314152240753\n",
      "EPOCH 120: LOSS train 0.27428750197092694 valid 0.40284502506256104\n",
      "EPOCH 130: LOSS train 0.2624172270298004 valid 0.4033929705619812\n",
      "EPOCH 140: LOSS train 0.25278643767038983 valid 0.4038344621658325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:00:46,865] Trial 72 finished with value: 0.4046609401702881 and parameters: {'weight_decay': 6.154894316862067e-08, 'L1': 9.200143847736816e-08, 'learning_rate': 0.0007736812787069532, 'dropout': 0.007526150404762058, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n",
      "[I 2023-08-24 22:00:47,177] Trial 73 pruned. \n",
      "[I 2023-08-24 22:00:47,490] Trial 74 pruned. \n",
      "[I 2023-08-24 22:00:47,708] Trial 75 pruned. \n",
      "[I 2023-08-24 22:00:48,009] Trial 76 pruned. \n",
      "[I 2023-08-24 22:00:48,281] Trial 77 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6226935982704163 valid 0.6201522946357727\n",
      "EPOCH 20: LOSS train 0.5247420867284139 valid 0.5302541255950928\n",
      "EPOCH 30: LOSS train 0.4618280331293742 valid 0.4791456162929535\n",
      "EPOCH 40: LOSS train 0.41985562443733215 valid 0.4545227289199829\n",
      "EPOCH 50: LOSS train 0.3905555307865143 valid 0.4370284378528595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:01:03,440] Trial 78 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 60: LOSS train 0.3654837707678477 valid 0.42615363001823425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:01:04,076] Trial 79 pruned. \n",
      "[I 2023-08-24 22:01:04,486] Trial 80 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6226606369018555 valid 0.6203916668891907\n",
      "EPOCH 20: LOSS train 0.5150792002677917 valid 0.526653528213501\n",
      "EPOCH 30: LOSS train 0.4602656463781993 valid 0.47663170099258423\n",
      "EPOCH 40: LOSS train 0.42075566450754803 valid 0.4496656358242035\n",
      "EPOCH 50: LOSS train 0.38931159178415936 valid 0.4292866885662079\n",
      "EPOCH 60: LOSS train 0.36287551124890643 valid 0.41813358664512634\n",
      "EPOCH 70: LOSS train 0.3418727119763692 valid 0.409724622964859\n",
      "EPOCH 80: LOSS train 0.3247931698958079 valid 0.40537866950035095\n",
      "EPOCH 90: LOSS train 0.30440401037534076 valid 0.4014865458011627\n",
      "EPOCH 100: LOSS train 0.29180334011713666 valid 0.39952483773231506\n",
      "EPOCH 110: LOSS train 0.275953749815623 valid 0.3990650177001953\n",
      "EPOCH 120: LOSS train 0.2627299924691518 valid 0.3992474675178528\n",
      "EPOCH 130: LOSS train 0.2530096769332886 valid 0.4004523754119873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:01:39,293] Trial 81 finished with value: 0.4012560248374939 and parameters: {'weight_decay': 5.772679534471049e-08, 'L1': 8.242058389769371e-08, 'learning_rate': 0.0007668116656816653, 'dropout': 0.0005090086176401101, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6120386123657227 valid 0.6091952919960022\n",
      "EPOCH 20: LOSS train 0.504263311624527 valid 0.5158877372741699\n",
      "EPOCH 30: LOSS train 0.44096561272939044 valid 0.4687405824661255\n",
      "EPOCH 40: LOSS train 0.4037154217561086 valid 0.4424274265766144\n",
      "EPOCH 50: LOSS train 0.37084147334098816 valid 0.427623450756073\n",
      "EPOCH 60: LOSS train 0.3420415421326955 valid 0.4167119860649109\n",
      "EPOCH 70: LOSS train 0.31899112462997437 valid 0.4119891822338104\n",
      "EPOCH 80: LOSS train 0.2986222604910533 valid 0.40927019715309143\n",
      "EPOCH 90: LOSS train 0.27974727749824524 valid 0.40938735008239746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:02:02,728] Trial 82 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6272212862968445 valid 0.6223018169403076\n",
      "EPOCH 20: LOSS train 0.5281678835550944 valid 0.5308610200881958\n",
      "EPOCH 30: LOSS train 0.46632755796114606 valid 0.48264068365097046\n",
      "EPOCH 40: LOSS train 0.426328182220459 valid 0.4543094038963318\n",
      "EPOCH 50: LOSS train 0.4005484879016876 valid 0.4374554455280304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:02:15,851] Trial 83 pruned. \n",
      "[I 2023-08-24 22:02:16,182] Trial 84 pruned. \n",
      "[I 2023-08-24 22:02:16,498] Trial 85 pruned. \n",
      "[I 2023-08-24 22:02:16,809] Trial 86 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.6144790450731913 valid 0.6187022924423218\n",
      "EPOCH 20: LOSS train 0.5060549080371857 valid 0.5208094120025635\n",
      "EPOCH 30: LOSS train 0.4521280527114868 valid 0.47370508313179016\n",
      "EPOCH 40: LOSS train 0.4091929892698924 valid 0.4445692002773285\n",
      "EPOCH 50: LOSS train 0.37404559055964154 valid 0.42739138007164\n",
      "EPOCH 60: LOSS train 0.34932340184847516 valid 0.4150191843509674\n",
      "EPOCH 70: LOSS train 0.3291104833285014 valid 0.40905308723449707\n",
      "EPOCH 80: LOSS train 0.3096074163913727 valid 0.404399037361145\n",
      "EPOCH 90: LOSS train 0.29298968116442364 valid 0.40229031443595886\n",
      "EPOCH 100: LOSS train 0.2791650891304016 valid 0.4010295867919922\n",
      "EPOCH 110: LOSS train 0.26422903935114544 valid 0.4020666480064392\n",
      "EPOCH 120: LOSS train 0.25189321239789325 valid 0.40321817994117737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:02:45,268] Trial 87 finished with value: 0.40343475341796875 and parameters: {'weight_decay': 3.991692178727599e-07, 'L1': 5.4581729368099795e-08, 'learning_rate': 0.0008223109626947143, 'dropout': 0.0002608976355636054, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n",
      "[I 2023-08-24 22:02:45,584] Trial 88 pruned. \n",
      "[I 2023-08-24 22:02:45,811] Trial 89 pruned. \n",
      "[I 2023-08-24 22:02:46,128] Trial 90 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: LOSS train 0.633014440536499 valid 0.6189342737197876\n",
      "EPOCH 20: LOSS train 0.521550198396047 valid 0.5247904062271118\n",
      "EPOCH 30: LOSS train 0.4596646229426066 valid 0.47671809792518616\n",
      "EPOCH 40: LOSS train 0.4181629220644633 valid 0.44698455929756165\n",
      "EPOCH 50: LOSS train 0.3894341091314952 valid 0.4299885928630829\n",
      "EPOCH 60: LOSS train 0.365805188814799 valid 0.4187433421611786\n",
      "EPOCH 70: LOSS train 0.3422763446966807 valid 0.41136911511421204\n",
      "EPOCH 80: LOSS train 0.319073349237442 valid 0.40708985924720764\n",
      "EPOCH 90: LOSS train 0.30420616269111633 valid 0.40449824929237366\n",
      "EPOCH 100: LOSS train 0.29083962241808575 valid 0.40193676948547363\n",
      "EPOCH 110: LOSS train 0.27570924162864685 valid 0.402933806180954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:03:15,929] Trial 91 finished with value: 0.40429383516311646 and parameters: {'weight_decay': 2.480435454419332e-07, 'L1': 1.069458111732067e-07, 'learning_rate': 0.0008074055621061861, 'dropout': 0.002236404277136046, 'first_layer': 512}. Best is trial 13 with value: 0.3911953568458557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 120: LOSS train 0.2635584771633148 valid 0.40429383516311646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 22:03:16,381] Trial 92 pruned. \n",
      "[I 2023-08-24 22:03:17,136] Trial 93 pruned. \n",
      "[I 2023-08-24 22:03:19,109] Trial 94 pruned. \n",
      "[I 2023-08-24 22:03:19,370] Trial 95 pruned. \n",
      "[I 2023-08-24 22:03:19,708] Trial 96 pruned. \n",
      "[I 2023-08-24 22:03:20,035] Trial 97 pruned. \n",
      "[I 2023-08-24 22:03:22,191] Trial 98 pruned. \n",
      "[I 2023-08-24 22:03:22,505] Trial 99 pruned. \n"
     ]
    }
   ],
   "source": [
    "batch_size=512\n",
    "train_set_cell  = dataset(X_train_cell, y_train_cell)\n",
    "val_set_cell  = dataset(X_val_cell, y_val_cell)\n",
    "train_dl_cell = DataLoader(train_set_cell, batch_size=batch_size, shuffle=True)\n",
    "val_dl_cell = DataLoader(val_set_cell, batch_size=batch_size, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_cell))\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1450, 5011)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 256, 512]       2,566,144\n",
      "              ReLU-2             [-1, 256, 512]               0\n",
      "           Dropout-3             [-1, 256, 512]               0\n",
      "            Linear-4             [-1, 256, 512]         262,656\n",
      "              ReLU-5             [-1, 256, 512]               0\n",
      "           Dropout-6             [-1, 256, 512]               0\n",
      "            Linear-7             [-1, 256, 256]         131,328\n",
      "            Linear-8             [-1, 256, 512]         131,584\n",
      "              ReLU-9             [-1, 256, 512]               0\n",
      "          Dropout-10             [-1, 256, 512]               0\n",
      "           Linear-11             [-1, 256, 512]         262,656\n",
      "             ReLU-12             [-1, 256, 512]               0\n",
      "          Dropout-13             [-1, 256, 512]               0\n",
      "           Linear-14            [-1, 256, 5011]       2,570,643\n",
      "================================================================\n",
      "Total params: 5,925,011\n",
      "Trainable params: 5,925,011\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.89\n",
      "Forward/backward pass size (MB): 22.29\n",
      "Params size (MB): 22.60\n",
      "Estimated Total Size (MB): 49.78\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "EPOCH 10: LOSS train 0.5354260404904684\n",
      "EPOCH 20: LOSS train 0.43578791121641797\n",
      "EPOCH 30: LOSS train 0.3874096522728602\n",
      "EPOCH 40: LOSS train 0.3530871669451396\n",
      "EPOCH 50: LOSS train 0.32741330067316693\n",
      "EPOCH 60: LOSS train 0.30657560129960376\n",
      "EPOCH 70: LOSS train 0.28749021887779236\n",
      "EPOCH 80: LOSS train 0.27291540304819745\n",
      "EPOCH 90: LOSS train 0.2590872297684352\n",
      "EPOCH 100: LOSS train 0.2474228615562121\n",
      "EPOCH 110: LOSS train 0.23633062839508057\n",
      "EPOCH 120: LOSS train 0.22618636737267175\n",
      "EPOCH 130: LOSS train 0.21775576968987784\n",
      "EPOCH 140: LOSS train 0.211741603910923\n",
      "EPOCH 150: LOSS train 0.20337335268656412\n",
      "EPOCH 160: LOSS train 0.1977103129029274\n",
      "EPOCH 170: LOSS train 0.1919474477569262\n",
      "EPOCH 180: LOSS train 0.1873011365532875\n",
      "EPOCH 190: LOSS train 0.18321783343950906\n",
      "EPOCH 200: LOSS train 0.17926528304815292\n",
      "EPOCH 210: LOSS train 0.17509016394615173\n",
      "EPOCH 220: LOSS train 0.17280040929714838\n",
      "EPOCH 230: LOSS train 0.16898317635059357\n",
      "EPOCH 240: LOSS train 0.16637281825145087\n",
      "EPOCH 250: LOSS train 0.16459902127583823\n",
      "EPOCH 260: LOSS train 0.16133235394954681\n",
      "EPOCH 270: LOSS train 0.15965132166941962\n",
      "EPOCH 280: LOSS train 0.15722999473412833\n",
      "EPOCH 290: LOSS train 0.15684538582960764\n",
      "EPOCH 300: LOSS train 0.1545803522070249\n",
      "EPOCH 310: LOSS train 0.15429740647474924\n",
      "EPOCH 320: LOSS train 0.15175932894150415\n",
      "EPOCH 330: LOSS train 0.15114072958628336\n",
      "EPOCH 340: LOSS train 0.14956297228733698\n",
      "EPOCH 350: LOSS train 0.14867293337980905\n",
      "EPOCH 360: LOSS train 0.14764813830455145\n",
      "EPOCH 370: LOSS train 0.1471159135301908\n",
      "EPOCH 380: LOSS train 0.14658508201440176\n",
      "EPOCH 390: LOSS train 0.14600068082412085\n",
      "EPOCH 400: LOSS train 0.14484179019927979\n",
      "EPOCH 410: LOSS train 0.14479847997426987\n",
      "EPOCH 420: LOSS train 0.14366213232278824\n",
      "EPOCH 430: LOSS train 0.14315989861885706\n",
      "EPOCH 440: LOSS train 0.14297348260879517\n",
      "Epoch 00444: reducing learning rate of group 0 to 2.9815e-04.\n",
      "EPOCH 450: LOSS train 0.13865690430005392\n",
      "EPOCH 460: LOSS train 0.1380295306444168\n",
      "EPOCH 470: LOSS train 0.13787595431009927\n",
      "EPOCH 480: LOSS train 0.13782767206430435\n",
      "Epoch 00489: reducing learning rate of group 0 to 1.4907e-04.\n",
      "EPOCH 490: LOSS train 0.1375593294699987\n",
      "EPOCH 500: LOSS train 0.13630267481009165\n",
      "tensor(0.1241, device='cuda:0')\n",
      "tensor(0.1266, device='cuda:0')\n",
      "tensor(0.1250, device='cuda:0')\n",
      "tensor(0.1271, device='cuda:0')\n",
      "tensor(0.1273, device='cuda:0')\n",
      "tensor(0.1240, device='cuda:0')\n",
      "torch.Size([170, 5011])\n"
     ]
    }
   ],
   "source": [
    "param_dict = {'weight_decay': 1.123134977177082e-08, 'L1': 1.940470061480239e-08, 'learning_rate': 0.00019112034566968905, 'dropout': 0.005167929778613689, 'first_layer': 512}\n",
    "param_dict_10 = {'weight_decay': 1.9715224340849373e-07, 'L1': 8.085369143861159e-08, 'learning_rate': 0.0007516243324851748, 'dropout': 0.007615054022446145, 'first_layer': 512}\n",
    "param_dict_11 = {'weight_decay': 2.480435454419332e-07, 'L1': 1.069458111732067e-07, 'learning_rate': 0.0008074055621061861, 'dropout': 0.002236404277136046, 'first_layer': 512}\n",
    "# 13\n",
    "param_dict = {'weight_decay': 3.8115071306568935e-07, 'L1': 1.1354793800007698e-08, 'learning_rate': 0.0005962904416328184, 'dropout': 0.0027506894973574987, 'first_layer': 512}\n",
    "batch_size=256\n",
    "X_drug, y_drug = cell_data, cell_data\n",
    "train_set_drug  = dataset(X_drug, y_drug)\n",
    "print(X_drug.shape)\n",
    "train_dl_drug = DataLoader(train_set_drug, batch_size=batch_size, shuffle=True)\n",
    "xi, yi = next(iter(train_dl_drug))\n",
    "\n",
    "def fit():\n",
    "    lr = param_dict[\"learning_rate\"]\n",
    "    dropout=param_dict[\"dropout\"]\n",
    "    weight_decay=param_dict[\"weight_decay\"]\n",
    "    L1 = param_dict[\"L1\"]\n",
    "    first_layer=param_dict[\"first_layer\"]\n",
    "\n",
    "    ae = AE([xi.shape[1],first_layer,512,256,512,first_layer], dropout=dropout)\n",
    "    print(summary(ae.to(\"cuda\"), xi.shape))\n",
    "    early_stopper = EarlyStopper(patience=30)\n",
    "    optimizer = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, verbose=True, patience=10, min_lr=1e-7)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    epoch_number = 0\n",
    "\n",
    "    EPOCHS = 500\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        ae.train(True)\n",
    "        ae = ae.to(device=device)\n",
    "\n",
    "        avg_loss = train_one_epoch(ae, epoch_number, \"writer\", train_dl_drug, optimizer, loss_fn, L1=L1,device=device)\n",
    "\n",
    "        # Set the ae to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        # ae.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "\n",
    "        if epoch_number%10==9:print('EPOCH {}:'.format(epoch_number + 1),'LOSS train {}'.format(avg_loss))\n",
    "        scheduler.step(avg_loss)\n",
    "        if early_stopper.early_stop(avg_loss):             \n",
    "            break\n",
    "        epoch_number+=1\n",
    "    return ae\n",
    "\n",
    "ae = fit()\n",
    "ae.eval()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_dl_drug):\n",
    "        ae = ae.to(device)\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = ae(inputs)\n",
    "        loss = loss_fn(outputs, labels) \n",
    "        print(loss)\n",
    "with open(\"output_cell1.txt\", mode=\"w\") as f: \n",
    "    print(outputs.shape)\n",
    "    [f.write(\n",
    "        str(outputs[0][i].to(\"cpu\").numpy().round(3))+\"   \"+\n",
    "        str(inputs[0][i].to(\"cpu\").numpy().round(3))+\"\\n\") for i in range(len(outputs[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x20a63c583d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "inputs_np = inputs.to(\"cpu\").numpy().round(3)\n",
    "outputs_np = outputs.to(\"cpu\").numpy().round(3)\n",
    "inputs_0 = inputs_np[0]\n",
    "outputs_0 = outputs_np[0]\n",
    "mask = inputs_0<5\n",
    "\n",
    "# plt.xlim(-1, 1)\n",
    "# plt.ylim(-1, 1)\n",
    "plt.scatter(inputs_0[mask][:100], outputs_0[mask][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, h_sizes, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(h_sizes[0], h_sizes[1]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[1], h_sizes[2]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(h_sizes[2], h_sizes[3])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  =\"../models/cell_encoder.pt\"\n",
    "torch.save(ae.encoder.state_dict(), path)\n",
    "\n",
    "encoder = Encoder([xi.shape[1],param_dict[\"first_layer\"],512,256], dropout=0)\n",
    "encoder.encoder.load_state_dict(torch.load(path))\n",
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_dl_drug):\n",
    "        encoder = encoder.to(device)\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        outputs = encoder(inputs)\n",
    "with open(\"output_encoder_cell.txt\", mode=\"w\") as f: \n",
    "    [f.write(\n",
    "        str(outputs[0][i].to(\"cpu\").numpy().round(3))+\"   \"+\n",
    "        str(inputs[0][i].to(\"cpu\").numpy().round(3))+\"\\n\") for i in range(len(outputs[0]))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
